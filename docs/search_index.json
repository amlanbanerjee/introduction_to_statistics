[["index.html", "Introduction to Statistics Chapter 1 Course Information 1.1 Course Description 1.2 Course Outcomes", " Introduction to Statistics Amlan Banerjee, Ph.D. 2022-02-15 Chapter 1 Course Information The purpose of this elementary statistics course is to introduce students to the relationship between statistics and the world through use of a wide variety of real applications that bring life to theory and methods. 1.1 Course Description The course covers sampling methods, classification of data, probability, frequency and probability distributions, confidence intervals, tests of statistical significance, and simple regression and correlation. 1.2 Course Outcomes Upon successful completion of this course, the student will be able to: Interpret quantitative data using graphs and descriptive statistics with emphasis on histograms and boxplots. Compute measures of expectation and variation for a discrete probability distribution Compute probability for a binomial and normal distribution. Perform calculations to estimate parameters using confidence intervals based on the normal distribution and \\(t\\)-distribution. Perform hypotheses testing involving a sample mean, proportion, and standard deviation/variance Perform hypotheses testing involving two sample means (independent and dependent), two population proportions, and two standard deviations/variances. Construct linear regression models and correlation coefficients from datasets. "],["introduction.html", "Chapter 2 Introduction 2.1 What is Statistics? 2.2 Statistical Thinking 2.3 Types of Data or Variable 2.4 Sampling Methods 2.5 Sampling Errors 2.6 Bias and Precision 2.7 Observational Studies 2.8 Experimental Design", " Chapter 2 Introduction Learning Outcome: Select a suitable sampling design {simple random, systematic, stratified, cluster}, given information about the observational study or experiment. The chapter introduces various data types, sampling techniques, sampling errors, two main types of statistical studies, namely observational studies and experiments. Also discussed here are their benefits and drawbacks and how to design them well. 2.1 What is Statistics? The science of planning studies and experiments; obtaining data; and organizing, summarizing, presenting, analyzing, and interpreting those data and then drawing conclusions based on them. Application of statistics is literally everywhere - business, finance, engineering, health science, social science, environmental science, politics, education, and so on. 2.2 Statistical Thinking Statistical studies are designed by the following five steps: Raise a precise question about one or more variables. Will the COVID vaccine be effective on children (ages &lt; 5 years)? Design a plan to answer the question. Construct a theory or hypothesis based on existing knowledge, say laboratory experiments, or similar studies in the past. Who will be recruited to the clinical trial (target population)? What kind of data needs to be collected (age, sex, medical history, etc.)? How the data will be collected (sampling methods)? How the privacy of the participants will be protected? Collect the data. Analyze the data. Analyze the data to test your theory or hypothesis. Draw a conclusion from the data about the question and communicate the results with the stakeholders. Definitions Data are collections of observations, such as measurements or survey responses. Variable is a characteristics of the individuals to be measured or observed. Population is the complete collection of all measurements or data that are being considered. Typically, a population is the complete collection of data that we would like to make inference about. Census is the collection of data from every member of the population. Sampling Frame is a numbered list of all the individuals in the population from which a sample is drawn. Sample is a sub-collection of members selected from a population. Population Parameter is a numerical measurement describing some characteristics of a population. Sample Statistic is a numerical measurement describing some characteristics of a sample. Example (parameter vs. statistic): There are \\(17,246,372\\) high school students in the U.S. In a study of \\(8505\\) U.S. high school students \\(16\\) years of age or older, \\(44.5\\%\\) of them said that they texted while driving at least once during the previous \\(30\\) days. Parameter: What percent of the population texted while driving? (unknown) Statistic: \\(44.5\\%\\) 2.3 Types of Data or Variable 1) Categorical (or Qualitative) - consist of names or labels (not numbers that represent counts or measurements) Levels of Measurement of Qualitative Data - Nominal (unordered): the data fall into categories that have no particular order or ranking in relation to each other, e.g., color (blue, green, red,), gender (male, female), nationality (American, Canadian, Mexican,) - Ordinal (ordered): values have a natural order to ranking, but differences either cant be found or are meaningless e.g., temperature (low, medium, high), exam grade (A, B, C, D, F), satisfaction (high, neutral, low) 2) Numerical (or Quantitative) - consist of numbers representing measurements or counts. - Continuous: a subject or observation takes a value from an interval of real numbers, e.g., weight, height, age, etc. Continuous (numerical) data result from infinitely many possible quantitative values, where the collection of values is not countable, such as the lengths of distances from \\(0\\) inch to \\(12\\) inch. - Discrete: a subject or observation takes certain values from a finite set, e.g. population, traffic volume, etc. Discrete data result when values are quantitative and the number of values is finite, or countable, such as the number of tosses of a coin before getting tails. Levels of Measurement of Quantitative Data Interval Variables: these variables are measured along a continuum, and they have the property that equal differences between measures represent equal differences in the values of the variable. Therefore, differences are meaningful, but there is no natural zero starting point at which none of the quantity is present and ratios are meaningless. For example, temperature is measured in degrees Celsius. So the difference between \\(20^\\circ C\\) and \\(30^\\circ C\\) is the same as \\(30^\\circ C\\) to \\(40^\\circ C\\). However, \\(0^\\circ C\\) does not mean there is no temperature. Also, \\(\\dfrac{40^\\circ C}{20^\\circ C} = 2\\) does not mean \\(40^\\circ C\\) is twice the warmer than \\(20^\\circ C\\). Similarly the years 2021 and when you were born, say 1981, can be arranged in order, and the difference of \\(40\\) years can be found and is meaningful. However, time did not begin in year \\(0\\), so the year \\(0\\) is arbitrary instead of being a natural zero starting point representing no time. Ratio Variables: these variables have all the properties of interval variables, but in addition have the property that there is a natural zero starting point (where zero indicates that none of the quantity is present) and the ratios make sense. Examples of ratio variables include height, mass, distance, time etc. The name ratio reflects the fact that you can use the ratio of measurements. So, for example, a distance of \\(10\\) meters is twice the distance of \\(5\\) meters, and the measurement of distance starts at \\(0\\). 2.4 Sampling Methods Sampling from a Population Because populations are often very large, a common objective of the use of statistics is to obtain data from a sample and then use those data to form a conclusion about the population. Example: Identify the Variable, Sample, and Population of a Study In a poll of \\(1000\\) randomly selected American adults, \\(48\\%\\) of respondents said that they strongly disapprove of the way Congress is doing its job. The study then made an inference about all American adults. Define the variable of the study. Identify the sample. Identify the population. 2.4.1 Simple Random Sampling (SRS) A simple random sample of \\(n\\) subjects is selected in such a way that every possible sample of the sample size \\(n\\) has the same chance of being chosen. In statistics a sample of a population is said to be random if each member in the population has an equal chance of being chosen. Sampling with replacement - an individual is selected more than once. Sampling without replacement - an individual is selected only once. Source: OpenIntro.Org 2.4.2 Stratified Sampling The population is divided into non-overlapping, homogeneous subgroups called strata . Then, SRS is employed to select a certain number or a certain proportion of the whole within each stratum. Source: OpenIntro.Org Example: Stratified Sampling (Optional) Design a sample to survey \\(500\\) students using stratified sampling method. \\[ \\text { Strata Sizes } \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{3355} &amp; \\text{4693} &amp; \\text{8048} \\\\ \\text{Male} &amp; \\text{3734} &amp; \\text{6687} &amp; \\text{10421} \\\\ \\hline \\text{Total} &amp; \\text{7089} &amp; \\text{11380} &amp; \\text{18469} \\\\ \\end{array} } } \\] \\[ \\text { Strata Proportions } \\bbox[yellow,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{3355/18469 = .182} &amp; \\text{.254} &amp; \\text{.436} \\\\ \\text{Male} &amp; \\text{.202} &amp; \\text{.362} &amp; \\text{.564} \\\\ \\hline \\text{Total} &amp; \\text{.384} &amp; \\text{.616} &amp; \\text{1.000} \\\\ \\end{array} } } \\] \\[ \\text { Sample sizes } \\bbox[lightblue,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{.182(500)=91} &amp; \\text{127} &amp; \\text{218} \\\\ \\text{Male} &amp; \\text{101} &amp; \\text{181} &amp; \\text{282} \\\\ \\hline \\text{Total} &amp; \\text{192} &amp; \\text{308} &amp; \\text{500} \\\\ \\end{array} } } \\] Practice Design a sample to survey \\(1200\\) students using stratified sampling method. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Professional} \\\\ \\hline \\text{Female} &amp; \\text{10588} &amp; \\text{4475} &amp; \\text{1421} \\\\ \\text{Male} &amp; \\text{7762} &amp; \\text{3736} &amp; \\text{1153} \\\\ \\hline \\end{array} } } \\] 2.4.3 Cluster Sampling The population is often divided into non-overlapping mutually homogeneous yet internally heterogeneous subgroups called clusters. Cluster sampling is much like SRS, but instead of randomly selecting individuals, SRS is applied to select clusters. In other words, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves dont look very different from one another. That is, we expect strata to be self-similar (homogeneous), while we expect clusters to be diverse (heterogeneous). The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this is referred to as a one-stage cluster sampling plan. Sometimes cluster sampling can be a more economical random sampling technique than the alternatives. For example, if neighborhoods represented clusters, this sampling method works best when each neighborhood is very diverse. Because each neighborhood itself encompasses diversity, a cluster sample can reduce the time and cost associated with data collection, because the interviewer would need only go to some of the neighborhoods rather than to all parts of a city, in order to collect a useful sample. One-Stage Cluster Sampling Source: OpenIntro.Org Multistage Cluster Sampling A multistage or multistage cluster sampling is an extention of cluster sampling and involves two (or more) steps. First step is to take a cluster sample. Then, instead of including all of the individuals in these clusters in the sample, a second sampling method, usually SRS, is employed within each of the selected clusters. In the neighborhood example, we could first randomly select some number of neighborhoods and then take a SRS from just those selected neighborhoods. As seen in Figure, stratified sampling requires observations to be sampled from every stratum. Multistage sampling selects observations only from those clusters that were randomly selected in the first step. It is also possible to have more than two steps in multistage sampling. Each cluster may be naturally divided into subclusters. For example, each neighborhood could be divided into streets. To take a three-stage sample, we could first select some number of clusters (neighborhoods), and then, within the selected clusters, select some number of subclusters (streets). Finally, we could select some number of individuals from each of the selected streets. Source: OpenIntro.Org 2.4.4 Nonrandom Sampling Systematic Sampling Select every \\(k^{th}\\) individual from a list of the population, where the position of the first person chosen is randomly selected from the \\(k\\) individuals. This will give a non-representative sample if there is a structure to the list. Source: OpenIntro.Org Solve: The human resource department at a certain company wants to conduct a survey regarding worker benefits. The department has an alphabetical list of all \\(5465\\) employees at the company and wants to conduct a systematic sample of size \\(60\\). What is \\(k\\)? Convenience or Volunteer Sampling Use the first \\(n\\) individuals that are available or the individuals who volunteer to participate. This is almost sure to give a non-representative sample which cannot be generalized to the population. Exercise: Identifying Sampling Methods A researcher randomly selects 20 Taco Bell locations and surveys all the employees at those locations. A news station hosts a call-in survey about whether physician-assisted death should be legalized in all states. A researcher randomly selects an LED TV out of the first 200 LED TVs on an assembly line and also selects every 200th LED TV after that. In a study at a community college, 30 instructors are randomly selected from fulltime instructors and 50 instructors are selected from part-time instructors. The City Hall of Spring Hill, Kansas, creates a frame of its 5730 residents and randomly selects 60 residents. Example: Non-Representative Sample Survey Surveyed 10 million people who were subscribers or had telephones. 2.4 million people responded (i.e. 24% response rate) Prediction Landslide victory of Landon. Election Result Landslide victory of Roosevelt. What did go wrong with the poll? Sample was drawn from telephone directories, club membership, magazine subscibers, etc. who were upper middle class people, largely excluding poor unemployed people. The sample suffered from both selection and nonresponse bias. 2.5 Sampling Errors Statistical Inference Inferential Statistics is the practice of using information from a sample to draw conclusions about the entire population. It is the process of making judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling. \\(\\require{AMScd}\\) \\[ \\begin{CD} Sample @&gt; {\\text {statistical inference}} &gt;&gt; Population \\end{CD} \\] \\[\\underbrace{\\text {sample statistic}}_{\\text{investigator knows}} = \\underbrace{\\text {population parameter}}_{\\text{investigator wants to know}} + \\underbrace{\\text {bias}}_{\\text{nonsampling error}} + \\underbrace{\\text {chance variation}}_{\\text{random sampling error}} \\] Random Sampling Error - occurs when the sample has been selected with a random method, but there is a discrepancy between a sample result and the true population result. Non-sampling Error - is the results of human error, including such factors as wrong data entries, computing errors, questions with biased wording, false data provided by respondents, forming biased conclusions, or applying statistical methods that are not appropriate for the circumstances. A sampling method that consistently underestimates or overestimates some characteristics of the population is said to be biased. Selection/Sampling bias - occurs when the sample is selected in such a way that it systematically excludes or underrepresented part of the population. An online survey conducted to estimate the percentage of Americans who have a Facebook account. The survey is biased because people who go online are favored. People who never go online cannot participate in the poll. Nonresponse bias - occurs when responses are not obtained from all individuals selected for inclusion in a sample. It happens if individuals refuse to be part of the study or if the research cannot track down individuals identified to be in the sample. Measurement or response bias - occurs when the data are collected in such a way that it tends to result in observed values that are different from the actual value in some systematic way. Contributing factors: question wording and order; mode of survey; influence of the interviewer; people might exaggerate how much money they earn; or a researcher might record the information incorrectly etc. Response bias can also result from the wording of questions. For example, compare the impact of the following two questions: Do you brag about your past successes with others? Do you inspire others by sharing your past successes? Do you share your past successes with others? Nonrandom Sampling Error - is the results of using a sampling method that is not random, such as using a convenience sample or a voluntary response sample. 2.6 Bias and Precision Bias The average difference between the estimator and the true value. Precision The standard deviation of the estimator. \\[ \\begin{aligned} \\text{Mean Squared Error, MSE} &amp;= precision^2 + bias^2 \\\\ \\text{Root Mean Squared Error, RMSE} &amp;= \\sqrt{MSE} \\end{aligned} \\] Statistical Studies Explanatory and Response Variables In statistical studies, we want to know whether a variable \\(x\\) explains (or affects) another variables \\(y\\). The \\(x\\) variable is called the explanatory or independent variable. The \\(y\\) variable is called the response or dependent variable. Association vs. Causation There is an association between an explanatory and response variable when the response variable changes as the explanatory variable changes. If the change in the explanatory variable causes the change in the response variable, then there is a causation between the variables. 2.7 Observational Studies Generally, data in observational studies are collected on specific characteristics only by passively monitoring study participants, but the observers dont attempt to modify the individuals being studied. These studies are inexpensive and good for discovering relationships related to rare outcomes. They are generally only sufficient to show associations. Key points: Observational studies seldom support causal inference, \\(X \\rightarrow Y\\). Variables in observational studies are often measured concurrently. Such measurements provide no temporal precedence. The study design cannot determine which of the two variables, a presumed cause and a presumed effect, occurred first. Hence, in this kind of design, the sole basis for causal inference is assumption, or ruling out alternative explanations of the association between \\(X\\) and \\(Y\\), as well as measuring other presumed causes of \\(Y\\). It is possible to correctly infer causation in nonexperimental designs, but the hurdles are much greater. As an example, think about the causal link cigarette smoking and lung cancer. Types of observational studies: 1. Cross-sectional Studies - data are observed, measured, and collected at one point in time, not over a period of time. 2. Retrospective Studies - data are collected from a past time period by going back in time (through examinations of records, interviews, and so on). 3. Prospective (or longitudinal or cohort) Studies - data are collected in the future from groups that share common factors (such groups are called cohorts). Observational Study: drinking coffee and longevity Coffee drinkers may live longer - nytimes.com, May 16, 2012. Coffee may help you live longer, study suggests - thestar.com, May 17, 2012. No, drinking coffee probably wont make you live longer - washingtonpost.com, May 17, 2012. Association of coffee drinking with total and cause-specific mortality, New England Journal of Medicine, May 2012. Sample Size: 400,000 Age Range: 50-71 years Period: 1995 - 2008 Death: 52,000 How would you interpret the result? Confounding Variable A confounding variable is a variable that is associated with both the explanatory and response variables. Simultaneously with the explanatory variable, it may cause the response variable to change during the study. Because of the confounding variables association with both variables, we do not know if the response is due to the explanatory variable or due to the confounding variable. Sun exposure is a confounding factor because it is associated with both the use of sunscreen and the development of skin cancer. People who are out in the sun all day are more likely to use sunscreen, and people who are out in the sun all day are more likely to get skin cancer. Lurking Variable Lurking variables are variables that are not considered in the analysis, but may affect the nature of the relationship between the explanatory variable and the outcome. Table: 20-year survival status of women by smoking status \\[ \\begin{array}{c|lcr} &amp; \\text{Smoker} \\\\ &amp; \\text{Yes} &amp; \\text{No} \\\\ \\hline \\text {Dead} &amp; 0.239 &amp; 0.314 \\\\ \\text {Alive} &amp; 0.761 &amp; 0.686 \\end{array} \\] Are smokers less likely to die? 2.8 Experimental Design While observational studies are effective tools for answering certain research questions, experiments are essential to measure the effect of a treatment. In an experiment, we apply some treatment and then proceed to observe its effects on the individuals. Subject: Entity who is participating in the study. Treatment Group: The group of subjects that receives treatments. Control Group: The group of subjects that receives no treatment. Response Variable: The outcome of interest, measured on each subject. Factor: The categorical variable that explains the outcome of the experiment. Each category is called level. Blinding: When researchers keep the subjects uninformed about their treatment, the study is said to be blind. Its purpose is to reduce the potential for both researchers and subjects emotional bias. Subjects would not know which experimental group they are assigned to. The researcher (i.e. the person who is measuring the outcome) would not know which treatment is assigned to which experimental unit. Single-blind: only one type of blinding is applied. Double-blind: both types of blinding are applied. Placebo: A substance or treatment with no active ingredients. The control group receives the placebo treatment. This phenomenon, in which the recipient perceives an improvement in condition due to personal expectations, rather than the treatment itself, is known as the placebo effect. Principles of Experimental Design Well-conducted experiments are built on three main principles. Direct Control Researchers assign treatments to cases, and they do their best to control any other differences in the groups. They want the groups to be as identical as possible except for the treatment, so that at the end of the experiment any difference in response between the groups can be attributed to the treatment and not to some other confounding or lurking variable. Direct control refers to variables that the researcher can control. Randomization Researchers randomize patients into treatment groups to account for variables that cannot be controlled. Randomizing patients into the treatment or control group helps even out the effects of such differences, and it also prevents accidental bias from entering the study. Replication In a single study, replication is done by imposing the treatment on a sufficiently large number of subjects or experimental units. Scientists may also replicate the entire experiment on an entirely different population of experimental units to verify earlier findings. 2.8.1 Randomized Blocked Design Researchers sometimes know or suspect that another variable, other than the treatment, influences the response. Under these circumstances, they may carry out a blocked experiment. In this design, they first group individuals into blocks based on the identified variable (in other words, form blocks or groups of subjects with similar characteristics) and then randomize subjects within each block to the treatment groups. This strategy is referred to as blocking. For example, blocks can be designed based on gender or age group of subjects. 2.8.2 Completely Randomized Experimental Design A completely randomized experimental design is one in which the subjects or experimental units are randomly assigned to each group in the experiment. Source: OpenIntro.Org Case Study 1: PATRICIA Study | PApilloma TRIal against Cancer In young Adults The Lancet, Volume 374, Issue 9686, Pages 301 - 314, 25 July 2009 Efficacy of human papillomavirus (HPV) - 16/18 AS04-adjuvanted vaccine against cervical infection and precancer caused by oncogenic HPV types (PATRICIA); final analysis of a double-blind, randomized study in young women. Paavonen, et. al. \\[ \\begin{array}{c|c} {\\text{Response Variable} \\\\ \\text {(Acquired an infection)}} &amp; {\\text{Explanatory Variable} \\\\ \\text{(Given the HPV vaccine)}} \\\\ \\hline \\text{Yes} &amp; \\text{Yes} \\\\ \\text{No} &amp; \\text{No} \\\\ \\end{array} \\] \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{c} {\\text{Factor 1} \\\\ \\text{(2 Levels)}} \\\\ \\hline \\text{Drug A} \\\\ \\text{Drug B} \\end{array} } } \\] \\[ \\bbox[silver,5px] { \\color{black} { \\begin{array}{c} {\\text{Factor 2} \\\\ \\text{(2 Levels)}} \\\\ \\hline \\text{Dose A} \\\\ \\text{Dose B} \\end{array} } } \\] \\[ \\bbox[5px,border:2px solid red] { \\begin{array}{c} \\text{4 Treatments} \\\\ \\hline \\text{Drug A &amp; Dose A}\\\\ \\text{Drug A &amp; Dose B}\\\\ \\text{Drug B &amp; Dose A}\\\\ \\text{Drug B &amp; Dose B} \\end{array} } \\] Case Study 2: Ischemic Preconditioning | Effect on Muscular Endurance Can Ischemic Preconditioning improve athletic performance? Experimental units: 40 male teenagers Response Variable: length of time a wall squat position can be held Control Groups: 2 groups who received 0 lb pressure Control of Extraneous Factors: Age, sex, athletic ability Randomization: Randomly assigned 10 experimental units to each of 4 treatment groups \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{c|c} \\text{Factor1} &amp; {\\text{Amount of pressure} \\\\ \\text{applied by the} \\\\ \\text{bloodpressure cuff}} \\\\ \\hline \\text{Level 1} &amp; \\text{20 lb} \\\\ \\text{Level 2} &amp; \\text{0 lb} \\\\ \\end{array} } } \\] \\[ \\bbox[silver,5px] { \\color{black} { \\begin{array}{c|c} \\text{Factor2} &amp; {\\text{Length of time pressure} \\\\ \\text{was applied}} \\\\ \\hline \\text{Level 1} &amp; \\text{10 min} \\\\ \\text{Level 2} &amp; \\text{20 min} \\\\ \\end{array} } } \\] \\[ \\bbox[5px,border:2px solid red] { \\begin{array}{c} \\text{4 Treatments} \\\\ \\hline \\text{20 lb/10 min}\\\\ \\text{20 lb/20 min}\\\\ \\text{0 lb/ 10 min}\\\\ \\text{0 lb/ 20 min} \\end{array} } \\] Components of a Well-Designed Study There should be a control group and at least one treatment group. Individuals should be randomly assigned to the control and treatment group(s). The sample size should be large enough. A placebo should be used when appropriate. The study should be double-blind when possible. If this is impossible, then the study should be single-blind if possible. Practice: Identifying an Experiment and an Observational Study Identify whether the study is an experiment or an observational study. Discuss whether the components of a good study were used. For five years, the author taught an innovative intermediate algebra course in which students learned by working in groups. Then the author compared the proportion of his successful intermediate algebra students who passed trigonometry with the proportion of other professors successful intermediate algebra students who passed trigonometry. Practice: Redesign an Observational Study into a Well-Designed Experiment A researcher wants to determine whether taking vitamin C helps people avoid getting the flu and the common cold. She randomly selects 100 people and asks them whether they take vitamin C and how often they had the flu or a cold in the past year. The researcher analyzes the responses and concludes that vitamin C helps people avoid the flu and colds. Describe some problems with the observational study. Include in your description at least one possible lurking or confounding variable and identify which type it is. Redesign the study so that it is a well-designed experiment. "],["exploring-data-with-tables-and-graphs.html", "Chapter 3 Exploring Data with Tables and Graphs 3.1 Frequency Distribution 3.2 Histogram 3.3 Other Charts", " Chapter 3 Exploring Data with Tables and Graphs Learning Outcome: Interpret quantitative data using tables and graphs, and descriptive statistics using histograms. The chapter introduces various techniques such as frequency table and histogram to organize quantitative data to explore its important characteristics. 3.1 Frequency Distribution \\(\\text {Table: Drive-Through Service Times (seconds) for McDonald&#39;s Lunches}\\) A frequency distribution (or frequency table) shows how data are partitioned among several categories (or classes) by listing the categories along with the number (frequency) of data values in each of them. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency} \\\\ \\hline \\text{75-124} &amp; \\text{11} \\\\ \\text{125-174} &amp; \\text{24} \\\\ \\text{175-224} &amp; \\text{10} \\\\ \\text{225-274} &amp; \\text{3} \\\\ \\text{275-324} &amp; \\text{2} \\\\ \\hline \\text{Total} &amp; 50 \\end{array} } } \\] Lower class limits: \\({75, 125, 175, 225, 275}\\) Upper class limits: \\({124, 174, 224, 274, 324}\\) Class boundaries: \\({74.5, 124.5, 174.5, 224.5, 274.5, 324.5}\\) Class midpoints: \\({99.5, 149.5, 199.5, 249.5, 299.5}\\) Class width: \\(124.5 - 74.5 = 50\\) Relative Frequency Distribution A relative frequency distribution (or percentage frequency distribution) is a variation of the basic frequency distribution in which a class frequency is replaced by a relative frequency (or proportion). \\[ \\begin{align} \\text{relative frequency for a class} &amp;= \\dfrac{\\text{frequency for a class}}{\\text{sum of all frequencies}} \\\\ \\text{percentage of a class} &amp;= \\dfrac{\\text{frequency for a class}}{\\text{sum of all frequencies}} \\times 100\\%\\\\ \\end{align} \\] \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c} \\text{Time(Seconds)} &amp; \\text{Relative Frequency} \\\\ \\hline \\text{75-124} &amp; \\text{22}\\% \\\\ \\text{125-174} &amp; \\text{48}\\% \\\\ \\text{175-224} &amp; \\text{20}\\% \\\\ \\text{225-274} &amp; \\text{6}\\% \\\\ \\text{275-324} &amp; \\text{4}\\% \\\\ \\hline &amp; 100\\% \\end{array} } } \\] Cumulative Distribution \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c} \\text{Time(Seconds)} &amp; \\text{N} &amp; \\text{%} \\\\ \\hline \\text{75-124} &amp; 11 &amp; \\text{22}\\% \\\\ \\text{125-174} &amp; 35 &amp; \\text{70}\\% \\\\ \\text{175-224} &amp; 45 &amp; \\text{90}\\% \\\\ \\text{225-274} &amp; 48 &amp; \\text{96}\\% \\\\ \\text{275-324} &amp; 50 &amp; \\text{100}\\% \\\\ \\hline \\end{array} } } \\] 3.2 Histogram A histogram is a graph consisting of bars of equal width drawn adjacent to each other. The horizontal scale represents classes of quantitative data values; and the vertical scale represents frequencies. A relative frequency histogram has the same shape and horizontal scale as a histogram, but the vertical scale uses relative frequencies (as percentages) instead of actual frequencies. Importance of Histogram Visually displays the shape of the distribution of the data Shows the location of the center of the data Shows the spread of the data Identifies outliers Density Histogram \\[ \\begin{align} \\textbf{Density} &amp;= \\dfrac{\\textbf{relative frequency}}{\\textbf{bin width}} \\\\ \\text {Density of class (75-124)} &amp;= \\dfrac{\\text{rel. freq of class (75-124)}}{\\text{class width}} \\\\ &amp;= \\dfrac{0.22}{50} \\\\ &amp;= 0.0044 \\end{align} \\] In density histogram, area of each rectangular bar is the relative frequency of its class. \\(\\textbf{Total area of a density histogram is equal to 1.}\\) Practice - Construct a Density Histogram The accompanying frequency distribution summarizes data on the number of times smokers attempted to quit before their final successful attempts. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{r|c} \\text{Number of attempts} &amp; \\text{Frequency} &amp; \\text{Relative Frequency} &amp; \\text{Density} \\\\ \\hline \\textbf{0-10} &amp; 778 \\\\ \\textbf{10-20} &amp; 306 \\\\ \\textbf{20-30} &amp; 274 \\\\ \\textbf{30-40} &amp; 221 \\\\ \\textbf{40-50} &amp; 238 \\end{array} } } \\] 3.3 Other Charts Dotplot A dotplot uses dots to show the frequency, or number of occurrences, of the values in a data set. The higher the stack of dots, the greater the number of occurrences there are of the corresponding value. Pie Charts The distribution of a categorical variable can be described by a pie chart, which is a disk where slices represent the categories. The proportion of the total area for one slice is equal to the relative frequency for the category represented by the slice. The relative frequencies are usually written as percentages. Example 1: Construct and Interpret a Pie Chart A total of 273 children were surveyed about what job they would want to do. The jobs and the percentages of the children who voted for them are shown in the table. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{r|c} \\text{Job} &amp; \\text{Percent} \\\\ \\hline \\text{Spy/Agent} &amp; 16 \\\\ \\text{Veterinarian} &amp; 13 \\\\ \\text{Professional Athlete} &amp; 12 \\\\ \\text{Movie Star} &amp; 10 \\\\ \\text{Video Game Designer} &amp; 8 \\\\ \\text{Doctor} &amp; 6 \\\\ \\text{Other} &amp; 35 \\end{array} } } \\] Questions: Find the proportion of the observations that fall in the spy category. Find the proportion of the observations that do NOT fall in the spy category. Find the proportion of the observations that fall in the athlete category OR fall in the movie-star category. Interpreting a Multiple Bar Graph In a survey in 2012, 1960 adults were asked the following question: Generally speaking, do you usually think of yourself as a Republican, Democrat, Independent, or other? The results of the survey are described by the multiple bar graph. What proportion of women thought of themselves as Democrats? Which political party did the greatest proportion of men choose? Compare the proportion of women who thought of themselves as Independents to the proportion of men who thought of themselves as Independents. A total of 1081 women and 879 men responded to the survey. Were there more women or men who thought of themselves as Independents? How is this possible, given there was a smaller proportion of women who thought of themselves as Independents than men? Two-Way (Contingency) Table The table summarizes the responses from all 42 students who participated in the survey about whether they had read a novel in the past year. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{l|c|c|c} \\text{Gender} &amp; \\text{Did Not Read Novel} &amp; \\text{Read Novel} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; 6 &amp; 19 &amp; 25 \\\\ \\text{Male} &amp; 6 &amp; 11 &amp; 17 \\\\ \\hline \\text{Total} &amp; 12 &amp; 30 &amp; 42 \\\\ \\hline \\end{array} } } \\] How many of the students read a novel in the past year? What proportion of the students did not read a novel in the past year? What proportion of the women read a novel in the past year? What proportion of the students is men AND read a novel in the past year? "],["describing-exploring-and-comparing-data.html", "Chapter 4 Describing, Exploring, and Comparing Data 4.1 Measures of Shape 4.2 Measures of Center 4.3 Measures of Spread or Variation 4.4 Normal Distribution and Standard Deviation 4.5 Percentiles and Quartiles 4.6 Boxplot 4.7 Group Comparison", " Chapter 4 Describing, Exploring, and Comparing Data Learning Outcome: Calculate measures of central tendency, position, and spread, including standard deviation. Interpret quantitative data using graphs and descriptive statistics with emphasis on histograms and boxplots. In this chapter, we will numerically describe distributions of quantitative variables. Distributions, such as histograms, can be described by three characteristics: shape, center, and spread. 4.1 Measures of Shape Modes The mode of some data is an observation with the greatest frequency. There can be more than one mode, but if all the observations have frequency 1, then there is no mode. Mode is represented by a prominent peak in the distribution. Unimodel Distribution Bimodal Distribution Multimodal Distribution Uniform Distribution All the bins have the same frequency, or at least close to the same frequency. It is a distribution without a mode. Symmetry The histogram for a symmetric distribution will look the same on the left and the right of its center. Skew A histogram is skewed right if the longer tail is on the right side of the mode. A histogram is skewed left if the longer tail is on the left side of the mode. Outlier An Outlier is a data value that is far above or far below the rest of the data values. 4.2 Measures of Center Mean The sample mean of a numerical variable is computed as the sum of all of the observations \\(\\{x_1, x_2, \\cdots, x_n\\}\\) divided by the number of observations \\((n)\\). If \\(\\bar x\\) is the mean, then \\[ \\displaystyle (\\bar x - x_1) + (\\bar x - x_2) + \\cdots + (\\bar x - x_n) = 0 \\\\ \\displaystyle \\text{Therefore}, \\ \\bar x = \\frac{(x_1+x_2+...+x_n)}{n} = \\dfrac{1}{n}\\sum_{i=1}^n x_i \\] The mean follows the tail In a right skewed distribution, the mean is greater than the median. In a left skewed distribution, the mean is less than the median. In a symmetric distribution, the mean and median are approximately equal. Median The median splits an ordered data set in half. If there are an even number of observations, the median is the average of the two middle values. If there are an odd number of observations, the median is the middle value. 0 0 0 0 0 0 1 1 1 1 1 2 2 3 3 3 4 4 5 5 5 6 6 7 7 7 9 9 9 10 10 10 11 11 12 14 14 16 17 22 25 25 25 26 26 27 29 42 43 64 Calculating the Median \\(n\\) is odd Sort the series in ascending order. If the series has odd number \\((n)\\) of entries, the median is at position \\(x_1, \\cdots, \\underbrace{x_{\\frac{n+1}{2}}}, \\cdots, x_n.\\) Find the median of the series: \\(2,4,5,(6),7,9,9\\) The median is \\(6\\). \\(n\\) is even Sort the series in ascending order. If the series has even number \\((n)\\) of entries, the median is the average of the two middle numbers: \\(x_1, \\cdots, \\underbrace{x_{\\frac{n}{2}}, x_{\\frac{n}{2}+1}}_{ \\frac{1}{2}\\big(x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1} \\big)}, \\cdots, x_n.\\) Find the median of the numbers: \\(\\{2,2,4,6,7,8\\}\\) Median is the average of the third and the fourth numbers: \\(\\dfrac{4+6}{2}=5\\) Example: Comparing the Medians of Two Distributions \\[ \\begin{array}{lclc} \\text{Pacific State} &amp; \\text{Minimum Wage} &amp; \\text{Mountain State} &amp; \\text{Minimum Wage} \\\\ \\hline \\text {Alaska} &amp; 7.75 &amp; \\text {Arizona} &amp; 7.90 \\\\ \\text {California} &amp; 8.00 &amp; \\text {Colorado} &amp; 8.00 \\\\ \\text {Hawaii} &amp; 7.25 &amp; \\text {Idaho} &amp; 7.25 \\\\ \\text {Oregon} &amp; 9.10 &amp; \\text {Montana} &amp; 7.90 \\\\ \\text {Washington} &amp; 9.32 &amp; \\text {Nevada} &amp; 8.25 \\\\ &amp; &amp; \\text {New Mexico} &amp; 7.50 \\\\ &amp; &amp; \\text {Utah} &amp; 7.25 \\\\ &amp; &amp; \\text {Wyoming} &amp; 5.15 \\\\ \\end{array} \\] Find the median minimum wages of the Pacific and Mountain states. How the Shape of a Distribution Affects the Mean and the Median If a distribution is skewed left, the mean is usually less than the median and the median is usually a better measure of the center. If a distribution is symmetric, the mean is approximately equal to the median and both are reasonable measures of the center. If a distribution is skewed right, the mean is usually greater than the median and the median is usually a better measure of the center. List: \\(\\{2,3,3,4\\}\\) List: \\(\\{2,3,3,7\\}\\) \\(\\textbf {Notice: The median is unaffected by outliers.}\\) Weighted Mean The weighted mean is the same as the mean, except that it is influenced more by some observations than others. We assign weights to observations as a sort of way of describing its relative importance. The weighted mean of observations \\(x_1, x_2,...,x_n\\) using weights \\(w_1, w_2,...,w_n\\) is given by \\(\\displaystyle \\bar x =\\frac{w_1x_1+w_2x_2+...+w_nx_n}{w_1+w_2+...+w_n}\\) The simple mean is a weighted mean where all the weights are \\(1\\). \\(\\displaystyle \\bar x =\\frac{1\\times x_1+1\\times x_2+...+1\\times x_n}{1+1+...+1} = \\frac{x_1+x_2+...+x_n}{n}\\) Example 1: The chart below shows ratings from \\(10\\) visitors on a \\(0-5\\) scale. What is the average rating? \\[ \\begin{align} &amp;\\text{Method 1: } \\dfrac{4 + 1 + 3 + 5 + 3 + 5 + 2 + 5 + 5 + 4}{10} = 3.7 \\\\ \\\\ &amp;\\text{Method 2: } \\dfrac{(1)1 + (1)2 + (2)3 + (2)4 + (4)5}{10} = 3.7 \\\\ \\end{align} \\] Example 2: The consumer price index (CPI) is a weighted average of the change in prices paid by consumers for a representative set of goods and services. Use the table to calculate the CPI from March 2008 to March 2009. Round to the nearest tenth of a percent. \\[ \\begin{array}{l|c|r} \\text{Category} &amp; \\text{Weight} &amp; \\text{Percent Change} \\\\ \\hline \\text{food and beverages} &amp; 16\\% &amp; 4.3 \\\\ \\text{housing} &amp; 43\\% &amp; 1.4 \\\\ \\text{apparel} &amp; 4\\% &amp; 1.4 \\\\ \\text{transportation} &amp; 15\\% &amp; -13.1 \\\\ \\text{medical care} &amp; 6\\% &amp; 2.8 \\\\ \\text{recreation} &amp; 6\\% &amp; 1.7 \\\\ \\text{education and communication } &amp; 6\\% &amp; 3.6 \\\\ \\text{other} &amp; 4\\% &amp; 5.7 \\\\ \\hline \\end{array} \\] Calculating Mean from a Frequency Distribution \\[ \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} \\] where, \\(f\\) is the class frequency and \\(x\\) is the class midpoint. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f \\cdot x \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 1094.5 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 3588.0 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 1995.0 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 748.5 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 599.0 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum(f \\cdot x) = 8025.0 \\\\ \\end{array} } } \\] \\[ \\displaystyle \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} = \\dfrac{8025.0}{50} = 160.5 \\] Midrange The midrange of a data set is the measure of center that is the value midway between the maximum and minimum values in the original data set. It is found by adding the maximum data value to the minimum data value and then dividing the sum by \\(2\\), as the following formula: \\[ \\text {Midrange} = \\dfrac{\\text{maximum data value + minimum data value}}{2} \\] 4.3 Measures of Spread or Variation Range The range of a set of data is the difference between the maximum and the minimum data values. \\(\\textbf {range = maximum - minimum}\\) The range is sensitive to outliers. A single high or low value will affect the range significantly. Standard Deviation of a Sample SD \\((s)\\) of a set of sample values is a measure of how much, on average, the data values deviate away from the sample mean. In other word, SD describes the variability of the data set within the range of the dataset. Low variability or small spread means that the values tend to be more clustered together. High variability or large spread means that the values tend to be far apart. Calculating the Standard Deviation The standard deviation is the square root of the variance. It is roughly the average distance of the observations from the mean. \\[ \\bbox[yellow,5px] { \\color{black}{s= \\sqrt{\\frac{1}{n-1}\\sum(x_i-\\bar x)^2}} } \\] Exercise: \\(1. Calculate \\space SD \\space of \\space [0,1]\\) \\(2. Calculate \\space SD \\space of \\space [30,20, 41, 21]\\) Which histogram has the largest SD? Hint: pay attention to the range of the distributions. Calculating Standard Deviation from a Frequency Distribution \\(\\text {Step 1: Calculate weighted average from the class midpoint and class frequency}\\) \\[ \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} \\] where, \\(f\\) is the class frequency and \\(x\\) is the class midpoint. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f \\cdot x \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 1094.5 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 3588.0 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 1995.0 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 748.5 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 599.0 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum(f \\cdot x) = 8025.0 \\\\ \\end{array} } } \\] \\[ \\displaystyle \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} = \\dfrac{8025.0}{50} = 160.5 \\] \\(\\text {Step 2: Calculate variance}\\) \\[ \\displaystyle s^2 = \\dfrac{1}{\\sum_{i} f_i - 1}\\sum_{i} f_i(x_i - \\bar x)^2 \\] \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f_i(x_i - \\bar x)^2 \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 40931 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 2904 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 15210 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 23763 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 38642 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum f_i(x_i - \\bar x)^2 = 121450 \\\\ \\end{array} } } \\] \\[ \\begin{align} s^2 &amp;= \\dfrac{121450}{50-1} = 2478.6 \\\\ \\end{align} \\] \\(\\text {Step 3: Calculate standard deviation}\\) \\[ \\begin{align} s &amp;= \\sqrt {2478.6} = 49.8 \\end{align} \\] Standard Deviation of a Population \\[ \\bbox[yellow,5px] { \\color{black}{\\sigma= \\sqrt{\\frac{1}{N}\\sum(x_i-\\mu)^2}} } \\] Variance of a Sample and Population The variance of a set of values is a measure of variation equal to the square of the standard variation. Sample variance: \\(s^2 =\\) square of the sample standard deviation \\(s\\). Population variance: \\(\\sigma^2 =\\) square of the population standard deviation \\(\\sigma\\). Why is \\((n-1)\\) used to calculate standard deviation? \\(\\acute s = \\sqrt{\\frac{1}{n}\\sum(x_i-\\bar x)^2}\\) is not an unbiased estimator of the population standard deviation \\(\\sigma\\), meaning that the distribution of \\(\\acute s\\) does not tend to center around \\(\\sigma\\). Therefore, an adjustment is applied by replacing \\(n\\) by \\((n-1)\\) in the denominator. \\(s = \\sqrt{\\frac{1}{n-1}\\sum(x_i-\\bar x)^2}\\) is an unbiased estimator of \\(\\sigma\\). Also, with division by \\(n-1\\), sample variance \\(s^2\\) tend to center around the value of the population variance \\(\\sigma^2\\); with division by \\(n\\), sample variances \\(s^2\\) tend to underestimate the value of the population variance \\(\\sigma^2\\). Coefficient of Variation The coefficient of variation (CV) for a set of non-negative sample or population data, expressed as a percent, describes the standard deviation relative to the mean, and is given by the following: \\[ Sample: CV = \\frac{s}{x}.100 \\\\ Population: CV = \\frac{\\sigma}{x}.100 \\] \\(CV\\) is useful compare the spreads of multiple distributions. Suppose, you are looking for a safe investment option among stocks, EFTs, and bonds. \\[ \\text {CV =} \\dfrac{\\text{volatality } (\\sigma)}{\\text {expected return } (\\mu)} \\times 100 \\% \\] \\[ \\text {CV (Stocks) =} \\dfrac{11\\%}{15\\%} \\times 100 \\% = 73.3\\% \\\\ \\text {CV (EFT) =} \\dfrac{9\\%}{13\\%} \\times 100 \\% = 69.2\\% \\\\ \\text {CV (Bonds) =} \\dfrac{2\\%}{3\\%} \\times 100 \\% = 66.7\\% \\\\ \\] 4.4 Normal Distribution and Standard Deviation Probabilities for falling 1, 2, and 3 standard deviations of the mean in a normal distribution. Empirical Rule of Normal Distribution \\[ \\begin{array}{lc} \\text{Interval} &amp; \\text{Percent} \\\\ \\hline \\mu \\pm \\sigma &amp; 68 \\% \\\\ \\mu \\pm 2\\sigma &amp; 95 \\% \\\\ \\mu \\pm 3\\sigma &amp; 99.7 \\% \\\\ \\hline \\end{array} \\] z-score A z-score (or standard score or standardized value) is the number of standard deviations that a given value \\(x\\) is above or below the mean. The \\(z\\) score is calculated by using one of the following: \\[z= \\dfrac{x-\\bar x}{s}\\] \\[ \\begin{align} x = \\bar x - 2s &amp;\\implies z = -2 \\\\ x = \\bar x - s &amp;\\implies z = -1 \\\\ x = \\bar x &amp;\\implies z = 0 \\\\ x = \\bar x + s &amp;\\implies z = +1 \\\\ x = \\bar x + 2s &amp;\\implies z = +2 \\\\ \\end{align} \\] Note: \\(z\\)-score is useful in comparing two variables measured on different scales. Example: The \\(4000\\) g weight of a newborn baby (among 400 weighs with sample mean \\(\\bar x = 3152.0\\) g and sample standard deviation \\(s = 693.4\\) g) \\[ z = \\dfrac{x - \\bar x}{s} = \\dfrac{4000 - 3152.0}{693.4} = 1.22 \\] The weight \\(4000\\) g is \\(1.22\\) standard deviations away from the mean. 4.5 Percentiles and Quartiles Percentiles are measures of location, denoted \\(P_1, P_2, \\cdots, P_{99}\\), which divide a set of data into \\(100\\) groups with about \\(1\\%\\) of the values in each group. Example - the \\(50th\\) percentile, denoted \\(P_{50}\\) has about \\(50\\%\\) of the data values below it. The \\(n^{th}\\) percentile is the data value such that \\(n\\) percent of the data lies below that value. \\[ \\text{Percentile of value } x = \\frac{\\text{number of values less than } x}{\\text{total number of values}} \\times 100 \\] Finding a Percentile \\(\\text {Table: (Sorted) Verizon Airport Data Speeds (Mbps)}\\) Find the percentile for the data of \\(11.8\\) mbps. There are \\(50\\) data speeds in the table. There are \\(20\\) data speeds less than \\(11.8\\) mbps. \\(\\therefore\\) percentile of \\(11.8 = \\dfrac{20}{50} \\cdot 100 = 40\\) Interpretation: A data speed of \\(11.8\\) mbps is in the \\(40\\)th percentile. This means \\(40\\%\\) of the airports reported data speeds less than \\(11.8\\) mbps. Find the \\(40th\\) percentile. First, compute the locator, \\(L = \\dfrac{k}{100} \\cdot n = \\dfrac{40}{100} \\cdot 50 = 20.\\) By definition, \\(P_{40}\\) below which \\(40\\%\\) of the data points fall. Hence, the \\(40\\)th percentile is midway between the \\(20\\)th and \\(21\\)st value. In the table above, \\(20th\\) value is \\(11.6\\) and \\(21\\)st value is \\(11.8\\), so the midway between them is \\(11.7\\) mbps. \\(\\therefore P_{40} = 11.7\\) mbps. Note: in some references, a percentile value is chosen from the existing data. Therefore, the \\(21st\\) data point \\(11.8\\) can also be considered the \\(40th\\) percentile value. This answer is consistent with the previous example. Find the \\(25th\\) percentile. \\(L = \\dfrac{k}{100} \\cdot n = \\dfrac{25}{100} \\cdot 50 = 12.5.\\) In this case, we round up the value of \\(L\\) to get \\(13\\) to make sure at least \\(12.5\\) data points fall below that point. \\(\\therefore P_{25} = 7.9\\) mbps. Three Quartiles \\((Q_1, Q_2, Q_3)\\) \\(Q_1\\) represents the first quartile, which is the 25th percentile, and is the median of the smaller half of the data set. \\(Q_2\\) represents the second quartile, which is equivalent to the 50th percentile (i.e. the median). \\(Q_3\\) represents the third quartile, or 75th percentile, and is the median of the larger half of the data set Interquartile Range \\((IQR) = Q_3 - Q_1\\) 4.6 Boxplot 5-Number Summary For a set of data, the 5-number summary consists of five values: minimum first quartile, \\(Q_1\\) second quartile (median), \\(Q_2\\) third quartile, \\(Q_3\\) Maximum A boxplot (or box-and-whisker diagram) is a graph of a data set that consists of a line extending from the minimum value to the maximum value, and a box with lines drawn at the first quartile \\((Q_1)\\), the median \\((Q_2)\\), and the third quartile \\((Q_3)\\). Outlier and Fences When in the context of a box plot, define an outlier as an observation that right fence: \\[ Q_3 + 1.5 \\times IQR &lt; \\text {outlier} \\] left fence: \\[ \\text {outlier} &lt; Q_1 - 1.5 \\times IQR \\] Such points are marked using a dot or asterisk in a box plot. Example: Data: \\([5, 5, 9, 10, 15, 16, 20, 30, 40]\\) \\[ \\text{Q}_1 = 9 \\\\ \\text{Q}_2 = 15 \\\\ \\text{Q}_3 = 20 \\\\ \\text{IQR} = 20 - 9 = 11 \\\\ \\text{LF} = \\text{Q}_1 - 1.5 \\times \\text{IQR} = -7.5 \\\\ \\text{UF} = \\text{Q}_3 + 1.5 \\times \\text{IQR} = 36.5 \\\\ \\] Five Number Summary: Min. 1st Qu. Median Mean 3rd Qu. Max. 5.00 9.00 15.00 16.67 20.00 40.00 Exercise: Drawing a Boxplot with an Outlier Students in one of the authors statistics classes were surveyed about the number of novels they read in the past year. Here are the anonymous responses (in numbers of novels) of 11 of the students: \\([2, 5, 2, 0, 2, 3, 0, 5, 6, 4, 12]\\) Construct a boxplot. Check if there is any outlier. 4.7 Group Comparison Comparing distributions of median household income for counties by population gain status Source: OpenIntroOrg "],["discrete-probability-distributions.html", "Chapter 5 Discrete Probability Distributions 5.1 Definitions 5.2 Finding Probabilities 5.3 Probability Distributions 5.4 Parameters of a Probability Distribution 5.5 Binomial Probability Distribution", " Chapter 5 Discrete Probability Distributions Learning Outcome Compute measures of expectation and variation for a discrete probability distribution. In this chapter, we will extend the concept of relative frequencies to understand and calculate the probability of occurrence of a random event. We will also learn about normal distribution, its properties, and methods to calculate probabilities of random events that are described by this distribution. 5.1 Definitions An event is any collection of results or outcomes of a procedure. For example, tossing a coin is an event with possible outcomes heads and tails. A simple event is an event that has one outcome. For example, births of \\(2\\) girls followed by a boy is a simple event because the only possible outcome is \\(\\{ggb\\}\\). However, births of \\(2\\) girls and a boy is an event that has three possible outcomes \\(\\{ggb, gbg, bgg \\}\\). A sample space for a procedure consists of all possible simple events. For example, with births of three children, the sample space consists of eight different simple events \\(\\{ bbb, bbg, bgb, bgg, gbb, gbg, ggb, ggg\\}\\) An event is random if individual outcomes of it are unpredictable, meaning they have no apparent pattern of occurrence, but there is nonetheless a predictable distribution (i.e. the frequencies) of those different outcomes over a large number of repetitions of the event. The probability of any outcome of a random event can be defined as the proportion of times the outcome would occur in a very long series of repetitions. The probability is defined as a proportion, and it always takes values between \\(0\\) and \\(1\\) (inclusively). It may also be displayed as a percentage between \\(0\\%\\) and \\(100\\%\\). \\(0\\%: \\text{event is impossible}\\) \\(100\\%: \\text{event is certain}\\) THEORETICAL VERSUS EXPERIMENTAL PROBABILITY The Theoretical probability is the likelihood of occurring of an event. It is simply the ratio of the number of desired outcomes and the number of all possible outcomes. The experimental probability is an estimate of the likelihood of occurring of an event based on repeated trials. The subjective probability is an estimate of the likelihood of occurring of an event based on someones belief. LAW OF LARGE NUMBERS Consider: Rolling a 1 of a die If the sample space of a random experiment consists of \\(n\\) equally likely outcomes and an event \\(E\\) consists of \\(m\\) of those outcomes, then \\[\\text {Theorerical Probability} : P(E) = \\frac{\\text{number of outcomes in the event}(m)}{\\text{total number of outcomes}(n)}\\] Let \\(\\hat{p_n}\\) be the proportion of outcomes that are \\(1\\) after the \\(n\\) rolls. As the number of rolls \\((n)\\) increases, \\(\\hat{p_n}\\) (the relative frequency of rolls, or the experimental probability) will converge to the theoretical probability of rolling a \\(1,\\space p = 1/6.\\) The figure shows the convergence for \\(100,000\\) die rolls. The tendency of \\(\\hat{p_n}\\) to stabilize around \\(p\\), i.e. the tendency of the relative frequency to stabilize around the true probability, is described by the Law of Large Numbers. As more observations are collected, the observed proportion \\(\\hat{p_n}\\) of occurrences with a particular outcome after \\(n\\) trials converges to the true probability \\(p\\) of that outcome. Die Rolls Simulation The figure shows the fraction of die rolls that are \\(1\\) at each stage in a simulation. The relative frequency tends to get closer to the probability \\(1/6 \\approx 0.167\\) as the number of rolls increases. Example: Calculating Classical Probabilities Assuming that births of boys and girls are equally likely, find the probability of getting children of all of the same gender when three children are born. Recall the sample space of three children: \\(\\{ bbb, bbg, bgb, bgg, gbb, gbg, ggb, ggg \\}\\), includes eight equally likely outcomes, and there are exactly two outcomes in which three children are of the same gender: \\(\\{ bbb, ggg \\}\\) . \\[ P(\\text{ three children of the same gender}) = \\dfrac{2}{8} = \\dfrac{1}{4} = 0.25 \\] 5.2 Finding Probabilities This section presents the addition and multiplication rules of calculating probabilities. 5.2.1 Disjoint or mutually exclusive outcomes Two events or outcomes are called disjoint or mutually exclusive if they cannot both happen in the same trial. When rolling a die, the outcomes \\(1\\) and \\(2\\) are disjoint, and we compute the probability that one of these outcomes will occur by adding their separate probabilities: \\[P(1 \\text{ or } 2)=P(1)+P(2)=1/6+1/6=1/3\\] What about the probability of rolling a \\(1, 2, 3, 4, 5, \\ or \\ 6\\) ? \\[ \\begin{array}{ll} P(1 \\text{ or } 2 \\text{ or } 3 \\text{ or } 4 \\text{ or } 5 \\text{ or }6) = P(1)+P(2)+P(3)+P(4)+P(5)+P(6) \\\\ =1/6+1/6+1/6+1/6+1/6+1/6 =1 \\end{array} \\] It is no surprise that the probability is \\(1\\), since it is certain that one of the six outcomes must occur. ADDITION RULE OF DISJOINT OUTCOMES If \\(A_1,...,A_k\\) represent \\(k\\) disjoint outcomes, then the probability that one of them occurs is given by: \\[P(A_1\\text{ or }A_2 \\text{ or ... or }A_k)=P(A_1)+P(A_2)+...+P(A_k)\\] Example: Consider a standard deck of cards. \\[ \\text {4 suits} \\left\\{ \\begin{array}{ll} \\text{hearts: } \\color{red}{\\heartsuit} \\\\ \\text{diamonds: } \\color{red}{\\diamondsuit} \\\\ \\text{clubs: } \\spadesuit \\\\ \\text{spades: } \\clubsuit \\end{array} \\right. \\] \\[\\text{13 cards in each suit: } Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King\\] One card is dealt from a well shuffled deck. \\[ \\begin{align} P(\\text{the card is an ace or a king}) &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a king}) \\\\ &amp; = 4/52+4/52 \\\\ &amp; = 8/52 \\\\ &amp; = 2/13 \\end{align} \\] Venn Diagram - a diagram style to illustrate simple set relationships in probability. Venn Diagram | When events are disjoint \\[ \\begin{align} P(\\text{the card is an ace or a king}) &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a king}) \\\\ &amp; = 4/52+4/52 \\\\ &amp; = 2/13 \\end{align} \\] ## (polygon[GRID.polygon.385], polygon[GRID.polygon.386], polygon[GRID.polygon.387], polygon[GRID.polygon.388], text[GRID.text.389], text[GRID.text.390], text[GRID.text.391], text[GRID.text.392]) 5.2.2 Probabilities when events are NOT disjoint or mutually exclusive \\[ \\begin{align} &amp; P(\\text{the card is an ace or a heart}) \\\\ &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a heart})-P(\\text{it&#39;s an ace &amp; heart}) \\\\ &amp; = 4/52+13/52 - \\underbrace{1/52}_{\\text {adjustment made to avoid double-counting of the ace of hearts}} \\\\ &amp; = 16/52 \\\\ &amp; = 4/13 \\end{align} \\] Venn Diagram | When events are not disjoint \\[ \\begin{align} &amp; P(\\text{the card is an ace or a heart}) \\\\ &amp; = P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a heart})-P(\\text{it&#39;s an ace AND heart}) \\\\ &amp; = 4/52+13/52 - 1/52 = 16/52 \\end{align} \\] FALSE (polygon[GRID.polygon.393], polygon[GRID.polygon.394], polygon[GRID.polygon.395], polygon[GRID.polygon.396], text[GRID.text.397], text[GRID.text.398], text[GRID.text.399], text[GRID.text.400], text[GRID.text.401]) GENERAL ADDITION RULE OF PROBABILITIES \\[ \\bbox[yellow,5px] {\\color{black}{P(A \\space or \\space B) = P(A) + P(B) - P(A \\space and \\space B)}} \\] where \\(P(A \\text{ and } B)\\) is the probability that both events occur. If \\(A\\) and \\(B\\) are mutually exclusive, \\(P(A \\space and \\space B) = 0\\) Therefore, \\[ P(A \\space or \\space B) = P(A) + P(B)\\] 5.2.3 Complement of an event The complement of event \\(A\\) is denoted \\(A^c\\), and \\(A^c\\) represents all outcomes not in \\(A\\). \\(A\\) and \\(A^c\\) are mathematically related: \\[ \\begin{align} &amp; P(A) + P(A^c) = 1 \\\\ or, \\space &amp; P(A^c) = 1 - P(A) \\end{align} \\] Example: if an event has chance \\(40\\%\\), then the chance that it doesnt happen is \\(60\\%\\). Venn Diagram | Exercise \\[ \\begin{align} P(email) &amp;=0.73 \\\\ P(text) &amp;= 0.62 \\\\ P(\\text {email &amp; text}) &amp;= 0.49 \\\\ P(\\text {only email}) &amp;= 0.73 - 0.49 = 0.24 \\\\ P(\\text{only text}) &amp;= 0.62 - 0.49 = 0.13 \\\\ P(\\text{neither email nor text}) &amp;= 1 - (0.24 + 0.49 + 0.13) = 0.14 \\end{align} \\] (polygon[GRID.polygon.402], polygon[GRID.polygon.403], polygon[GRID.polygon.404], polygon[GRID.polygon.405], text[GRID.text.406], text[GRID.text.407], text[GRID.text.408], text[GRID.text.409], text[GRID.text.410]) 5.2.4 Multiplication Rule | for independent processes If \\(A\\) and \\(B\\) represent events from two different and independent processes, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their seprarate probabilities: \\[P(A \\text{ and } B) = P(A) \\times P(B)\\] Similarly, if there are \\(k\\) events \\(A_1,...,A_k\\) from \\(k\\) independent processes, then the probability they all occur is \\[ \\bbox[yellow,5px] { \\color{black} {P(A_1\\text{ and }A_2 \\text{ and ... and }A_k)=P(A_1)\\times P(A_2)\\times...\\times P(A_k)} } \\] Example 1: If a card is randomly drawn from a well-shuffled deck, what is the probability that it is the ace of hearts? [Note: Ace and Hearts are two independent events.] \\[ \\begin{align} P(Ace \\text{ and } Hearts) &amp;= P(Ace) \\times P(Hearts) \\\\ &amp;= (4/52) \\times (13/52) = 1/52 \\end{align} \\] Example 2: About \\(9\\%\\) of people are left-handed. Suppose \\(5\\) people are selected at random from the US population. (a) What is the probability that all are right-handed? (b) What is the probability that all are left-handed? (c) What is the probability that not all of them are right-handed? \\[ \\begin{align} &amp;(a) \\space P\\text{(All are RH)} = (1-0.09)^5 = 0.624 \\\\ &amp;(b) \\space P\\text{(All are LH)} = (0.09)^5 = 0.0000059 \\\\ &amp;(c) \\space P\\text{(not all RH)} = 1- P(\\text {all RH}) = 1-0.624 = 0.376 \\end{align} \\] GENERAL MULTIPLICATION RULE If \\(A\\) and \\(B\\) represent two outcomes or events, then \\[ \\bbox[yellow,5px] {\\color{black}{P(A \\space and \\space B) = P(A|B) \\times P(B)}} \\] Example 3: During the smallpox outbreak, \\(96.08\\%\\) of Boston residents were not inoculated, and \\(85.88\\%\\) of the residents who were not inoculated ended up surviving. What is the probability that a resident was not inoculated and lived? To answer the question, we want to determine \\(\\text {P(lived and inoculated)}\\), and we are given that, \\[ \\begin{align} P(\\text{lived | not inoculated}) &amp;= 0.8588 \\\\ P(\\text {not inoculated}) &amp;= 0.9608 \\end{align} \\] Among the \\(96.08\\%\\) of people who were not inoculated, \\(85.88\\%\\) survived: \\[ P(\\text {lived and not inoculated}) = 0.8588 \\times 0.9608 = 0.8251 \\] This is equivalent to the General Multiplication Rule. 5.2.5 Marginal and joint probabilities If a probability is based on a single variable, it is a marginal probability. The probability of outcomes for two or more variables or processes is called a joint probability. Exercise: Calculating Probabilities with a Contingency Table: Table: College enrollment and parents educational attainment \\[ \\begin{array} {l|cc|r} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{total} \\\\ \\hline \\text {teen: college} &amp; 231 &amp; 214 &amp; 445 \\\\ \\text {teen: no college} &amp; 49 &amp; 298 &amp; 347 \\\\ \\hline \\text {total} &amp; 280 &amp; 512 &amp; 792 \\end{array} \\] a) Finding Marginal and Joint Probabilities: \\[ \\begin{array} {l|cc|c} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{marginal} \\\\ \\hline \\text {teen: college} &amp; \\color{red}{0.29} &amp; \\color{red}{0.27} &amp; \\color{blue}{0.56} \\\\ \\text {teen: no college} &amp; \\color{red}{0.06} &amp; \\color{red}{0.38} &amp; \\color{blue}{0.44} \\\\ \\hline \\text {marginal} &amp; \\color{blue}{0.35} &amp; \\color{blue}{0.65} &amp; 1.00 \\end{array} \\] \\[ \\begin{align} &amp;\\color{blue}{\\text{Marginal Probability: }} P(\\text{teen: college})=\\frac{445}{792}=0.56 \\\\ &amp;\\color{red}{\\text{Joint Probability: }} P(\\text {teen: college and parents: no degree})=\\frac{214}{792}=0.27 \\end{align} \\] b) Finding Conditional Probability: Conditional Probability The conditional probability of the outcome of interest \\(A\\) given condition \\(B\\) is computed as the following: \\[P(A|B) = \\frac{P(A \\text{ and } B)}{P(B)}\\] \\[ \\begin{array} {l|cc|r} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{total} \\\\ \\hline \\text {teen: college} &amp; 231 &amp; 214 &amp; 445 \\\\ \\text {teen: no college} &amp; 49 &amp; 298 &amp; 347 \\\\ \\hline \\text {total} &amp; 280 &amp; 512 &amp; 792 \\end{array} \\] \\[ \\begin{align} P(\\text {teen college | parents degree}) &amp;= \\frac{231/792}{280/792} = 0.825 \\\\ P(\\text {teen college | parents no degree}) &amp;= \\frac{214/792}{512/792} = 0.418 \\\\ P(\\text {teen no college | parents degree}) &amp;= \\frac{49/792}{280/792} = 0.175 \\\\ P(\\text {teen no college | parents no degree}) &amp;= \\frac{298/792}{512/792} = 0.582 \\end{align} \\] c) Condition of Independence Verify whether one of the following equations holds: \\[ \\begin{align} P(A|B) &amp;= P(A) \\tag 1 \\\\ P(A \\space and \\space B) &amp;=P (A) \\times P(B) \\tag 2 \\end{align} \\] Check if the equality holds in the following equation: \\[ \\begin{align} P(\\text{teen college | parent degree})&amp;\\stackrel{?}{=} P(\\text {teen college}) \\\\ 0.825 &amp;\\ne 0.560 \\end{align} \\] Because both sides are not equal, teenager college attendance and parent degree are not independent. Two events are mutually exclusive If \\(A\\) and \\(B\\) are mutually exclusive events, then they cannot occur at the same time. If asked to determine if events \\(A\\) and \\(B\\) are mutually exclusive, verify one of the following equations holds: \\[ \\begin{align} P(\\text{A and B})&amp;= 0 \\tag 1 \\\\ P(\\text{A or B}) &amp;= P(A)+P(B) \\tag 2 \\end{align} \\] If the equation that is checked holds true, \\(A\\) and \\(B\\) are mutually exclusive. If the equation does not hold, then \\(A\\) and \\(B\\) are not mutually exclusive. At Least One A poker hand (5 cards) is dealt from a well shuffled deck. What is the chance that there is at least one ace in the hand? \\[ \\begin{align} &amp;P(\\text{at least one ace}) \\\\ &amp;=1-P(\\text{no aces}) \\\\ &amp;=1-(48/52) \\times (47/51) \\times (46/50) \\times (45/49) \\times (44/48) \\\\ &amp;=34.11\\% \\end{align} \\] 5.3 Probability Distributions DEFINITIONS A random variable is a numerical measure of an outcome from a random experiment. It has a single numerical value, determined by chance, for each outcome of an event. We often use a capital letter such as \\(X\\) to stand for a random variable. Let \\(X\\) be a random variable representing all possible outcomes of rolling a six-sided die once. Find the given probability: \\(1. P(X=4)\\) \\(2. P(X\\le 4)\\) \\(3. P(X&gt;4)\\) \\(4. P(3 \\le X \\le 6)\\) A discrete random variable has a collection of values that is finite or countable, such as number of tosses of a coin before getting heads. A continuous random variable has infinitely many values, and the collection of values is not countable, such as body temperature. A probability distribution is a description that gives the probability for each value of the random variable. It can be expressed as a table or graph, or formula. Probability Distribution: Requirements There is a numerical (not categorical) random variable \\(x\\), and its number values are associated with corresponding probabilities. \\(\\sum P(x)=1\\), where \\(x\\) assumes all possible values. \\(0 \\le P(x) \\le 1\\) for every individual value of the random variable \\(x\\). \\[ \\begin{array}{c|lcr} x: \\text{ number of heads} \\\\ \\text {when two coins are tossed} &amp; P(x) \\\\ \\hline 0 &amp; 0.25 \\\\ 1 &amp; 0.50 \\\\ 2 &amp; 0.25 \\end{array} \\] Find the probability of getting two heads out of two tosses? Find the probability of getting at least one head out of two tosses? Find the probability of getting at most one head out of two tosses? 5.4 Parameters of a Probability Distribution Mean \\(\\mu\\) of a probability distribution \\(\\mu = \\sum [x_i \\cdot P(x_i)]\\) Variance \\(\\sigma^2\\) for a probability distribution \\(\\sigma^2 = \\sum[(x_i-\\mu)^2 \\cdot P(x_i)] = \\sum[x_i^2 \\cdot P(x_i)]-\\mu^2\\) Standard deviation \\(\\sigma\\) for a probability distribution \\(\\sigma = \\sqrt{\\sum[x_i^2 \\cdot P(x_i)]-\\mu^2}\\) Expected Value In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents. For example, the expected value in rolling a six-sided die is \\(3.5\\), because the average of all the numbers that come up in an extremely large number of rolls is close to \\(3.5\\). The law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same principle applies to an absolutely continuous random variable, except that an integral of the variable with respect to its probability density replaces the sum. The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variables value from the variables expected value. Expected value of a discrete random variable If \\(X\\) takes outcomes \\(x_1, x_2,\\cdots,x_m\\) with probabilities \\(p_1, p_2,\\cdots, p_m\\) the expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[ \\begin{align} E(X) &amp;= \\mu_X = x_1 \\times p_1 + x_2 \\times p_2 +\\cdots+x_m \\times p_m \\\\ &amp;= \\sum^{m}_{i=1}(x_i \\times p_i) \\end{align} \\] Example: \\(\\text{ Random Variable } X: \\text{the number of spots on one roll of a die}\\) Probability distribution table for \\(X\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6\\\\ \\hline P(x) &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\end{array} \\] \\[ E(X) = 1 \\cdot (1/6) + 2 \\cdot (1/6) + 3 \\cdot (1/6) + 4 \\cdot (1/6) + 5 \\cdot (1/6) + 6 \\cdot (1/6) = 3.5 \\] Variability in Discrete Random Variables Variance and standard deviation of a discrete random variable If \\(X\\) takes outcomes \\(x_1, x_2, \\cdots ,x_m\\) with probabilities \\(p_1, p_2, \\cdots ,p_m\\) and expected value \\(\\mu_x = E(X),\\) then to find the standard deviation of \\(X\\), we first find the variance and then take its square root. \\[ \\begin{align} Var(X)=\\sigma^2_x &amp;= (x_1 - \\mu_x)^2 \\times p_1 + (x_2 - \\mu_x)^2 \\times p_2 + \\cdots + (x_m - \\mu_x)^2 \\times p_m \\\\ &amp;= \\sum^m_{i=1}(x_i-\\mu_x)^2 \\times p_i \\\\ \\\\ SD(X) = \\sigma_x &amp;= \\sqrt{\\sum^m_{i=1}(x_i-\\mu_x)^2 \\times p_i} \\\\ \\\\ \\text {From the above example, } \\\\ \\\\ Var(X)=\\sigma^2_x &amp;= [(1 - 3.5)^2 + (2 - 3.5)^2 + \\cdots + (6 - 3.5)^2]\\times (1/6) \\\\ &amp;= 2.92 \\\\ \\\\ SD(X) = \\sigma_x &amp;= \\sqrt{2.92} = 1.71 \\end{align} \\] Exercise 1: Coin Toss \\[ \\begin{array}{c|lcr} x : \\text{ number of heads} \\\\ \\text {when two coins are tossed} &amp; P(x) \\\\ \\hline 0 &amp; 0.25 \\\\ 1 &amp; 0.50 \\\\ 2 &amp; 0.25 \\end{array} \\] Calculate expected value/average number of heads and SD from three tosses. Solution: \\[\\begin{align} E(X) &amp;= 0 \\times .25 + 1 \\times .50 + 2 \\times .25 = 1 \\\\ VAR(X) &amp;= (0 - 1)^2 \\times .25 + (1 - 1)^2 \\times .50 + (2 - 1)^2 \\times .25 \\\\ &amp;= .5 \\\\ SD(X) &amp;= \\sqrt {.5} \\\\ &amp; = .7071 \\end{align}\\] Exercise 2: Be A Better Bettor \\[ \\begin{array}{c|c|lcr} \\text{Roulette} \\\\ \\text{Event} &amp; x &amp; P(x) \\\\ \\hline \\text{Lose} &amp; - \\$ 5 &amp; \\frac{37}{38} \\\\ \\text{Win} &amp; \\$175 &amp; \\frac{1}{38} \\end{array} \\] \\[ \\begin{array}{c|c|lcr} \\text{Craps Game} \\\\ \\text{Event} &amp; x &amp; P(x) \\\\ \\hline \\text{Lose} &amp; - \\$ 5 &amp; \\frac{251}{495} \\\\ \\text{Win} &amp; \\$5 &amp; \\frac{244}{495} \\end{array} \\] Which of the bets is better in the sense of producing higher expected value? Solution: Roulette \\[ E(X) = (- \\$ 5) \\cdot (\\frac{37}{38}) + (\\$175) \\cdot (\\frac{1}{38}) = -\\$.26 \\] Craps Game \\[ E(X) = (- \\$ 5) \\cdot (\\frac{251}{495}) + (\\$5) \\cdot (\\frac{244}{495}) = -\\$.07 \\] \\(\\therefore\\) Craps Game seems to be a better bet since it has the higher expected return. However, both games will generate profit for the casino owner. 5.5 Binomial Probability Distribution A binomial probability distribution results from a procedure that meets the following requirements: The procedure has a fixed number of trials (A trial is a single observation). The trials must be independent, meaning that the outcome of any individual trial doesnt affect the probabilities in the other trials. Each trial must have all outcomes classified into exactly two categories, commonly referred to as success and failure. The probability of a success remains the same in all trials. Notation for binomial probability distribution \\[ \\begin{align} S, F &amp;: \\text {success and failure denote the two possible outcomes from each trial.} \\\\ P(S) &amp;= p \\text{ probability of success in one of the } n \\text { trials} \\\\ P(F) &amp;= q = 1 - p \\text{ probability of failure in one of the } n \\text { trials} \\\\ n &amp;: \\text { number of trials} \\\\ x &amp;: \\text {number of successes in } n \\text { trials}; 0 \\le x \\le n \\\\ \\end{align} \\] Probability Distribution of \\(3\\) independent Trials Probability distribution of the number of successes \\((x)\\) in \\(n = 3\\) independent success/failure trials, each of which is a success with chance \\(p = \\dfrac{1}{6}.\\) \\[ \\begin{array}{rclr} \\text {No. of successes } (x=k) &amp; \\text{pattern} &amp; \\text{chance of pattern} &amp; P(x) \\\\ \\hline 0 &amp; FFF &amp; (5/6) \\cdot (5/6) \\cdot (5/6) &amp; 1 \\cdot (5/6)^3=0.5787 \\\\ \\hline 1 &amp; SFF &amp; (1/6) \\cdot (5/6) \\cdot (5/6) &amp; \\\\ &amp; FSF &amp; (5/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; FFS &amp; (5/6) \\cdot (5/6) \\cdot (1/6) &amp; 3 \\cdot (1/6) \\cdot (5/6)^2 = 0.3472 \\\\ \\hline 2 &amp; SSF &amp; (1/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; SFS &amp; (1/6) \\cdot (5/6) \\cdot (1/6) &amp; \\\\ &amp; FSS &amp; (5/6) \\cdot (1/6) \\cdot (1/6) &amp; 3 \\cdot (1/6)^2 \\cdot (5/6) = 0.0694 \\\\ \\hline 3 &amp; SSS &amp; (1/6) \\cdot (1/6) \\cdot (1/6) &amp; 1 \\cdot (1/6)^3 = 0.0046 \\\\ \\hline &amp; &amp; &amp; \\sum = 1.0000 \\end{array} \\] Binomial Probability Distribution Formula Lets reconsider the table above. \\[ \\begin{array}{rclr} \\text {No. of successes } (x=k) &amp; \\text{pattern} &amp; \\text{chance of pattern} &amp; P(x) \\\\ \\hline 0 &amp; FFF &amp; (5/6) \\cdot (5/6) \\cdot (5/6) &amp; \\underbrace{1}_{\\displaystyle \\binom{3}{0}} \\cdot \\underbrace{(5/6)^3}_{\\displaystyle (1/6)^0 \\cdot (5/6)^3} = 0.5787 \\\\ \\hline 1 &amp; SFF &amp; (1/6) \\cdot (5/6) \\cdot (5/6) &amp; \\\\ &amp; FSF &amp; (5/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; FFS &amp; (5/6) \\cdot (5/6) \\cdot (1/6) &amp; \\underbrace{3}_{\\displaystyle \\binom{3}{1}} \\cdot \\underbrace{(1/6) \\cdot (5/6)^2}_{\\displaystyle (1/6)^1 \\cdot (5/6)^2} = 0.3472 \\\\ \\hline 2 &amp; SSF &amp; (1/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; SFS &amp; (1/6) \\cdot (5/6) \\cdot (1/6) &amp; \\\\ &amp; FSS &amp; (5/6) \\cdot (1/6) \\cdot (1/6) &amp; \\underbrace{3}_{\\displaystyle \\binom{3}{2}} \\cdot \\underbrace{(1/6)^2 \\cdot (5/6)^1}_{\\displaystyle (1/6)^2 \\cdot (5/6)^1} = 0.0694 \\\\ \\hline 3 &amp; SSS &amp; (1/6) \\cdot (1/6) \\cdot (1/6) &amp; \\underbrace{1}_{\\displaystyle \\binom{3}{3}} \\cdot \\underbrace{(1/6)^3}_{\\displaystyle (1/6)^3 \\cdot (5/6)^0} = 0.0046 \\\\ \\hline &amp; &amp; &amp; \\sum = 1.0000 \\end{array} \\] The probability of \\(k\\) successes in \\(n\\) independent trials: \\[ \\begin{align} \\displaystyle P(x) &amp;= \\binom{n}{x}(p)^x(1-p)^{n-x} \\\\ &amp;= \\underbrace{\\dfrac{n!}{x!(n-x)!}}_{\\text{The number of outcomes with} \\\\ \\text{exactly x successes among n trials }} \\times \\underbrace{(p)^x(1-p)^{n-x}}_{\\text{ The probability of x successes among} \\\\ \\text{ n trials for any particular order}} \\end{align} \\] where, \\[ \\begin{cases} x = 0,1,2,......,n \\\\ n = \\text{ number of trials} \\\\ k = \\text{ number of successes among } n \\text{ trials} \\\\ p = \\text{ probability of success in any one trial} \\end{cases} \\] When \\(k = 0,\\) the chance of no successes (in other words, the chance of \\(n\\) failures in a row) is \\[ \\frac{n!}{0!(n)!}p^0(1-p)^{n} = (1-p)^n \\] Question: What is the probability that the first success will be observed after \\(n\\) trials? This means the first success is observed on \\((n+1)^{th}\\) trial. \\[ \\underbrace{F}_{ 1^{st} } FF.......FF \\underbrace{F}_{ n^{th} } \\underbrace{S}_{ n+1 } \\\\ \\underbrace{FFF.......FFF}_{ \\displaystyle \\binom{n}{0} (p)^0(1-p)^n } \\underbrace{S}_{\\times p } \\\\ = (1-p)^n \\cdot p \\\\ \\text{ (This is called geometric distribution.)} \\] Sampling with Replacement A random number generator draws at random with replacement from the ten digits \\(0,1,2,3,4,5,6,7,8,9. \\space\\) Run the generator 20 times. Find the chance that \\(0\\) appears once. Solution: \\[binomial: n=20, p = 0.1, k = 1 \\implies \\binom{20}{1}(0.1)^1(0.9)^{19} = 0.2702\\] Find the chance that \\(0\\) appears at most once. Solution: \\[binomial: n=20, p = 0.1, k = (0,1) \\\\ \\implies \\binom{20}{0}(0.1)^0(0.9)^{20} + \\binom{20}{1}(0.1)^1(0.9)^{19} = 0.3917\\] Find the chance that \\(0\\) appears more than once. Solution: \\[binomial: n=20, p = 0.1, k = (2,3,...,20) \\\\ \\implies 1-P(k=0,1)= (1- 0.3917) = 0.6083.\\] Exercise: Cytomegalovirus (CMV) is a virus that infects one half of young adults. If a random sample of \\(10\\) young adults is taken, find the probability that between \\(30\\%\\) and \\(40\\%\\) (inclusive ) of those sampled will have CMV. Solution: \\[\\begin{align} P((.3)(10) \\le X \\le (.4)(10)) &amp;= P(X=3) + P(X=4) \\\\ &amp;= \\binom{10}{3}(.5)^3(.5)^7 + \\binom{10}{4}(.5)^4(.5)^6 \\\\ &amp;= 0.1172 + 0.2051 \\\\ &amp;= 0.3223 \\end{align}\\] 5.5.1 Expected Value of the Binomial Distribution \\(X:\\) number of successes with probability \\(p\\) of success on each trial. Probability distribution table for \\(X:\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 0 \\\\ \\hline P(x) &amp; p &amp; 1-p \\end{array} \\] \\[ \\begin{align} \\text{Average successes per trial: } E(X) &amp;= 1 \\times p + 0 \\times (1-p) = p \\\\ \\text{Expected total successes from } n \\text { trials: } E(X) &amp;= n \\times [1 \\times p + 0 \\times (1-p)] = np \\end{align} \\] 5.5.2 Standard Deviation of the Binomial Distribution \\(X:\\) number of successes with probability \\(p\\) of success on each trial. Probability distribution table for \\(X:\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 0 \\\\ \\hline P(x) &amp; p &amp; 1-p \\end{array} \\] \\[ \\begin{align} \\text {for one trial} \\\\ SD(X) &amp;= \\sqrt{(1-p)^2 \\times p + (0-p)^2 \\times (1-p)} \\\\ &amp;= \\sqrt{(1-p)^2 \\times p + p^2 \\times (1-p)} \\\\ &amp;= \\sqrt{p(1-p)(1-p+p)} \\\\ &amp;= \\sqrt{p(1-p)} \\\\ \\text {for } n \\text{ trials} \\\\ SD(X) &amp;= \\sqrt{np(1-p)} \\end{align} \\] Example: Calculate expected total number of heads from \\(100\\) tosses \\(X\\) is a binomial distributed random variable with parameters \\(n=100\\) and \\(p=0.5\\) \\[ \\begin{align} P(x=k) &amp;= \\binom{100}{k}(0.5)^k(1-0.5)^{100-k}, k = 0,1,2,...,100 \\\\ \\\\ E(X) &amp;= 100 \\times 0.5 \\\\ &amp;= 50 \\\\ \\\\ SE(X) &amp;= \\sqrt{100 \\times 0.5 \\times 0.5} \\\\ &amp;= 5 \\end{align} \\] Exercise Calculate expected total number of sixes from \\(100\\) rolls \\(X\\) has the binomial distribution with parameters \\(n=100\\) and \\(p=1/6\\) Solution: \\[ \\begin{align} P(X=k) &amp;= \\binom{100}{k}(1/6)^k(1-1/6)^{100-k}, k = 0,1,2,...,100 \\\\ \\\\ E(X) &amp;= 100 \\times 1/6 \\\\ &amp;= 16.7 \\\\ \\\\ SE(X) &amp;= \\sqrt{100 \\times 1/6 \\times 5/6} \\\\ &amp;= 3.73 \\end{align} \\] Empirical Rule Probabilities for falling \\(1,\\) \\(2,\\) and \\(3\\) standard deviations of the mean in a normal distribution. Linear Transformation of Normal Curve | Standardizing with Z-Scores Consider a normally distributed random variable \\(x\\) with mean \\(\\mu\\) and sd \\(\\sigma\\): \\(x \\tilde \\space N(\\mu, \\sigma)\\) Two-step linear transformation of \\(x\\) subtract \\(\\mu\\) from \\(x\\) divide \\((x-\\mu)\\) by \\(\\sigma\\) \\[\\bbox[yellow,5px]{\\color{black}{\\text{standard normal deviate: } z = \\frac {x-\\mu}{\\sigma}}}\\] The Z-score of an observation is defined as the number of standard deviations it falls above or below the mean. If the observation is one standard deviation above the mean, its Z-score is 1. If it is 1.5 standard deviations below the mean, then its Z-score is -1.5. Linear Transformation | Normal Curve to Standard Normal Curve \\[ \\bbox[yellow,5px] { \\color{black}{{\\text {Density at z}} = \\frac {1}{\\sqrt {2\\pi}}\\exp{-\\frac{1}{2}z^2}, -\\infty&lt;z&lt;+\\infty} } \\] Example: Compute and Interpret a \\(z\\)-Score The 2014 draft picks for NBA basketball teams have heights that are approximately normally distributed with mean 79.1 inches and standard deviation 3.0 inches (Source: nbadraft.net). Shabazz Napier was the shortest 2014 draft pick with a height of 72 inches. Find the z-score for 72 inches. What does it mean? \\[ z = \\frac {x - \\mu}{\\sigma} \\\\ z = \\frac {72 - 79.1}{3} \\\\ z = - 2.37 \\] The z-score is 2.37, which means that Napiers height is 2.37 standard deviations less than the mean. Empirical Rules in terms of \\(z\\)-scores Because the z-score of an observation is the number of standard deviations that the observation is from the mean, we can restate the Empirical Rule in terms of \\(z\\)-scores. So, if a distribution is normally distributed, then \\(68\\%\\) of its z-scores lie between 1 and 1 \\(95\\%\\) of its z-scores lie between 2 and 2 \\(99.7\\%\\) of its z-scores lie between 3 and 3 Normal Probability Examples | z-score to percentile Cumulative SAT scores are approximated by a normal model with \\(\\mu = 1500 \\text { and } \\sigma = 300\\). What is the probability that a randomly selected SAT taker scores at least 1630 on the SAT? \\(z = \\frac{x-\\mu}{\\sigma}=\\frac{1630-1500}{300}=\\frac{130}{300}=0.43\\) \\(P(z\\ge0.43)=0.3336\\) The probability that a randomly selected score is at least 1630 on the SAT is 33%. Exercise Peoples IQ scores are normally distributed with mean \\(\\textbf {100}\\) points and standard deviation \\(\\textbf {15}\\) points. Let X be the IQ (in points) of a randomly selected person. Find the probability that a randomly selected person has an IQ less than \\(\\textbf {78}\\) points. greater than \\(\\textbf {117}\\) points. Normal Probability Examples | z-score to percentile Edward earned a 1400 on his SAT. What is his percentile? \\(z = \\frac{x-\\mu}{\\sigma}=\\frac{1400-1500}{300}=\\frac{100}{300}=-0.33\\) \\(P(z\\le-0.33)=0.3707\\) Edward is at the \\(37\\)th percentile. Normal Probability Examples | percentile to z-score Carlos believes he can get into his preferred college if he scores at least in the 80th percentile on the SAT. What score should he aim for? At \\(80th\\) percentile, \\(z = 0.84\\) \\[ \\begin{align} z &amp; = \\frac{x-\\mu}{\\sigma} \\\\ 0.84 &amp; = \\frac{x-1500}{300} \\\\ 0.84 \\times 300 + 1500 &amp; = x \\\\ x &amp; = 1752 \\end{align} \\] The \\(80\\)th percentile on the SAT corresponds to a score of \\(1752\\). Practice Problems The scores on the Wechsler IQ test are normally distributed with mean \\(100\\) points and standard deviation \\(15\\) points. Film director Quentin Tarantino is reported to have a z-score of \\(z = 4\\) (Source: Chicago Tribune). What is his IQ score? A professor gives a test to her calculus students. The scores are approximately normally distributed with mean \\(75\\) points and standard deviation \\(9\\) points. The professor decides to give As to approximately \\(10\\%\\) of the students but not less than 10%. Find the cutoff score for an A. Maria and Roberto took tests on probability in two different sections of prestatistics. Maria scored \\(91\\) points on a test with mean \\(77\\) points and standard deviation 6 points. Roberto scored \\(80\\) points on a test with mean \\(68\\) points and standard deviation \\(4\\) points. The test scores on each test are approximately normally distributed. Find the \\(z\\)-score for Marias test score. What does it mean in this situation? Find the \\(z\\)-score for Robertos test score. What does it mean in this situation? Assuming a typical student in one class knows the material as well as a typical student in the other class, determine whether Maria did relatively better than Roberto. Assuming the sodium levels per serving of the low-salt chips are approximately normally distributed with mean \\(85\\) mg and standard deviation \\(5\\) mg, find the probability that a randomly selected bag would have at least \\(104\\) mg of sodium per serving. What is the probability that a randomly selected data point from a normal distribution can be classified as an outlier? \\[ Q_3 + 1.5 \\times IQR \\le \\text {outlier} \\\\ \\text {outlier} \\le Q_1 - 1.5 \\times IQR \\] "],["normal-probability-distributions.html", "Chapter 6 Normal Probability Distributions 6.1 Normal Distribution 6.2 Sampling Distribution of a Statistic 6.3 Central Limit Theorem", " Chapter 6 Normal Probability Distributions Learning Outcome Compute probability for binomial and normal distributions. In this chapter, we introduce continuous probability distributions, with the focus on normal probability distributions. We will learn how to calculate probabilities from the standard normal distribution and apply that knowledge to solve some practical problems. Finally, we will learn about sampling distribution and use it to state the Central Limit Theorem. 6.1 Normal Distribution Properties of a Normal Curve A normal curve is unimodal and symmetric. The mean is equal to the median. Both are the center of the curve. Probability density function of the normal distribution: \\[P(x) = \\dfrac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2} \\big( \\frac{x-\\mu}{\\sigma} \\big)^2 }, -\\infty&lt;x&lt;+\\infty\\] \\(\\mu\\) is called the location parameter because it determines where the distribution is located on the horizontal axis. \\(\\sigma\\) is called the shape parameter because it determines the shape of the distribution. Linear Transformation of the Nonstandardized Normal Distribution to the Standardized Normal Distribution Recall : A z-score is the number of standard deviations that a given value \\(x\\) is above or below the mean. The \\(z\\) score is calculated by using one of the following: \\[ \\begin{align} z &amp;= \\dfrac{x-\\bar x}{s} \\\\ x = \\bar x - 2s &amp;\\implies z = -2 \\\\ x = \\bar x - s &amp;\\implies z = -1 \\\\ x = \\bar x &amp;\\implies z = 0 \\\\ x = \\bar x + s &amp;\\implies z = +1 \\\\ x = \\bar x + 2s &amp;\\implies z = +2 \\\\ \\end{align} \\] 6.1.1 Standard Normal Distribution The standard normal distribution is a normal distribution with the parameters of \\(\\mu = 0\\) and \\(\\sigma = 1\\). The total area under its density curve is equal to \\(1\\). \\[ \\bbox[yellow,5px] { \\color{black}{{P(z)} = \\frac {1}{\\sqrt {2\\pi}}\\exp{-\\frac{1}{2}z^2}, -\\infty&lt;z&lt;+\\infty} } \\] Finding Areas Between Two \\(z\\) Scores Notation \\[ \\begin{cases} P(a &lt; z &lt; b) \\ \\text{ denotes the probability that the } z \\text{ score is between } a \\text{ and } b \\\\ P(z &gt; a) \\ \\text{ denotes the probability that the } z \\text{ score is greater than } a \\\\ P(z &lt; a) \\ \\text{ denotes the probability that the } z \\text{ score is less than } a \\end{cases} \\] Exercise: \\[ \\begin{align} &amp;1. \\text { Find } P(z &lt; -1) \\\\ &amp;2. \\text { Find } P(z &gt; 2) \\\\ &amp;3. \\text { Find } P(-3 &lt; z &lt; 3) \\\\ \\end{align} \\] Calculation of percentile from \\(z\\) scores Example: Suppose cumulative SAT scores are approximated by a normal model with \\(\\mu = 1500 \\text { and } \\sigma = 300\\). What is the probability that a randomly selected SAT taker scores at least \\(1630\\) on the SAT? \\(z = \\frac{x-\\mu}{\\sigma}=\\frac{1630-1500}{300}=\\frac{130}{300}=0.43\\) \\(P(z\\ge0.43)=0.3336\\) The probability that a randomly selected score is at least \\(1630\\) on the SAT is \\(33\\%\\). In order words, \\(1630\\) is \\((100 - 33) = 73\\) percentile score. Example: Edward earned a \\(1400\\) on his SAT. What is his percentile? \\(z = \\dfrac{x-\\mu}{\\sigma}=\\dfrac{1400-1500}{300}=\\dfrac{100}{300}=-0.33\\) \\(P(z\\le-0.33)=0.3707\\) Edward is at the \\(37\\)th percentile. Calculation of \\(z\\) score from percentile \\(z = 1.645\\) Interpretation: \\(95\\%\\) of the area under the curve is below \\(z = 1.645.\\) Example: Carlos believes he can get into his preferred college if he scores at least in the \\(80\\)th percentile on the SAT. What score should he aim for? At \\(80th\\) percentile, \\(z = 0.84\\) \\[ \\begin{align} z &amp; = \\dfrac{x-\\mu}{\\sigma} \\\\ 0.84 &amp; = \\dfrac{x-1500}{300} \\\\ 0.84 \\times 300 + 1500 &amp; = x \\\\ x &amp; = 1752 \\end{align} \\] the 80th percentile on the SAT corresponds to a score of \\(1752\\). More Exercises The U.S. Air Force requires that pilots have heights between 64 in. and 77 in. Heights of women are normally distributed with a mean of \\(63.7\\) in. and a standard deviation of \\(2.9\\) in. What percentage of women meet that height requirement? To recruit more women pilots in the Air Force, if the height requirements are relaxed to allow middle \\(95\\%\\) of women based on the height distribution \\((N \\sim (63.7, 2.9))\\), what will be the heights of the tallest and shortest women meeting the requirements? A professor gives a test and the scores are normally distributed with a mean of \\(60\\) and a standard deviation of \\(12\\). She plans to curve the scores. If she curves by adding \\(15\\) to each grade, what is the new mean and standard deviation? If the grades are curved so that the grades of \\(B\\) are given to scores above the bottom \\(70\\%\\) and below the top \\(10\\%\\), find the new numerical limits for a grade of \\(B\\). Which method of curving the grades is fairer? What is the probability that when a value is randomly selected from a normal distribution, it is an outlier? Note: outliers are defined as data values that are above \\(Q_3\\) by an amount greater than \\(1.5 \\times IQR\\) or below \\(Q_1\\) by an amount greater than \\(1.5 \\times IQR\\), where \\(IQR\\) is the interquartile range. 6.2 Sampling Distribution of a Statistic The sampling distribution of a statistic represents the distribution of all values of the statistic (e.g. \\(\\text { sample mean, sample proporton, sample variance, etc.}\\)) when all possible samples of the same size \\(n\\) are drawn from the same population. Understanding the concept of a sampling distribution is central to understanding statistical inference. Parameter and Statistics A statistic is a value from our observed data. A parameter is a value that describes the population. \\[ \\begin{array} {l|c} \\text{Name} &amp; \\text{Statistic} &amp; \\text{Parameter} \\\\ \\hline \\text {Proportion} &amp; \\hat p &amp; p \\\\ \\text {Mean} &amp; \\bar x &amp; \\mu \\\\ \\text {Std. Deviation} &amp; s &amp; \\sigma \\\\ \\text {Variance} &amp; s^2 &amp; \\sigma^2 \\\\ \\text {Correlation} &amp; r &amp; \\rho \\\\ \\text {Regression Coefficient} &amp; b &amp; \\beta \\\\ \\end{array} \\] Sampling Distribution of Sample Mean The sampling distribution of the sample mean is the distribution of all values of the sample mean (or the distribution of the variable \\(\\overline X\\)) when all possible samples of the same size \\(n\\) are drawn from the same population (with mean \\(\\mu\\) and variance \\(\\sigma^2\\)). Recall: \\(E(X) = \\dfrac{1}{6}[1 + 2 + 3 + 4 + 5 + 6] = 3.5\\) We can show that \\[ \\begin{cases} E(\\overline X) = \\mu \\\\ Var(\\overline X) = \\dfrac{\\sigma^2}{n} \\end{cases} \\] Proof (optional): \\(E(\\overline X) = \\mu\\) Let \\(X_1, X_2,...,X_n\\) be \\(n\\) independently drawn observations from a population distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\overline X\\) be the mean of these \\(n\\) independent observations: \\[ \\begin{align} \\overline X &amp;= \\frac{X_1 + X_2 +...+ X_n}{n} \\\\ \\\\ E(\\overline X) &amp;= E\\bigg( \\frac{X_1 + X_2 +...+ X_n}{n} \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) E(X_1 + X_2 +...+ X_n) \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [E(X_1) + E(X_2) +...+ E(X_n)] \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [\\mu + \\mu +...+ \\mu] \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [n.\\mu] \\\\ &amp;= \\mu \\\\ \\end{align} \\] Proof (optional): \\(Var(\\overline X) = \\sigma^2/{n}\\) \\[ \\begin{align} \\overline X &amp;= \\frac{X_1 + X_2 +...+ X_n}{n} \\\\ \\\\ Var(\\overline X) &amp;= Var \\bigg( \\frac{X_1 + X_2 +...+ X_n}{n} \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)Var(X_1 + X_2 +...+ X_n \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[Var(X_1) + Var(X_2) +...+ Var(X_n)] \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[\\sigma^2 + \\sigma^2 +...+ \\sigma^2] \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[n.\\sigma^2] \\\\ &amp;= \\frac{\\sigma^2}{n} \\\\ \\sigma^2_{\\overline X} &amp;= \\frac{\\sigma^2}{n} \\\\ SD_{\\overline X} = \\sigma_{\\overline X} &amp;= {\\frac{\\sigma}{\\sqrt n}} \\end{align} \\] Sampling Distribution of a Sample Proportion The sampling distribution of the sample proportion is the distribution of sample proportions (or the distribution of the variable \\(\\mathbf { \\hat p}\\)) with all samples having the same size \\(n\\) drawn from the same population (with mean proportion \\(p\\) and variance \\(p(1-p)\\)). \\[ \\begin{align} E(\\mathbf { \\hat p}) &amp;= p \\\\ Var(\\mathbf { \\hat p}) &amp;= \\dfrac{p(1-p)}{n} \\end{align} \\] Sample Means of 10,000 Trials Sampling Distribution of a Sample Variance The sampling distribution of the sample variance is the distribution of sample variances (i.e. the variable \\(s^2\\)), with all samples having the same sample size \\(n\\) taken from the same population. Sample Means of 10,000 Trials Recall: \\(\\sigma^2 = \\dfrac{1}{6}[(1 - 3.5)^2 + (2 - 3.5)^2 + (3 - 3.5)^2 + (4 - 3.5)^2 + (5 - 3.5)^2 + (6 - 3.5)^2] \\\\ = 2.9\\) Notice that the distribution of sample variances tends to be a distribution skewed to the right. 6.3 Central Limit Theorem When taking a random sample of independent observations from a population with a fixed mean and standard deviation, the distribution of \\(\\bar x\\) approaches a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\), as \\(n\\) increases. Normal Approximation for the Sampling Distribution Three important facts about the distribution of a sample proportion \\(\\bar x\\) Population has a normal distribution or \\(n &gt; 30\\) \\[ \\begin{align} \\text {Mean of all values of } \\bar x &amp;: \\mu_{\\bar x} = \\mu \\\\ \\text {Standard deviation of all values of } \\bar x &amp;: \\sigma_{\\bar x} = \\dfrac{\\sigma}{\\sqrt n} \\\\ z \\text { score conversion of } \\bar x &amp;: z = \\dfrac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt n}} \\\\ \\end{align} \\] Original population is not normally distributed and \\(n \\le 30\\): The distribution of \\(\\bar x\\) cannot be approximated well by a normal distribution, and the methods of this section do not apply. Three important facts about the distribution of a sample proportion \\(\\hat p\\) When \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\) \\[ \\begin{align} \\text {Mean of all values of } \\hat p &amp;: \\mu_{\\hat p} = p \\\\ \\text {Standard deviation of all values of } \\hat p &amp;: \\sigma_{\\hat p} = \\sqrt{\\dfrac{p(1-p)}{n}} \\\\ z \\text { score conversion of } \\hat p &amp;: z = \\dfrac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\\\ \\end{align} \\] When \\(np \\lt 10\\) and \\(n(1-p) \\lt 10\\): The distribution of \\(\\hat p\\) cannot be approximated well by a normal distribution, and the methods of this section do not apply. Problem 1: In the 2012 Cherry Blossom 10 mile run, the average time for all of the runners is \\(94.52\\) minutes with a standard deviation of \\(8.97\\) minutes. The distribution of run times is approximately normal. Find the probability that a randomly selected runner completes the run in less than \\(90\\) minutes. Solution: Because the distribution of run times is approximately normal, we can use normal approximation. \\[ \\begin{align} z &amp;= \\frac{\\bar x-\\mu_{\\bar x}}{\\sigma_{\\bar x}} \\\\ &amp;= \\frac{90-94.52}{8.97/\\sqrt 1} \\\\ &amp;= -0.504 \\\\ \\\\ P(z &lt; -0.504) &amp;= 0.3072 \\end{align} \\] There is a \\(30.72\\%\\) probability that a randomly selected runner will complete the run in less than \\(90\\) minutes. Problem 2: Find the probability that the average of 20 runners is less than 90 minutes. Solution: Here, \\(n = 20 &lt; 30\\), but the distribution of the population, that is, the distribution of run times is stated to be approximately normal. Because of this, the sampling distribution will be normal for any sample size. \\[ \\begin{align} \\sigma_{\\bar x} &amp;= \\frac{\\sigma}{\\sqrt n} = \\frac{8.97}{\\sqrt {20}} = 2.01 \\\\ z &amp;= \\frac{\\bar x-\\mu_{\\bar x}}{\\sigma_{\\bar x}} = \\frac{90-94.52}{2.01}= - 2.25 \\\\ P(z&lt;-0.504) &amp;= 0.0123 \\end{align} \\] There is a \\(1.23\\%\\) probability that the average run time of 20 randomly selected runners will be less than 90 minutes. Problem 3: Find the probability that less than \\(15\\%\\) of the sample of \\(400\\) people will be smokers if the true proportion is \\(20\\%.\\) Solutions: The mean of the sample proportion is the population proportion: \\(\\mu_\\hat p = 0.20.\\) The standard deviation of \\(\\hat p\\) is described by the standard deviation for the proportion: \\[\\sigma_{\\hat p}=\\sqrt \\frac{p(1-p)}{n} = \\sqrt \\frac{0.2(0.8)}{400} = 0.02\\] \\[ \\begin{align} z &amp;= \\frac{\\hat p - \\mu_\\hat p}{\\sigma_\\hat p} = \\frac{0.15 - 0.20}{0.02} = -2.5 \\\\ \\\\ P(z&lt;-2.5) &amp;= 0.0062 \\end{align} \\] Problem 4: \\(13\\%\\) of the US population are left-handed. If an auditorium has \\(15\\) lefty seats, what is the probability that there will not be enough lefty seats for a class of \\(90\\) students (in other words, what is the probability that there will be more than \\(15\\) lefty students in the group)? Solutions: \\[ \\begin{align} \\mu_\\hat p &amp;= 0.13 \\\\ \\hat{p} &amp;= 15/90 = 0.167 \\\\ \\sigma_{\\hat p}&amp;=\\sqrt \\frac{p(1- p)}{n} = \\sqrt \\frac{0.13(0.87)}{90} = 0.035 \\\\ \\\\ z &amp;= \\frac{\\hat p - \\mu_\\hat p}{\\sigma_\\hat p} = \\frac{0.167 - 0.13}{0.035} = 1.06 \\\\ P(\\hat{p}&gt;0.167) &amp;= P(z&gt;1.06) = 0.1446 \\end{align} \\] "],["estimating-parameters-and-determining-sample-sizes.html", "Chapter 7 Estimating Parameters and Determining Sample Sizes 7.1 Estimate a Population Proportion 7.2 Confidence Intervals 7.3 Finding the Sample Size to Estimate a Population Proportion 7.4 Estimating a Population Mean 7.5 Estimating a Population Standard Deviation or Variance", " Chapter 7 Estimating Parameters and Determining Sample Sizes Learning Outcome Perform calculations to estimate parameters using confidence intervals based on the normal distribution and t-distribution. In this chapter, we will use sample data to estimate values of population parameters (such as a population proportions or population mean). We will learn how to construct a confidence interval estimate of a population proportion and interpret such confidence interval estimates. Finally, we will learn how to determine the sample size necessary to estimate a population proportion. 7.1 Estimate a Population Proportion This section presents methods for using a sample proportion \\((\\hat p)\\) to make an inference about the value of the corresponding population proportion \\((p)\\). Point Estimate A point estimate is a single value used to estimate a population parameter. For example, the sample proportion \\(\\hat p\\) is the best point estimate (also called unbiased estimator) of the population proportion \\(p\\). Unbiased Estimator: An unbiased estimator is a statistic that targets the value of the corresponding population parameter in the sense that the sampling distribution of the statistic has a mean that is equal to the corresponding population parameter. For example, the statistic \\(\\hat p\\) targets the population proportion \\(p\\), \\(E(\\mathbf {\\hat p} ) = p.\\) Also, \\(\\hat p\\) is the consistent estimator of \\(p\\) because as the sample size \\(n\\) increases to indefinitely, the resulting sequences of \\(\\hat p\\) converges in probability to \\(p\\). Consistent Estimator \\({T_1, T_2, T_3, ...}\\) is a sequence of estimators for parameter \\(\\theta_0\\), the true value of which is \\(4\\). This sequence is consistent: the estimators are getting more and more concentrated near the true value \\(\\theta_0\\). The limiting distribution of the sequence is a degenerate random variable which equals \\(\\theta_0\\) with probability \\(1\\). An unbiased estimator can be inconsistent - meaning its expected value is equal to the population parameter, but it does not consistently converge to any value as \\(n\\) increases. On the other hand, a consistent estimator can be biased - meaning it consistently converges to the correct population parameter, but its expected value is higher or lower than the targeted value. Source: Wikipedia 7.2 Confidence Intervals A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. A confidence interval or (interval estimate) is a range (or an interval) of values used to estimate the true value of a population parameter. A confidence interval is sometimes abbreviated as CI. The confidence level is the probability \\(1-\\alpha\\) (such as \\(0.95\\), or \\(95\\%\\)) that the confidence interval actually does contain the population parameter, assuming that the estimate process is repeated a large number of times. (The confidence level is also called the degree of confidence, or the confidence coefficient.) \\(\\alpha : \\text { significance level }\\) Constructing a \\(95\\%\\) confidence interval When the sampling distribution of a point estimate can reasonably be modeled as normal, the point estimate we observe will be within \\(1.96\\) standard errors of the true value of interest about \\(95\\%\\) of the time. Thus, a \\(95\\%\\) confidence interval for such a point estimate can be constructed: \\(\\text {point estimate} \\pm 1.96 \\times SE\\) 95% confidence interval Critical Value The values of the \\(z\\) scores at the borderlines of the confidence interval are called the critical values. Critical Value Example: Find the critical value \\(z_{\\alpha/2}\\) corresponding to a \\(95\\%\\) confidence level. \\[ \\begin{array} {|c|c|} \\hline \\text{Confidence Level} &amp; \\alpha &amp; \\text{Critical Value, } z_{\\alpha/2} \\\\ \\hline 90\\% &amp; 0.1 &amp; 1.645 \\\\ \\hline 95\\% &amp; 0.05 &amp; 1.960 \\\\ \\hline 99\\% &amp; 0.01 &amp; 2.575 \\\\ \\hline \\end{array}\\] Generalizing Confidence Interval If the point estimate follows the normal model with standard error \\(SE\\), then a confidence interval for the population parameter is \\[ \\text {point estimate} \\pm z{^{\\star}_{\\alpha/2}} \\times SE \\] where \\(z^\\star\\) , the critical value, depends on the confidence level (\\(CL\\)) selected, and \\(\\alpha = 1-CL\\) Calculating Confidence Intervals Example: The heart patients who receive stents are \\(9\\%\\) more likely to suffer stroke from usage of the stent than those who do not have it. The estimates standard error \\((SE)\\) is \\(0.028\\). Construct a \\(95\\%\\) confidence interval for the change in strole rates from the usage of stent. \\[ \\begin{align} \\text {95% Confidence Interval} &amp;= \\text {point estimate} \\pm 1.96 \\times SE \\\\ &amp;= 0.090 \\pm 1.96 \\times 0.028 \\\\ &amp;=(0.035, 0.145) \\end{align} \\] \\[ \\begin{align} \\text {90% Confidence Interval} &amp;= \\text {point estimate} \\pm 1.645 \\times SE \\\\ &amp;= 0.090 \\pm 1.645 \\times 0.028 \\\\ &amp;=(0.044, 0.136) \\end{align} \\] Simulating Confidence Intervals \\(50\\) samples of size \\(n = 300\\) were drawn from a population with proportion parameter \\(p = 0.30\\). For each sample, a confidence interval was created to capture the true proportion \\(p\\). How many did not capture \\(p = 0.30?\\) Interpretation of Confidence Intervals Suppose, the \\(95\\%\\) confidence interval estimate of the population proportion \\(p\\) is \\(0.405 \\lt p \\lt 0.455\\). \\(\\textbf {Correct }\\) We are \\(95\\%\\) confident that the interval from \\(0.405\\) to \\(0.455\\) actually does contain the true value of the population proportion \\(p\\). \\(\\textbf {Wrong }\\) There is a \\(95\\%\\) chance that the true value of the population proportion \\(p\\) will fall between \\(0.405\\) to \\(0.455\\). 7.2.1 Margin of Error (ME) The margin of error (ME) is the distance between the point estimate and the lower or upper bound of a confidence interval. \\[ \\begin{align} \\text{confidence interval} &amp;= \\text {point estimate} \\pm z{^{\\star}_{\\alpha/2}} \\times SE \\\\ &amp;= \\text {point estimate} \\pm \\text{margin of error} \\end{align} \\] 7.3 Finding the Sample Size to Estimate a Population Proportion We assume that the sampling distribution of sample proportion follows a normal distribution, which allows us to use critical \\(z\\) score to determine the sample size from a given margin of error and confidence level. Example: A pilot study showed that \\(0.5\\%\\) of credit card offers in the mail end up with the person signing up. To be within \\(0.1\\%\\) of the true rate with \\(95\\%\\) confidence, how big does the test mailing have to be? \\[ \\begin{align} ME &amp;= z{^{\\star}_{\\alpha/2}} \\times SE \\\\ ME &amp;= z{^{\\star}_{\\alpha/2}} \\times \\sqrt \\frac{\\hat p \\hat q}{n} \\\\ 0.001 &amp;= 1.96 \\times \\sqrt \\frac{(0.005)(0.995)}{n} \\\\ (0.001)^2 &amp;= (1.96)^2 \\times \\frac{(0.005)(0.995)}{n} \\\\ n &amp;= (1.96)^2 \\times \\frac{(0.005)(0.995)}{(0.001)^2} \\\\ n &amp;= 19112 \\end{align} \\] 7.4 Estimating a Population Mean The main goal of this section is to make an inference about the population mean \\((\\mu)\\) from the mean \\((\\bar x)\\) of a sample drawn from the same population. Therefore, \\(\\bar x\\) is the point estimate of the population \\(\\mu\\). 7.4.1 Estimating a Population Mean When \\(\\sigma\\) Is Known It is extremely rare that we want to estimate an unknown value of a population mean \\(\\mu\\) but we somehow know the value of the population standard deviation \\(\\sigma\\). If we somehow do know the value of \\(\\sigma\\), the confidence interval is constructed using the standard normal distribution. Confidence interval estimate of the true population mean \\(\\mu\\): \\[ ME = z_{\\alpha/2}.\\frac{\\sigma}{\\sqrt{n}} \\text { ; use with known } \\sigma \\] \\[ n = 15, \\bar x = 30.9, \\sigma = 2.9, CL = 95\\% \\\\ ME = 1.96.\\frac{2.9}{\\sqrt{15}} = 1.46760 \\\\ \\bar x - ME &lt; \\mu &lt; \\bar x + ME \\\\ 30.9 - 1.46760 &lt; \\mu &lt; 30.9 + 1.46760 \\\\ 29.4 &lt; \\mu &lt; 32.4 \\] 7.4.2 Estimating a Population Mean When \\(\\sigma\\) Is Not Known \\(\\text {The sample mean } \\bar x \\text { is the best estimate of the population mean } \\mu.\\) Normality: The method for finding a confidence interval estimate of \\(\\mu\\) is robust against a departure from normality, which means that the normality requirement is relaxed. The distribution need not be perfectly normal, but it has to be symmetric with one mode. In this case, population standard deviation \\(\\sigma\\) is estimated from the sample. The sampling distribution employed to calculate the confidence interval of \\(\\mu\\) from sample mean \\(\\bar x\\) is called \\(\\text{Student } t \\text{ Distribution}.\\) \\(\\text {Student } t- \\text { Distribution}\\) According to the Central Limit Theorem, the sampling distribution of a statistic (e.g. sample mean) will follow a normal distribution, as long as the sample size is sufficiently large \\((n&gt;30)\\). But sample sizes are sometimes small, and often we do not know the standard deviation of the population. When either of these occurs, statisticians rely on the distribution of the \\(\\text {t-statistic (t-score)}\\) with the degrees of freedom \\((n-1)\\), \\(t = \\dfrac {\\bar x - \\mu}{s/\\sqrt n}\\) \\(\\text{Degrees of Freedom} = n - 1\\) \\(\\text {t-distribution }\\) is determined by its degrees of freedom. The degrees of freedom refers to the number of independent observations in a set of data. Properties of the \\(t \\text {-Distribution:}\\) The \\(\\text {Student } t\\) distribution has the same general symmetric shape as the standard normal distribution, but with more variability, which is typical of sampling distributions used for small sample size. As the sample size \\(n\\) gets larger, the \\(t\\) distribution gets closer to the standard normal distribution. The mean of the distribution is equal to \\(0\\). Unlike the variance of the standard normal distribution \\((\\sigma^2 = 1)\\), The variance of \\(t\\) distribution depends on the sample size \\((n)\\) and is equal to \\(\\frac {v}{v-2}\\), where \\(v\\) is the DF and \\(v \\ge 2\\). 7.4.3 \\(t \\text { Confidence Interval}\\) Find a \\(95\\%\\) confidence interval for mirex concentrations in salmon. \\[ \\begin{align} n &amp;= 150 \\\\ \\bar x &amp;= 0.0913 \\space ppm \\\\ s &amp;= 0.0495 \\space ppm \\\\ df &amp;= 150 - 1 = 149 \\\\ \\\\ SE(\\bar x) &amp;= \\frac{0.0495}{\\sqrt{150}} = 0.0040 \\\\ t^*_{149} &amp;= 1.976 \\\\ \\end{align} \\] \\[ \\begin{align} \\text {Confidence Interval of } \\bar x: \\\\ &amp;\\bar x \\pm t^*_{149} \\times SE(\\bar x) \\\\ &amp;= 0.0913 \\pm 1.976 \\times 0.0040 \\\\ &amp;= 0.0913 \\pm 0.0079 \\\\ &amp;= (0.0834, 0.0992) \\end{align} \\] 7.4.4 Finding the Sample Size to Estimate a Population Mean Should you buy a movie download accelerator? To test the download times, find the minimum number of downloads that you need to do during a free trial period to obtain a \\(95\\%\\) CL with a ME &lt; \\(8\\) minutes. Given, \\(\\sigma = 10 \\space min\\) First, calculate \\(n\\) using \\(z-score\\) \\[ \\begin{align} ME &amp;&lt; 8 \\\\ \\\\ \\text {with } z^* &amp;= 2 \\text { at 95% CL} \\\\ \\\\ 2 \\times \\frac {10}{\\sqrt n} &amp;&lt; 8 \\\\ n &amp;&gt; 6.25 \\\\ n &amp;\\approx 7 \\\\ \\\\ \\end{align} \\] Sample size turns out to be too small for normal approximation. Therefore, re-estimate \\(n\\) using \\(t-score\\). Second, find the critical value of \\(t_{\\alpha/2}\\) corresponding to a \\(95\\%\\) confidence level, given that \\(n = 6\\). \\[ \\begin{align} df &amp;= 7 - 1 = 6 \\\\ \\\\ \\text {with } t^*_6 &amp;= 2.447 \\text { at 95% CL} \\\\ \\\\ 2.447 \\times \\frac {10}{\\sqrt n} &amp;&lt; 8 \\\\ n &amp;&gt; 9.36 \\\\ n &amp;\\approx 10 \\\\ \\\\ \\end{align} \\] Hence, at least 10 trial downloads need to be run to make sure ME remains less than \\(8\\) min. Choosing between Student \\(t\\) and \\(z\\) Distributions \\[ \\begin{array}{l|c} \\text{conditions} &amp; \\text{method} \\\\ \\hline \\sigma \\text{ not known; normally distributed population or n &gt; 30} &amp; t \\\\ \\hline \\sigma \\text{ known; normally distributed population or n &gt;30} &amp; z \\end{array} \\] 7.5 Estimating a Population Standard Deviation or Variance This section presents methods for using a sample standard deviation \\(s\\) (or a sample variance \\(s^2\\)) to estimate the value of the corresponding population standard deviation \\(\\sigma\\) (or population variance \\(\\sigma^2\\)). Point Estimate: The sample variance \\(s^2\\) is the best point estimator of the population variance \\(\\sigma^2\\). The sample standard deviation \\(s\\) is commonly used as a point estimate of \\(\\sigma\\), even though it is a biased estimator. Confidence Interval: When constructing a confidence interval estimate of a population standard deviation (or population variance), we construct the confidence interval using the \\(\\chi^2 \\text{ distribution}\\). Chi-Squared \\(\\chi^2\\) Distribution In a normally distributed population with variance \\(\\sigma^2\\), if we randomly select independent samples of size \\(n\\) and, for each sample, compute the sample variance \\(s^2\\), the sample statistic \\(\\chi^2 = (n -1)s^2/{\\sigma^2}\\) has sampling distribution called the chi-squared distribution, \\[ \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2} \\] \\(\\text{Degrees of freedom: df } = n -1\\) \\(\\chi^2 \\text{ distribution }\\) is skewed to the right, unlike normal and student \\(t\\) distributions. \\(\\chi^2 \\ge 0\\) The chi-squared distribution is different for each number of degrees of freedom. As the degrees of freedom increases, chi-squared distribution approaches a normal distribution. 7.5.1 Confidence Interval for Estimating a Population Standard Deviation or Variance Confidence Interval for the Population Variance \\(\\sigma^2\\) \\[ \\frac{(n-1)s^2}{\\chi^2_L} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_R} \\] Confidence Interval for the Population Variance \\(\\sigma\\) \\[ \\sqrt{\\frac{(n-1)s^2}{\\chi^2_L}} &lt; \\sigma &lt; \\sqrt{\\frac{(n-1)s^2}{\\chi^2_R}} \\] Example: Confidence Interval for Estimating a Population Standard Deviation or Variance \\[ \\begin{align} &amp;s = 14.29263 \\\\ &amp;n = 22 \\\\ &amp;CL = 95\\% \\\\ \\\\ &amp;\\frac{(n-1)s^2}{\\chi^2_L} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_R} \\\\ \\\\ or, \\ &amp;\\frac{21.(14.29263)^2}{10.283^2} &lt; \\sigma^2 &lt; \\frac{21.(14.29263)^2}{35.479^2} \\\\ \\\\ or, \\ &amp;120.9 &lt; \\sigma^2 &lt; 417.2 \\\\ or, \\ &amp;11.0 &lt; \\sigma &lt; 20.4 \\end{align} \\] "],["hypothesis-testing.html", "Chapter 8 Hypothesis Testing 8.1 Hypothesis Testing 8.2 Inference for a Single Proportion 8.3 \\(t \\text {-test}\\) | Testing Hypothesis About \\(\\mu\\) with \\(\\sigma\\) Not Known 8.4 \\(\\chi^2 \\text{-test}\\) | Testing Hypothesis About a Variance", " Chapter 8 Hypothesis Testing Learning Outcome Perform hypotheses testing involving a sample mean, proportion, and standard deviation or variance. This chapter introduces the statistical method of hypothesis testing to test a given claim about a population parameter, such as proportion, mean, standard deviation, or variance. This method combines the concepts learned in the previous chapters, including sampling distribution, standard error, critical scores, and probability theory. 8.1 Hypothesis Testing In statistics, a hypothesis is a claim or statement about a property of a population. A hypothesis test (or test of significance) is a procedure for testing a claim about a property of a population. The null hypothesis (\\(H_0\\)) is a statement that the value of a population parameter (such as proportion, mean, or standard deviation) is equal to some claimed value. The alternative hypothesis (\\(H_A\\)) is a statement that the parameter has a value that somehow differs from the null hypothesis. Example: Probability of getting head from a single toss of coin, \\(p = 0.5\\). Therefore, expected value of the number of heads from \\(20\\) tosses = \\(10\\). Suppose, you have tossed a coin \\(20\\) times and seen \\(15\\) heads, \\(\\hat p = 0.75\\). Is the coin fair, or is it biased towards heads? Null and Alternative Hypotheses Null hypothesis \\((H_0)\\): states that any deviation from what was expected is due to chance error (i.e. the coin is fair). Alternative hypothesis \\((H_A)\\): asserts that the observed deviation is too large to be explained by chance alone (i.e. the coin is biased towards heads). \\[ H_0: p = 0.5 \\\\ H_A: p &gt; 0.5 \\] Now, what is the probability of \\(p \\ge 0.75?\\) From normal approximation of the sampling distribution of \\(\\hat p\\), \\[ \\begin{align} p &amp;= 0.5 \\\\ se &amp;= \\sqrt{(0.5)(0.5)/20} = 0.112 \\\\ z &amp;= (0.75 - 0.50)/0.112 = 2.236 \\\\ \\\\ P(z\\ge2.236) &amp;= 0.0127 \\Leftarrow \\text {this probability is also called p-value} \\end{align} \\] Hence, there is only \\(1.27\\%\\) probability that observing \\(15\\) heads from \\(20\\) tosses is merely a chance. Then the question is, if the coin is indeed fair, is the p-value too small? Interpretation of \\(\\text{p-value}\\) A \\(\\text{p-value}\\) is the probability of obtaining the observed effect (or larger) under a null hypothesis. Thus, a \\(\\text{p-value}\\) that is very small indicates that the observed effect is very unlikely to have arisen purely by chance, and therefore provides evidence against the null hypothesis. It has been common practice to interpret a \\(\\text{p-value}\\) by examining whether it is smaller than particular threshold values or significance level. In particular, \\(\\text{p-values}\\) less than \\(5\\%\\) are often reported as statistically significant, and interpreted as being small enough to justify rejection of the null hypothesis. By definition, the significance level \\(\\alpha\\) is the probability of mistakenly rejecting the null hypothesis when it is true. \\[\\textbf {Significance level } \\alpha = P \\textbf { (rejecting } H_0 \\textbf { when } H_0 \\textbf { is true)} \\] In common practice, \\(\\alpha\\) is set at \\(10\\%, 5\\%\\) or \\(1\\%\\). In the coin toss example: p-value = \\(1.27\\%\\) which is less than the \\(5\\%\\) significance level. Therefore, the result is statistically significant. Conclusion: The coin is biased towards heads. Critical Value Method In a hypothesis test, the critical value(s) separates the critical region (where we reject the null hypothesis) from the values of the test statistic that do not lead to rejection of the null hypothesis. With the critical value method of testing hypothesis, we make a decision by comparing the test statistic to the critical value(s). One-sided and two-sided tests If the researchers are only interested in showing an increase or a decrease, but not both, use a one-sided test. If the researchers would be interested in any difference from the null value - an increase or decrease - then the test should be two-sided. After observing data, it is tempting to turn a two-sided test into a one-sided test. Hypotheses must be set up before observing the data. If they are not, the test must be two-sided. 8.1.1 Type I and Type II Errors When testing a null hypothesis, sometimes the test comes to a wrong conclusion by rejecting it or failing to reject it. There are two kinds of errors: type I and type II errors. \\(\\textbf {Type I error} :\\) The error of rejecting the null hypothesis when it is actually true. \\(\\alpha = P (\\textbf{type I error}) = P (\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true } )\\) The probability of \\(\\text {Type I}\\) can be minimized by choosing a smaller \\(\\alpha\\). \\(\\textbf {Type II error} :\\) The error of failing to reject the null hypothesis when it is actually false. \\(\\beta = P (\\textbf{type II error}) = P (\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false} )\\) The probability of \\(\\text {Type II}\\) can be minimized by choosing a larger sample size \\(n\\). Power of a Hypothesis Test The power of a hypothesis test is the probability \\(1-\\beta\\) of rejecting a false null hypothesis. The value of the power is computed by using a particular significance level \\(\\alpha\\) and a particular value of the population parameter that is an alternative to the value of assumed true in the null hypothesis. In practice, statistical studies are commonly designed with a statistical power of at least \\(80%\\). Figure: Statistical Power Post-hoc Power Calculation for One Study Group vs. Population Suppose, \\[ H_0 : p = P_0 \\\\ H_A : p \\ne P_0 \\\\ \\] \\[ \\begin{align} P_0 &amp;= \\text{proportion of population} \\\\ P_1 &amp;= \\text{proportion observed from the data (an alternative population)} \\\\ N &amp;= \\text{sample size} \\\\ \\alpha &amp;= \\text{probability of type I error} \\\\ \\beta &amp;= \\text{probability of type II error} \\\\ z &amp;= \\text{critical z score for a given } \\alpha \\text { or } \\beta \\end{align} \\] Suppose, \\(P_1\\) is an alternative to the value assumed in \\(H_0\\). Under \\(H_0\\), \\[ P&#39;_0 = P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\] Under \\(H_A\\), \\[ \\therefore z_{\\beta} = \\dfrac{P&#39;_0 - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} = \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\\\ \\\\ P(\\textbf{Type II error}) = \\beta = \\Phi \\left \\{ \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\right \\} \\\\ \\] \\[ \\begin{align} \\text{where,} \\\\ Q_0 &amp;= 1 - P_0 \\\\ Q_1 &amp;= 1 - P_1 \\\\ \\Phi &amp;= \\text{cumulative normal distribution function} \\end{align} \\] Example: Calculate statistical power for various alternative hypotheses. \\[ Suppose, \\begin{cases} H_0: p = 0.5 \\\\ H_A: p \\ne 0.5 \\\\ P(\\text{type I error}) = \\alpha = 0.05 \\\\ \\text{Critical z score, } z_{1 - \\alpha/2} = 1.96 \\\\ N = 14 \\\\ \\end{cases} \\] \\[ \\begin{array}{r|r|r} P_1 &amp; \\Phi(z_{\\beta}) = \\beta &amp; 1-\\beta \\\\ \\hline 0.6 &amp; \\Phi(1.2367) = 0.8919 &amp; 0.1081 \\\\ 0.7 &amp; \\Phi(0.5055) = 0.6934 &amp; 0.3066 \\\\ 0.8 &amp; \\Phi(-0.3562) = 0.3608 &amp; 0.6392 \\\\ 0.9 &amp; \\Phi(-1.7222) = 0.0425 &amp; 0.9575 \\\\ \\hline \\end{array} \\] Example: Sample size calculation to achieve power (when \\(P_0\\) and \\(P_1\\) are known) \\[ \\begin{align} z_{\\beta} = \\Phi^{-1}(\\beta) &amp;= \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\\\ \\\\ z_{1-\\alpha/2} &amp;= 1.96 \\\\ 1- \\beta &amp;= 0.8 \\\\ \\\\ \\Phi^{-1}(0.2) = -0.84 &amp;= \\dfrac{\\Bigg( 0.5 + 1.96 \\cdot \\sqrt{\\dfrac{(0.5) (0.5)}{N}} \\Bigg) - 0.9}{\\sqrt{\\dfrac{(0.9)(0.1)}{N}}} \\\\ \\implies N &amp;= \\Bigg( \\dfrac{1.96\\sqrt{0.25} + 0.84\\sqrt{0.09}}{0.4} \\Bigg)^2 \\\\ &amp;\\approx 10 \\end{align} \\] Example: Sample size calculation to achieve power (when \\(P_0\\) and \\(P_1\\) are unknown) \\[ N = \\Bigg( \\dfrac{z_{1-\\alpha/2} + z_{1-\\beta}}{ES} \\Bigg)^2 \\\\ \\] where, \\[ \\text{effect size, } ES = \\dfrac{|P_1-P_0|}{\\sqrt{P_0Q_0}} \\] Statistical power and design of experiment: When designing an experiment, it is essential to determine the minimize sample size that would be needed to detect an acceptable difference between the true value of the population parameter and what is observed from the data. A \\(5\\%\\) significance level \\((\\alpha)\\) and a statistical power of at least \\(80\\%\\) are common requirements for determining that a hypothesis test is effective. Formal Test of Hypothesis Follow these seven steps when carrying out a hypothesis test. State the name of the test being used. Verify conditions to ensure the standard error estimate is reasonable and the point estimate follows the appropriate distribution and is unbiased. Write the hypotheses and set them up in mathematical notation. Identify the significance level \\(\\alpha\\). Calculate the test statistics (e.g. \\(z\\)), using an appropriate point estimate of the paramater of interest and its standard error. \\[\\text{test statistics} = \\frac{\\text{point estimate - null value}}{\\text{SE of estimate}}\\] Find the \\(\\text{p-value}\\), compare it to \\(\\alpha\\), and state whether to reject or not reject the null hypothesis. Write your conclusion in context. 8.2 Inference for a Single Proportion Conditions of the sampling distributions of \\(\\hat p\\) being nearly normal The sampling distribution for \\(\\hat p\\), taken from a sample of size \\(n\\) from a population with a true proportion \\(p\\), is nearly normal when the sample observations are independent and we expect to see at least \\(10\\) successes and \\(10\\) failures in our sample, i.e. \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\). This is called the success-failure condition. If the conditions are met, then the sampling distribution of \\(\\hat p\\) is nearly normal with mean \\(\\mu_{\\hat p} = p\\) and standard deviation \\(\\sigma_{\\hat p} = \\sqrt \\frac{p(1-p)}{n}\\). Example 1: The DMV claims that \\(80\\%\\) of all drivers pass the driving test. In a survey of \\(90\\) teens, only \\(61\\) passed. Is there evidence that teen pass rates are significantly below \\(80\\%?\\) Lets say, \\(p\\) is the true population proportion. \\[ \\begin{align} \\text {One-tailed test} &amp;:\\\\ H_0&amp;: p = 0.80 \\\\ H_A&amp;: p &lt; 0.80 \\end{align} \\] Verify success-failure condition: \\[ \\begin{align} np \\ge 10 \\rightarrow 90 \\times 0.80 \\ge 10 \\\\ n(1-p) \\ge 10 \\rightarrow 90 \\times (1-0.80) \\ge 10 \\end{align} \\] Therefore, the conditions for a normal model are met. Now, \\[ \\begin{align} \\hat p &amp;= \\frac {61}{90} = 0.678 \\\\ \\\\ SD(\\hat p) &amp;= \\sqrt \\frac{p_0q_0}{n} = \\sqrt \\frac{(0.80)(0.20)}{90} = 0.042 \\\\ \\\\ z &amp;= \\frac {0.678-0.80}{0.042} = -2.90 \\\\ \\\\ p\\text{-value} &amp;= P(z &lt; -2.90) = 0.002 &lt; 0.05 \\end{align} \\] Hence, we reject \\(H_0\\). Teen pass rate is significantly below population pass rate. Example 2: Under natural conditions, \\(51.7\\%\\) of births are male. In Punjab Indias hospital \\(56.9\\%\\) of the \\(550\\) births were male. Is there evidence that the proportion of male births is significantly different for this hospital? \\[ \\begin{align} \\text {Two-tailed test} &amp;:\\\\ H_0&amp;: p = 0.517 \\\\ H_A&amp;: p \\ne 0.517 \\end{align} \\] Verify success-failure condition: \\[ \\begin{align} np \\ge 10 \\rightarrow 550 \\times 0.517 \\ge 10 \\\\ n(1-p) \\ge 10 \\rightarrow 550 \\times (1-0.517) \\ge 10 \\end{align} \\] \\[ \\begin{align} \\hat p &amp;= 0.569 \\\\ \\\\ SD(\\hat p) &amp;= \\sqrt \\frac{p_0q_0}{n} = \\sqrt \\frac{(0.517)(1-0.517)}{550} = 0.0213 \\\\ \\\\ z &amp;= \\frac {0.569-0.517}{0.0213} = 2.44 \\\\ \\\\ p\\text{-value} &amp;= 2 \\times P(z &gt; 2.44) = 2 \\times 0.0073 = 0.0146 &lt; 0.05 \\end{align} \\] Hence, we reject \\(H_0\\). Male birth rate is significantly higher at the hospital than the natural birth rate. 8.3 \\(t \\text {-test}\\) | Testing Hypothesis About \\(\\mu\\) with \\(\\sigma\\) Not Known Example 1: Average weight of a mice population of a particular breed and age is \\(30 \\text{ gm}\\). Weights recorded from a random sample of \\(5\\) mice from that population are \\({31.8, 30.9, 34.2, 32.1, 28.8}.\\) Test whether the sample mean is significantly greater than the population mean. \\[ \\begin{align} H_0&amp;: \\mu = 30 \\\\ H_A&amp;: \\mu &gt; 30 \\\\ \\\\ \\bar x &amp;= 31.56 \\\\ s &amp;= 1.9604 \\\\ SE(\\bar x) &amp;= 1.9604/\\sqrt 5 = 0.8767 \\\\ \\\\ t &amp;= (31.56 - 30)/0.8767 = 1.779 \\\\ df &amp;= (5 -1) = 4 \\\\ p-value &amp;= 7.5\\% &gt; 5\\% \\end{align} \\] Conclusion: \\(H_0\\) cannot be rejected. The sample mean is not significantly greater than the population mean. Example 2: EPA recommended mirex screening is 0.08 ppm. A study of a sample of 150 salmon found an average mirex concentration of 0.0913 ppm with a std. deviation of 0.0495 ppm. Are farmed salmon contaminated beyond the permitted EPA level? Also, find a \\(95\\%\\) confidence interval for the mirex concentration in salmon. \\[ \\begin{align} H_0&amp;: \\mu = 0.08 \\\\ H_A&amp;: \\mu &gt; 0.08 \\\\ \\\\ \\bar x &amp;= 0.0913 \\\\ s &amp;= 0.0495 \\\\ SE(\\bar x) &amp;= 0.0495/\\sqrt {150} = 0.0040 \\\\ \\\\ t_{149} &amp;= \\dfrac{\\bar x - \\mu}{SE(\\bar x)} = \\dfrac{(0.0913 - 0.08)}{0.0040} = 2.795 \\\\ df &amp;= (150 -1) = 149 \\\\ p-value &amp;= P(t_{149}&gt;2.795)= 0.29\\% &lt; 5\\% \\end{align} \\] Conclusion: Reject \\(H_0\\). The sample mean mirex level significantly higher that the EPA screening level. 8.4 \\(\\chi^2 \\text{-test}\\) | Testing Hypothesis About a Variance Listed below are the heights (cm) for the simple random sample of female supermodels. Use a \\(0.01\\) significance level to test the claim that supermodels have heights with a standard deviation that is less than \\(\\sigma=7.5 \\text { cm}\\) for the population of women. Does it appear that heights of supermodels vary less than heights of women from the population? \\[ \\text{178, 177, 176, 174, 175, 178, 175, 178} \\\\ \\text{178, 177, 180, 176, 180, 178, 180, 176} \\\\ s^2 = 3.4 \\] \\[ \\begin{align} H_0: \\sigma^2 = 56.25 \\\\ H_A: \\sigma^2 &lt; 56.25 \\\\\\\\ \\chi^2 = (n-1)\\frac{s^2}{\\sigma^2} &amp;= (15)\\frac{(3.4)}{(56.25)} \\\\ &amp;= 0.907 \\\\ \\\\ \\end{align} \\] From \\(\\chi^2\\) table, \\[ \\text {The critical value of } \\chi^2 = 5.229 \\text { at } \\alpha = 0.01. \\\\ \\] Hence, we reject \\(H_0\\). Confidence Interval Calculation: \\[ \\sqrt{ \\dfrac{(n-1)s^2}{\\chi_R^2} } &lt; \\sigma &lt; \\sqrt{ \\dfrac{(n-1)s^2}{\\chi_L^2} } \\\\ \\sqrt{ \\dfrac{(16-1)3.4}{30.578} } &lt; \\sigma &lt; \\sqrt{ \\dfrac{(16-1)3.4}{5.229} } \\\\ 1.3 \\text{ cm } &lt; \\sigma &lt; 3.1 \\text { cm } \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
