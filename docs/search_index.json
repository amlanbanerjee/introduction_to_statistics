[["index.html", "Introduction to Statistics Chapter 1 Course Information 1.1 Course Description 1.2 Course Outcomes", " Introduction to Statistics Amlan Banerjee, Ph.D. 2022-09-15 Chapter 1 Course Information The purpose of this elementary statistics course is to introduce students to the relationship between statistics and the world through use of a wide variety of real applications that bring life to theory and methods. 1.1 Course Description The course covers sampling methods, classification of data, probability, frequency and probability distributions, confidence intervals, tests of statistical significance, and simple regression and correlation. 1.2 Course Outcomes Upon successful completion of this course, the student will be able to: Interpret quantitative data using graphs and descriptive statistics with emphasis on histograms and boxplots. Compute measures of expectation and variation for a discrete probability distribution Compute probability for a binomial and normal distribution. Perform calculations to estimate parameters using confidence intervals based on the normal distribution and \\(t\\)-distribution. Perform hypotheses testing involving a sample mean, proportion, and standard deviation/variance Perform hypotheses testing involving two sample means (independent and dependent), two population proportions, and two standard deviations/variances. Construct linear regression models and correlation coefficients from datasets. "],["introduction.html", "Chapter 2 Introduction 2.1 What is Statistics? 2.2 Statistical Thinking 2.3 Types of Data or Variable 2.4 Sampling Methods 2.5 Bias and Precision 2.6 Sampling Errors 2.7 Observational Studies 2.8 Experimental Design", " Chapter 2 Introduction Learning Outcome: Select a suitable sampling design {simple random, systematic, stratified, cluster}, given information about the observational study or experiment. The chapter introduces various data types, sampling techniques, sampling errors, two main types of statistical studies, namely observational studies and experiments. Also discussed here are their benefits and drawbacks and how to design them well. 2.1 What is Statistics? The science of planning studies and experiments; obtaining data; and organizing, summarizing, presenting, analyzing, and interpreting those data and then drawing conclusions based on them. Application of statistics is literally everywhere - business, finance, engineering, health science, social science, environmental science, politics, education, and so on. 2.2 Statistical Thinking Statistics is an investigative process that has five steps: Formulate a precise question about one or more variables. Will the COVID vaccine be effective on children (ages &lt; 5 years)? Design a plan to answer the question. Construct a theory or hypothesis based on existing knowledge, say laboratory experiments, or similar studies in the past. Who will be recruited to the clinical trial (target population)? What kind of data needs to be collected (age, sex, medical history, etc.)? How the data will be collected (sampling methods)? How the privacy of the participants will be protected? Collect the data. Describe and analyze the data. Analyze the data to test your theory or hypothesis. Draw a conclusion from the data about the question and communicate the results with the stakeholders. Definitions Data are collections of observations, such as measurements or survey responses. Variable is a characteristics of the individuals to be measured or observed. Population is the complete collection of all measurements or data that are being considered. Typically, a population is the complete collection of data that we would like to make inference about. Census is the collection of data from every member of the population. Sampling Frame is a numbered list of all the individuals in the population from which a sample is drawn. Sample is a sub-collection of members selected from a population. Population Parameter is a numerical measurement describing some characteristics of a population. Sample Statistic is a numerical measurement describing some characteristics of a sample. Example (parameter vs. statistic): There are \\(17,246,372\\) high school students in the U.S. In a study of \\(8505\\) U.S. high school students \\(16\\) years of age or older, \\(44.5\\%\\) of them said that they texted while driving at least once during the previous \\(30\\) days. Parameter: What percent of the population texted while driving? (unknown) Statistic: \\(44.5\\%\\) 2.3 Types of Data or Variable 1) Categorical (or Qualitative) - consist of names or labels (not numbers that represent counts or measurements) Levels of Measurement of Qualitative Data - Nominal (unordered): the data fall into categories that have no particular order or ranking in relation to each other, e.g., color (blue, green, red,…), gender (male, female), nationality (American, Canadian, Mexican,…) - Ordinal (ordered): values have a natural order to ranking, but differences either can’t be found or are meaningless e.g., temperature (low, medium, high), exam grade (A, B, C, D, F), satisfaction (high, neutral, low) 2) Numerical (or Quantitative) - consist of numbers representing measurements or counts. - Continuous: a subject or observation takes a value from an interval of real numbers, e.g., weight, height, age, etc. Continuous (numerical) data result from infinitely many possible quantitative values, where the collection of values is not countable, such as the lengths of distances from \\(0\\) inch to \\(12\\) inch. - Discrete: a subject or observation takes certain values from a finite set, e.g. population, traffic volume, etc. Discrete data result when values are quantitative and the number of values is finite, or “countable”, such as the number of tosses of a coin before getting tails. Levels of Measurement of Quantitative Data Interval Variables: these variables are measured along a continuum, and they have the property that equal differences between measures represent equal differences in the values of the variable. Therefore, differences are meaningful, but there is no natural zero starting point at which none of the quantity is present and ratios are meaningless. For example, temperature is measured in degrees Celsius. So the difference between \\(20^\\circ C\\) and \\(30^\\circ C\\) is the same as \\(30^\\circ C\\) to \\(40^\\circ C\\). However, \\(0^\\circ C\\) does not mean there is no temperature. Also, \\(\\dfrac{40^\\circ C}{20^\\circ C} = 2\\) does not mean \\(40^\\circ C\\) is twice the warmer than \\(20^\\circ C\\). Similarly the years 2021 and when you were born, say 1981, can be arranged in order, and the difference of \\(40\\) years can be found and is meaningful. However, time did not begin in year \\(0\\), so the year \\(0\\) is arbitrary instead of being a natural zero starting point representing “no time”. Ratio Variables: these variables have all the properties of interval variables, but in addition have the property that there is a natural zero starting point (where zero indicates that none of the quantity is present) and the ratios make sense. Examples of ratio variables include height, mass, distance, time etc. The name “ratio” reflects the fact that you can use the ratio of measurements. So, for example, a distance of \\(10\\) meters is twice the distance of \\(5\\) meters, and the measurement of distance starts at \\(0\\). 2.4 Sampling Methods Sampling from a Population Because populations are often very large, a common objective of the use of statistics is to obtain data from a sample and then use those data to form a conclusion about the population. Example: Identify the Variable, Sample, and Population of a Study In a poll of \\(1000\\) randomly selected American adults, \\(48\\%\\) of respondents said that they strongly disapprove of the way Congress is doing its job. The study then made an inference about all American adults. Define the variable of the study. Identify the sample. Identify the population. 2.4.1 Simple Random Sampling (SRS) A simple random sample of size \\(n\\) is a sample chosen by a method in which each collection of \\(n\\) population items is equally likely to make up the sample. Example: A physical education professor wants to study the physical fitness levels of \\(20,000\\) students enrolled at her university. She obtains a list of all \\(20,000\\) students, numbered from \\(1\\) to \\(20,000\\) and uses a computer random number generator to generate 100 random integers between \\(1\\) and \\(20,000\\), then invites the \\(100\\) students corresponding to those numbers to participate in the study. Is this a simple random sample? Solution: Yes, this is a simple random sample since any group of \\(100\\) students would have been equally likely to have been chosen. In other words, each student in the group has an equal chance to be part of the sample. In statistics a sample of a population is said to be random if each member in the population has an equal chance of being chosen. Sampling with replacement - an individual is selected more than once. Sampling without replacement - an individual is selected only once. Source: OpenIntro.Org 2.4.2 Stratified Sampling The population is divided into non-overlapping, homogeneous subgroups called strata . Then, SRS is employed to select a certain number or a certain proportion of the whole within each stratum. Source: OpenIntro.Org Example: Stratified Sampling (Optional) Design a sample to survey \\(500\\) students using stratified sampling method. \\[ \\text { Strata Sizes } \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{3355} &amp; \\text{4693} &amp; \\text{8048} \\\\ \\text{Male} &amp; \\text{3734} &amp; \\text{6687} &amp; \\text{10421} \\\\ \\hline \\text{Total} &amp; \\text{7089} &amp; \\text{11380} &amp; \\text{18469} \\\\ \\end{array} } } \\] \\[ \\text { Strata Proportions } \\bbox[yellow,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{3355/18469 = .182} &amp; \\text{.254} &amp; \\text{.436} \\\\ \\text{Male} &amp; \\text{.202} &amp; \\text{.362} &amp; \\text{.564} \\\\ \\hline \\text{Total} &amp; \\text{.384} &amp; \\text{.616} &amp; \\text{1.000} \\\\ \\end{array} } } \\] \\[ \\text { Sample sizes } \\bbox[lightblue,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; \\text{.182(500)=91} &amp; \\text{127} &amp; \\text{218} \\\\ \\text{Male} &amp; \\text{101} &amp; \\text{181} &amp; \\text{282} \\\\ \\hline \\text{Total} &amp; \\text{192} &amp; \\text{308} &amp; \\text{500} \\\\ \\end{array} } } \\] Practice Design a sample to survey \\(1200\\) students using stratified sampling method. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Gender} &amp; \\text{Undergraduate} &amp; \\text{Graduate} &amp; \\text{Professional} \\\\ \\hline \\text{Female} &amp; \\text{10588} &amp; \\text{4475} &amp; \\text{1421} \\\\ \\text{Male} &amp; \\text{7762} &amp; \\text{3736} &amp; \\text{1153} \\\\ \\hline \\end{array} } } \\] 2.4.3 Cluster Sampling The population is often divided into non-overlapping mutually homogeneous yet internally heterogeneous subgroups called clusters. Cluster sampling is much like SRS, but instead of randomly selecting individuals, SRS is applied to select clusters. In other words, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. That is, we expect strata to be self-similar (homogeneous), while we expect clusters to be diverse (heterogeneous). The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this is referred to as a “one-stage” cluster sampling plan. Sometimes cluster sampling can be a more economical random sampling technique than the alternatives. For example, if neighborhoods represented clusters, this sampling method works best when each neighborhood is very diverse. Because each neighborhood itself encompasses diversity, a cluster sample can reduce the time and cost associated with data collection, because the interviewer would need only go to some of the neighborhoods rather than to all parts of a city, in order to collect a useful sample. One-Stage Cluster Sampling Source: OpenIntro.Org Multistage Cluster Sampling A “multistage” or “multistage cluster” sampling is an extention of cluster sampling and involves two (or more) steps. First step is to take a cluster sample. Then, instead of including all of the individuals in these clusters in the sample, a second sampling method, usually SRS, is employed within each of the selected clusters. In the neighborhood example, we could first randomly select some number of neighborhoods and then take a SRS from just those selected neighborhoods. As seen in Figure, stratified sampling requires observations to be sampled from every stratum. Multistage sampling selects observations only from those clusters that were randomly selected in the first step. It is also possible to have more than two steps in multistage sampling. Each cluster may be naturally divided into subclusters. For example, each neighborhood could be divided into streets. To take a three-stage sample, we could first select some number of clusters (neighborhoods), and then, within the selected clusters, select some number of subclusters (streets). Finally, we could select some number of individuals from each of the selected streets. Source: OpenIntro.Org 2.4.4 Nonrandom Sampling Systematic Sampling Select every \\(k^{th}\\) individual from a list of the population, where the position of the first person chosen is randomly selected from the \\(k\\) individuals. This will give a non-representative sample if there is a structure to the list. Source: OpenIntro.Org Solve: The human resource department at a certain company wants to conduct a survey regarding worker benefits. The department has an alphabetical list of all \\(5465\\) employees at the company and wants to conduct a systematic sample of size \\(60\\). What is \\(k\\)? Convenience or Volunteer Sampling Use the first \\(n\\) individuals that are available or the individuals who volunteer to participate. This is almost sure to give a non-representative sample which cannot be generalized to the population. Voluntary response samples are often used by the media to try to engage the audience. For example, a radio announcer will invite people to call the station to say what they think. Voluntary response samples are never reliable for the following reasons: * People who volunteer an opinion tend to have stronger opinions than is typical of the population. * People with negative opinions are often more likely to volunteer their response. Exercise: Identifying Sampling Methods A researcher randomly selects 20 Taco Bell locations and surveys all the employees at those locations. A news station hosts a call-in survey about whether physician-assisted death should be legalized in all states. A researcher randomly selects an LED TV out of the first 200 LED TVs on an assembly line and also selects every 200th LED TV after that. In a study at a community college, 30 instructors are randomly selected from full time instructors and 50 instructors are randomly selected from part-time instructors. The City Hall of Spring Hill, Kansas, creates a frame of its 5730 residents and randomly selects 60 residents. Every 10 years, the U.S. Census Bureau attempts to count every person living in the United States. To check the accuracy of their count in a certain city, they draw a sample of census districts and recount everyone in the sampled districts. What kind of sample is formed by the people who are recounted? Example: Non-Representative Sample Survey Surveyed 10 million people who were subscribers or had telephones. 2.4 million people responded (i.e. 24% response rate) Prediction Landslide victory of Landon. Election Result Landslide victory of Roosevelt. What did go wrong with the poll? Sample was drawn from telephone directories, club membership, magazine subscibers, etc. who were upper middle class people, largely excluding poor unemployed people. The sample suffered from both selection and nonresponse bias. 2.5 Bias and Precision Bias Bias is the degree to which a procedure systematically overestimates or underestimates a population value. A study conducted by a procedure that tends to overestimate or underestimate a population value is said to be biased. A study conducted by a procedure that produces the correct result on the average is said to be unbiased. Precision The standard deviation of the estimator. \\[ \\begin{aligned} \\text{Mean Squared Error, MSE} &amp;= precision^2 + bias^2 \\\\ \\text{Root Mean Squared Error, RMSE} &amp;= \\sqrt{MSE} \\end{aligned} \\] 2.6 Sampling Errors Statistical Inference Inferential Statistics is the practice of using information from a sample to draw conclusions about the entire population. It is the process of making judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling. \\(\\require{AMScd}\\) \\[ \\begin{CD} Sample @&gt; {\\text {statistical inference}} &gt;&gt; Population \\end{CD} \\] \\[\\underbrace{\\text {sample statistic}}_{\\text{investigator knows}} = \\underbrace{\\text {population parameter}}_{\\text{investigator wants to know}} + \\underbrace{\\text {bias}}_{\\text{nonsampling error}} + \\underbrace{\\text {chance variation}}_{\\text{random sampling error}} \\] Random Sampling Error - occurs when the sample has been selected with a random method, but there is a discrepancy between a sample result and the true population result. Non-sampling Error - is the results of human error, including such factors as wrong data entries, computing errors, questions with biased wording, false data provided by respondents, forming biased conclusions, or applying statistical methods that are not appropriate for the circumstances. A sampling method that consistently underestimates or overestimates some characteristics of the population is said to be biased. Selection/Sampling bias - occurs when the sample is selected in such a way that it systematically excludes or underrepresented part of the population. Voluntary response, self-interest, and social accountability bias fall into this category. An online survey conducted to estimate the percentage of Americans who have a Facebook account. The survey is biased because people who go online are favored. People who never go online cannot participate in the poll. Nonresponse bias - occurs when responses are not obtained from all individuals selected for inclusion in a sample. It happens if individuals refuse to be part of the study or if the research cannot track down individuals identified to be in the sample. Measurement or response bias - occurs when the data are collected in such a way that it tends to result in observed values that are different from the actual value in some systematic way. Contributing factors: question wording and order; mode of survey; influence of the interviewer; people might exaggerate how much money they earn; or a researcher might record the information incorrectly etc. Leading question bias is a type of measurement bias. For example, compare the impact on the respondent of the following three questions: Do you brag about your past successes with others? Do you inspire others by sharing your past successes? Do you share your past successes with others? Nonrandom Sampling Error - is the results of using a sampling method that is not random, such as using a convenience sample or a voluntary response sample. Statistical Studies Explanatory and Response Variables In statistical studies, we want to know whether a variable \\(x\\) explains (or affects) another variables \\(y\\). The \\(x\\) variable is called the explanatory or independent variable. The \\(y\\) variable is called the response or dependent variable. Association vs. Causation There is an association between an explanatory and response variable when the response variable changes as the explanatory variable changes. If the change in the explanatory variable causes the change in the response variable, then there is a causation between the variables. 2.7 Observational Studies Generally, data in observational studies are collected on specific characteristics only by passively monitoring study participants, but the observers don’t attempt to modify the individuals being studied. These studies are inexpensive and good for discovering relationships related to rare outcomes. They are generally only sufficient to show associations. Key points: Observational studies seldom support causal inference, \\(X \\rightarrow Y\\). Variables in observational studies are often measured concurrently. Such measurements provide no temporal precedence. The study design cannot determine which of the two variables, a presumed cause and a presumed effect, occurred first. Hence, in this kind of design, the sole basis for causal inference is assumption, or ruling out alternative explanations of the association between \\(X\\) and \\(Y\\), as well as measuring other presumed causes of \\(Y\\). It is possible to correctly infer causation in nonexperimental designs, but the hurdles are much greater. As an example, think about the causal link cigarette smoking and lung cancer. Types of observational studies: In a cohort study, a group of subjects is studied to determine whether various factors of interest are associated with an outcome. A prospective (or longitudinal) study is the one where the subjects are studied over time. Example: One of the most famous prospective cohort studies was the Framingham Heart Study. This study began in 1948 with \\(5209\\) men and women from the town of Framingham, Massachusetts. Every two years, these subjects were given physical exams and lifestyle interviews, which were studied to discover factors that increase the risk of heart disease. A cross-sectional study is where the subjects are measured at one point in time, not over a period of time. Example: I. Lang and colleagues studied the health effects of Bisphenol A, a chemical found in the linings of food and beverage containers. They measured the levels of Bisphenol A in urine samples from \\(1455\\) adults and found that people with higher levels of Bisphenol A were more likely to have heart disease and diabetes. In a retrospective study, data from the subjects is collected after the outcome has occurred (through examinations of records, interviews, and so on). Example: In a study published in The New England Journal of Medicine, T. Adams and colleagues sampled 9949 people who had undergone gastric bypass surgery between 5 and 15 years previously, along with \\(9668\\) obese patients who had not had bypass surgery. They looked back in time to see which patients were still alive. They found that the survival rates for the surgery patients were greater than for those who had not undergone surgery. In a case-control study, two samples are drawn where one consists of people who have the disease of interest (the cases), and the other consists of people who do not (the controls). The investigators look back in time to determine whether a factor of interest differs between the groups. Example: S.S. Nielsen and colleagues conducted a case-control study to determine whether exposure to pesticides is related to brain cancer in children. They sampled 201 children who had been diagnosed with brain cancer, and 285 children who did not. They interviewed the parents to estimate the extent to which the children had been exposed to pesticides. They did not find a clear relationship between pesticide exposure and brain cancer. Observational Study: drinking coffee and longevity Coffee drinkers may live longer - nytimes.com, May 16, 2012. Coffee may help you live longer, study suggests - thestar.com, May 17, 2012. No, drinking coffee probably won’t make you live longer - washingtonpost.com, May 17, 2012. Association of coffee drinking with total and cause-specific mortality,” New England Journal of Medicine, May 2012. Sample Size: 400,000 Age Range: 50-71 years Period: 1995 - 2008 Death: 52,000 How would you interpret the result? Confounding Variable A confounding variable is a variable that is associated with both the explanatory (or treatment) and response variables (or outcomes). Simultaneously with the explanatory variable, it may cause the response variable to change during the study. Because of the confounding variable’s association with both variables, we do not know if the response is due to the explanatory variable or due to the confounding variable. Sun exposure is a confounding factor because it is associated with both the use of sunscreen and the development of skin cancer. People who are out in the sun all day are more likely to use sunscreen (treatment), and people who are out in the sun (confounder) all day are more likely to get skin cancer (outcome). Lurking Variable Lurking variables are variables that are not considered in the analysis, but may affect the nature of the relationship between the explanatory variable and the outcome. Table: 20-year survival status of women by smoking status \\[ \\begin{array}{c|lcr} &amp; \\text{Smoker} \\\\ &amp; \\text{Yes} &amp; \\text{No} \\\\ \\hline \\text {Dead} &amp; 0.239 &amp; 0.314 \\\\ \\text {Alive} &amp; 0.761 &amp; 0.686 \\end{array} \\] Are smokers less likely to die? 2.8 Experimental Design While observational studies are effective tools for answering certain research questions, experiments are essential to measure the effect of a treatment. In an experiment, we apply some treatment and then proceed to observe its effects on the individuals. Subject or Experimental Unit: Entity who is participating in the study. Treatment Group: The group of subjects that receives treatments. Control Group: The group of subjects that receives no treatment. Outcome or Response Variable: The outcome of interest, measured on each subject. Factor: The categorical variable that explains the outcome of the experiment. Each category is called level. Randomization: the investigator assigns treatments to the experimental units at random. Example: To assess the effectiveness of a new method for teaching arithmetic to elementary school children, a simple random sample of \\(30\\) first graders were taught with the new method, and another simple random sample of \\(30\\) first graders were taught with the currently used method. At the end of eight weeks, the children were given a test to assess their knowledge. Blinding: When researchers keep the subjects uninformed about their treatment, the study is said to be blind. Its purpose is to reduce the potential for both researchers’ and subjects’ emotional bias. Subjects would not know which experimental group they are assigned to. The researcher (i.e. the person who is measuring the outcome) would not know which treatment is assigned to which experimental unit. Single-blind: only one type of blinding is applied. Double-blind: both types of blinding are applied. Placebo: A substance or treatment with no active ingredients. The control group receives the placebo treatment. This phenomenon, in which the recipient perceives an improvement in condition due to personal expectations, rather than the treatment itself, is known as the placebo effect. Principles of Experimental Design Well-conducted experiments are built on three main principles. Direct Control Researchers assign treatments to cases, and they do their best to control any other differences in the groups. They want the groups to be as identical as possible except for the treatment, so that at the end of the experiment any difference in response between the groups can be attributed to the treatment and not to some other confounding or lurking variable. Direct control refers to variables that the researcher can control. Randomization Researchers randomize patients into treatment groups to account for variables that cannot be controlled. Randomizing patients into the treatment or control group helps even out the effects of such differences, and it also prevents accidental bias from entering the study. In a randomized experiment, small differences among treatment groups are likely to be due only to chance. If there are large differences in outcomes among the treatment groups, we can conclude that the differences are due to the treatments. Replication In a single study, replication is done by imposing the treatment on a sufficiently large number of subjects or experimental units. Scientists may also replicate the entire experiment on an entirely different population of experimental units to verify earlier findings. 2.8.1 Randomized Blocked Design Researchers sometimes know or suspect that another variable, other than the treatment, influences the response. Under these circumstances, they may carry out a blocked experiment. In this design, they first group individuals into blocks based on the identified variable (in other words, form blocks or groups of subjects with similar characteristics) and then randomize subjects within each block to the treatment groups. This strategy is referred to as blocking. For example, blocks can be designed based on gender or age group of subjects. 2.8.2 Completely Randomized Experimental Design A completely randomized experimental design is one in which the subjects or experimental units are randomly assigned to each group in the experiment. Source: OpenIntro.Org Case Study 1: PATRICIA Study | PApilloma TRIal against Cancer In young Adults The Lancet, Volume 374, Issue 9686, Pages 301 - 314, 25 July 2009 Efficacy of human papillomavirus (HPV) - 16/18 AS04-adjuvanted vaccine against cervical infection and precancer caused by oncogenic HPV types (PATRICIA); final analysis of a double-blind, randomized study in young women. Paavonen, et. al. \\[ \\begin{array}{c|c} {\\text{Response Variable} \\\\ \\text {(Acquired an infection)}} &amp; {\\text{Explanatory Variable} \\\\ \\text{(Given the HPV vaccine)}} \\\\ \\hline \\text{Yes} &amp; \\text{Yes} \\\\ \\text{No} &amp; \\text{No} \\\\ \\end{array} \\] \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{c} {\\text{Factor 1} \\\\ \\text{(2 Levels)}} \\\\ \\hline \\text{Drug A} \\\\ \\text{Drug B} \\end{array} } } \\] \\[ \\bbox[silver,5px] { \\color{black} { \\begin{array}{c} {\\text{Factor 2} \\\\ \\text{(2 Levels)}} \\\\ \\hline \\text{Dose A} \\\\ \\text{Dose B} \\end{array} } } \\] \\[ \\bbox[5px,border:2px solid red] { \\begin{array}{c} \\text{4 Treatments} \\\\ \\hline \\text{Drug A &amp; Dose A}\\\\ \\text{Drug A &amp; Dose B}\\\\ \\text{Drug B &amp; Dose A}\\\\ \\text{Drug B &amp; Dose B} \\end{array} } \\] Case Study 2: Ischemic Preconditioning | Effect on Muscular Endurance Can Ischemic Preconditioning improve athletic performance? Experimental units: 40 male teenagers Response Variable: length of time a wall squat position can be held Control Groups: 2 groups who received 0 lb pressure Control of Extraneous Factors: Age, sex, athletic ability Randomization: Randomly assigned 10 experimental units to each of 4 treatment groups \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{c|c} \\text{Factor1} &amp; {\\text{Amount of pressure} \\\\ \\text{applied by the} \\\\ \\text{bloodpressure cuff}} \\\\ \\hline \\text{Level 1} &amp; \\text{20 lb} \\\\ \\text{Level 2} &amp; \\text{0 lb} \\\\ \\end{array} } } \\] \\[ \\bbox[silver,5px] { \\color{black} { \\begin{array}{c|c} \\text{Factor2} &amp; {\\text{Length of time pressure} \\\\ \\text{was applied}} \\\\ \\hline \\text{Level 1} &amp; \\text{10 min} \\\\ \\text{Level 2} &amp; \\text{20 min} \\\\ \\end{array} } } \\] \\[ \\bbox[5px,border:2px solid red] { \\begin{array}{c} \\text{4 Treatments} \\\\ \\hline \\text{20 lb/10 min}\\\\ \\text{20 lb/20 min}\\\\ \\text{0 lb/ 10 min}\\\\ \\text{0 lb/ 20 min} \\end{array} } \\] Components of a Well-Designed Study There should be a control group and at least one treatment group. Individuals should be randomly assigned to the control and treatment group(s). The sample size should be large enough. A placebo should be used when appropriate. The study should be double-blind when possible. If this is impossible, then the study should be single-blind if possible. Practice: Identifying an Experiment and an Observational Study Identify whether the study is an experiment or an observational study. Discuss whether the components of a good study were used. For five years, the author taught an innovative intermediate algebra course in which students learned by working in groups. Then the author compared the proportion of his successful intermediate algebra students who passed trigonometry with the proportion of other professors’ successful intermediate algebra students who passed trigonometry. Practice: Redesign an Observational Study into a Well-Designed Experiment A researcher wants to determine whether taking vitamin C helps people avoid getting the flu and the common cold. She randomly selects 100 people and asks them whether they take vitamin C and how often they had the flu or a cold in the past year. The researcher analyzes the responses and concludes that vitamin C helps people avoid the flu and colds. Describe some problems with the observational study. Include in your description at least one possible lurking or confounding variable and identify which type it is. Redesign the study so that it is a well-designed experiment. "],["exploring-data-with-tables-and-graphs.html", "Chapter 3 Exploring Data with Tables and Graphs 3.1 Frequency Distribution 3.2 Frequency Distribution for Quantitaive Data 3.3 Graphical Summaries for Small Data Sets", " Chapter 3 Exploring Data with Tables and Graphs Learning Outcome: Interpret quantitative data using tables and graphs, and descriptive statistics using histograms. The chapter introduces various techniques such as frequency table and histogram to organize quantitative data to explore its important characteristics. 3.1 Frequency Distribution A frequency distribution (or frequency table) shows how data are partitioned among several categories (or classes) by listing the categories along with the number (frequency) of data values in each of them. Example: A retailer accepts four types of credit cards and lists the types used by the last \\(50\\) customers as follows: The frequency distribution presents the frequencies for each type of credit card. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Type of Credit Cards} &amp; \\text{Frequency} \\\\ \\hline \\text{MasterCard} &amp; 11 \\\\ \\text{Visa} &amp; 23 \\\\ \\text{American Express} &amp; 9 \\\\ \\text{Discover} &amp; 7 \\\\ \\hline \\text{Total} &amp; 50 \\end{array} } } \\] Relative Frequency Distribution A relative frequency distribution (or percentage frequency distribution) is a variation of the basic frequency distribution in which a class frequency is replaced by a relative frequency (or proportion). \\[ \\begin{align} \\text{relative frequency for a class} &amp;= \\dfrac{\\text{frequency for a class}}{\\text{sum of all frequencies}} \\\\ \\text{percentage of a class} &amp;= \\dfrac{\\text{frequency for a class}}{\\text{sum of all frequencies}} \\times 100\\%\\\\ \\end{align} \\] \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Type of Credit Cards} &amp; \\text{Frequency} &amp; \\text{Relative Frequency} \\\\ \\hline \\text{MasterCard} &amp; 11 &amp; 11/50 = 0.22 \\\\ \\text{Visa} &amp; 23 &amp; 23/50 = 0.46 \\\\ \\text{American Express} &amp; 9 &amp; 9/50 = 0.18 \\\\ \\text{Discover} &amp; 7 &amp; 7/50 = 0.14 \\\\ \\hline \\text{Total} &amp; 50 &amp; 50/50 = 1 \\end{array} } } \\] 3.1.1 Bar Graphs A bar graph is a graphical representation of a frequency distribution. It consists of rectangles of equal width, with one rectangle for each category. The heights of the rectangles represent the frequencies or relative frequencies. Following are the frequency and relative frequency bar graphs for the credit card data. 3.2 Frequency Distribution for Quantitaive Data \\(\\text {Table: Drive-Through Service Times (seconds) for McDonald&#39;s Lunches}\\) \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency} \\\\ \\hline \\text{75-124} &amp; \\text{11} \\\\ \\text{125-174} &amp; \\text{24} \\\\ \\text{175-224} &amp; \\text{10} \\\\ \\text{225-274} &amp; \\text{3} \\\\ \\text{275-324} &amp; \\text{2} \\\\ \\hline \\text{Total} &amp; 50 \\end{array} } } \\] Lower class limits: \\({75, 125, 175, 225, 275}\\) Upper class limits: \\({124, 174, 224, 274, 324}\\) Class boundaries: \\({74.5, 124.5, 174.5, 224.5, 274.5, 324.5}\\) Class midpoints: \\({99.5, 149.5, 199.5, 249.5, 299.5}\\) Class width: \\(125 - 75 = 50\\) \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c} \\text{Time(Seconds)} &amp; \\text{Relative Frequency} \\\\ \\hline \\text{75-124} &amp; \\text{22}\\% \\\\ \\text{125-174} &amp; \\text{48}\\% \\\\ \\text{175-224} &amp; \\text{20}\\% \\\\ \\text{225-274} &amp; \\text{6}\\% \\\\ \\text{275-324} &amp; \\text{4}\\% \\\\ \\hline &amp; 100\\% \\end{array} } } \\] Cumulative Distribution \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c} \\text{Time(Seconds)} &amp; \\text{N} &amp; \\text{%} \\\\ \\hline \\text{75-124} &amp; 11 &amp; \\text{22}\\% \\\\ \\text{125-174} &amp; 35 &amp; \\text{70}\\% \\\\ \\text{175-224} &amp; 45 &amp; \\text{90}\\% \\\\ \\text{225-274} &amp; 48 &amp; \\text{96}\\% \\\\ \\text{275-324} &amp; 50 &amp; \\text{100}\\% \\\\ \\hline \\end{array} } } \\] 3.2.1 Histogram A histogram is a graph consisting of bars of equal width drawn adjacent to each other. The horizontal scale represents classes of quantitative data values; and the vertical scale represents frequencies. A relative frequency histogram has the same shape and horizontal scale as a histogram, but the vertical scale uses relative frequencies (as percentages) instead of actual frequencies. Importance of Histogram Visually displays the shape of the distribution of the data Shows the location of the center of the data Shows the spread of the data Identifies outliers Density Histogram \\[ \\begin{align} \\textbf{Density} &amp;= \\dfrac{\\textbf{relative frequency}}{\\textbf{bin width}} \\\\ \\text {Density of class (75-124)} &amp;= \\dfrac{\\text{rel. freq of class (75-124)}}{\\text{class width}} \\\\ &amp;= \\dfrac{0.22}{50} \\\\ &amp;= 0.0044 \\end{align} \\] In density histogram, area of each rectangular bar is the relative frequency of its class. \\(\\textbf{Total area of a density histogram is equal to 1.}\\) Practice - Construct a Density Histogram The accompanying frequency distribution summarizes data on the number of times smokers attempted to quit before their final successful attempts. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{r|c} \\text{Number of attempts} &amp; \\text{Frequency} &amp; \\text{Relative Frequency} &amp; \\text{Density} \\\\ \\hline \\textbf{0-10} &amp; 778 \\\\ \\textbf{10-20} &amp; 306 \\\\ \\textbf{20-30} &amp; 274 \\\\ \\textbf{30-40} &amp; 221 \\\\ \\textbf{40-50} &amp; 238 \\end{array} } } \\] Pie Charts The distribution of a categorical variable can be described by a pie chart, which is a disk where slices represent the categories. The proportion of the total area for one slice is equal to the relative frequency for the category represented by the slice. The relative frequencies are usually written as percentages. Example 1: Construct and Interpret a Pie Chart A total of 273 children were surveyed about what job they would want to do. The jobs and the percentages of the children who voted for them are shown in the table. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{r|c} \\text{Job} &amp; \\text{Percent} \\\\ \\hline \\text{Spy/Agent} &amp; 16 \\\\ \\text{Veterinarian} &amp; 13 \\\\ \\text{Professional Athlete} &amp; 12 \\\\ \\text{Movie Star} &amp; 10 \\\\ \\text{Video Game Designer} &amp; 8 \\\\ \\text{Doctor} &amp; 6 \\\\ \\text{Other} &amp; 35 \\end{array} } } \\] Questions: Find the proportion of the observations that fall in the spy category. Find the proportion of the observations that do NOT fall in the spy category. Find the proportion of the observations that fall in the athlete category OR fall in the movie-star category. Interpreting a Multiple Bar Graph In a survey in 2012, 1960 adults were asked the following question: “Generally speaking, do you usually think of yourself as a Republican, Democrat, Independent, or other?” The results of the survey are described by the multiple bar graph. What proportion of women thought of themselves as Democrats? Which political party did the greatest proportion of men choose? Compare the proportion of women who thought of themselves as Independents to the proportion of men who thought of themselves as Independents. A total of 1081 women and 879 men responded to the survey. Were there more women or men who thought of themselves as Independents? How is this possible, given there was a smaller proportion of women who thought of themselves as Independents than men? Two-Way (Contingency) Table The table summarizes the responses from all 42 students who participated in the survey about whether they had read a novel in the past year. \\[ \\bbox[yellow,5px] { \\color{black} { \\begin{array}{l|c|c|c} \\text{Gender} &amp; \\text{Did Not Read Novel} &amp; \\text{Read Novel} &amp; \\text{Total} \\\\ \\hline \\text{Female} &amp; 6 &amp; 19 &amp; 25 \\\\ \\text{Male} &amp; 6 &amp; 11 &amp; 17 \\\\ \\hline \\text{Total} &amp; 12 &amp; 30 &amp; 42 \\\\ \\hline \\end{array} } } \\] How many of the students read a novel in the past year? What proportion of the students did not read a novel in the past year? What proportion of the women read a novel in the past year? What proportion of the students is men AND read a novel in the past year? 3.3 Graphical Summaries for Small Data Sets Dotplot A dotplot uses dots to show the frequency, or number of occurrences, of the values in a data set. The higher the stack of dots, the greater the number of occurrences there are of the corresponding value. Stem-and-leaf Plot Stem-and-leaf plots are a simple way to display small data sets. In a stem-and-leaf plot, the rightmost digit is the leaf, and the remaining digits form the stem. Data: \\[ 1.2, 1.5, 1.6, 2.1, 2.4, 2.9, 3.0, 3.1, 3.2, 3.3, \\\\ 4.5, 4.6, 4.9, 5.0, 5.2, 5.8, 6.0, 6.3, 6.4, 6.5 \\] The decimal point is at the | 1 | 256 2 | 149 3 | 0123 4 | 569 5 | 028 6 | 0345 "],["describing-exploring-and-comparing-data.html", "Chapter 4 Describing, Exploring, and Comparing Data 4.1 Measures of Shape 4.2 Measures of Center 4.3 Measures of Spread or Variation 4.4 The Empirical Rule 4.5 Z-Score 4.6 Percentiles and Quartiles 4.7 Boxplot 4.8 Group Comparison", " Chapter 4 Describing, Exploring, and Comparing Data Learning Outcome: Calculate measures of central tendency, position, and spread, including standard deviation. Interpret quantitative data using graphs and descriptive statistics with emphasis on histograms and boxplots. In this chapter, we will numerically describe distributions of quantitative variables. Distributions, such as histograms, can be described by three characteristics: shape, center, and spread. 4.1 Measures of Shape Modes The mode of some data is an observation with the greatest frequency. There can be more than one mode, but if all the observations have frequency 1, then there is no mode. Mode is represented by a prominent peak in the distribution. Unimodel Distribution Bimodal Distribution Multimodal Distribution Uniform Distribution All the bins have the same frequency, or at least close to the same frequency. It is a distribution without a mode. Symmetry The histogram for a symmetric distribution will look the same on the left and the right of its center. Skew A histogram is skewed right if the longer tail is on the right side of the mode. A histogram is skewed left if the longer tail is on the left side of the mode. Outlier An Outlier is a data value that is far above or far below the rest of the data values. 4.2 Measures of Center Mean The sample mean of a numerical variable is computed as the sum of all of the observations \\(\\{x_1, x_2, \\cdots, x_n\\}\\) divided by the number of observations \\((n)\\). If \\(\\bar x\\) is the mean, then \\[ \\displaystyle (\\bar x - x_1) + (\\bar x - x_2) + \\cdots + (\\bar x - x_n) = 0 \\\\ \\displaystyle \\text{Therefore}, \\ \\bar x = \\frac{(x_1+x_2+...+x_n)}{n} = \\dfrac{1}{n}\\sum_{i=1}^n x_i \\] The mean follows the tail In a right skewed distribution, the mean is greater than the median. In a left skewed distribution, the mean is less than the median. In a symmetric distribution, the mean and median are approximately equal. Median The median splits an ordered data set in half. If there are an even number of observations, the median is the average of the two middle values. If there are an odd number of observations, the median is the middle value. 0 0 0 0 0 0 1 1 1 1 1 2 2 3 3 3 4 4 5 5 5 6 6 7 7 7 9 9 9 10 10 10 11 11 12 14 14 16 17 22 25 25 25 26 26 27 29 42 43 64 Calculating the Median \\(n\\) is odd Sort the series in ascending order. If the series has odd number \\((n)\\) of entries, the median is at position \\(x_1, \\cdots, \\underbrace{x_{\\frac{n+1}{2}}}, \\cdots, x_n.\\) Find the median of the series: \\(2,4,5,(6),7,9,9\\) The median is \\(6\\). \\(n\\) is even Sort the series in ascending order. If the series has even number \\((n)\\) of entries, the median is the average of the two middle numbers: \\(x_1, \\cdots, \\underbrace{x_{\\frac{n}{2}}, x_{\\frac{n}{2}+1}}_{ \\frac{1}{2}\\big(x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1} \\big)}, \\cdots, x_n.\\) Find the median of the numbers: \\(\\{2,2,4,6,7,8\\}\\) Median is the average of the third and the fourth numbers: \\(\\dfrac{4+6}{2}=5\\) Example: Comparing the Medians of Two Distributions \\[ \\begin{array}{lclc} \\text{Pacific State} &amp; \\text{Minimum Wage} &amp; \\text{Mountain State} &amp; \\text{Minimum Wage} \\\\ \\hline \\text {Alaska} &amp; 7.75 &amp; \\text {Arizona} &amp; 7.90 \\\\ \\text {California} &amp; 8.00 &amp; \\text {Colorado} &amp; 8.00 \\\\ \\text {Hawaii} &amp; 7.25 &amp; \\text {Idaho} &amp; 7.25 \\\\ \\text {Oregon} &amp; 9.10 &amp; \\text {Montana} &amp; 7.90 \\\\ \\text {Washington} &amp; 9.32 &amp; \\text {Nevada} &amp; 8.25 \\\\ &amp; &amp; \\text {New Mexico} &amp; 7.50 \\\\ &amp; &amp; \\text {Utah} &amp; 7.25 \\\\ &amp; &amp; \\text {Wyoming} &amp; 5.15 \\\\ \\end{array} \\] Find the median minimum wages of the Pacific and Mountain states. How the Shape of a Distribution Affects the Mean and the Median If a distribution is skewed left, the mean is usually less than the median and the median is usually a better measure of the center. If a distribution is symmetric, the mean is approximately equal to the median and both are reasonable measures of the center. If a distribution is skewed right, the mean is usually greater than the median and the median is usually a better measure of the center. List: \\(\\{2,3,3,4\\}\\) List: \\(\\{2,3,3,7\\}\\) \\(\\textbf {Notice: The median is unaffected by outliers.}\\) Weighted Mean The weighted mean is the same as the mean, except that it is influenced more by some observations than others. We assign weights to observations as a sort of way of describing its relative importance. The weighted mean of observations \\(x_1, x_2,...,x_n\\) using weights \\(w_1, w_2,...,w_n\\) is given by \\(\\displaystyle \\bar x =\\frac{w_1x_1+w_2x_2+...+w_nx_n}{w_1+w_2+...+w_n}\\) The simple mean is a weighted mean where all the weights are \\(1\\). \\(\\displaystyle \\bar x =\\frac{1\\times x_1+1\\times x_2+...+1\\times x_n}{1+1+...+1} = \\frac{x_1+x_2+...+x_n}{n}\\) Example 1: The chart below shows ratings from \\(10\\) visitors on a \\(0-5\\) scale. What is the average rating? \\[ \\begin{align} &amp;\\text{Method 1: } \\dfrac{4 + 1 + 3 + 5 + 3 + 5 + 2 + 5 + 5 + 4}{10} = 3.7 \\\\ \\\\ &amp;\\text{Method 2: } \\dfrac{(1)1 + (1)2 + (2)3 + (2)4 + (4)5}{10} = 3.7 \\\\ \\end{align} \\] Example 2: The consumer price index (CPI) is a weighted average of the change in prices paid by consumers for a representative set of goods and services. Use the table to calculate the CPI from March 2008 to March 2009. Round to the nearest tenth of a percent. \\[ \\begin{array}{l|c|r} \\text{Category} &amp; \\text{Weight} &amp; \\text{Percent Change} \\\\ \\hline \\text{food and beverages} &amp; 16\\% &amp; 4.3 \\\\ \\text{housing} &amp; 43\\% &amp; 1.4 \\\\ \\text{apparel} &amp; 4\\% &amp; 1.4 \\\\ \\text{transportation} &amp; 15\\% &amp; -13.1 \\\\ \\text{medical care} &amp; 6\\% &amp; 2.8 \\\\ \\text{recreation} &amp; 6\\% &amp; 1.7 \\\\ \\text{education and communication } &amp; 6\\% &amp; 3.6 \\\\ \\text{other} &amp; 4\\% &amp; 5.7 \\\\ \\hline \\end{array} \\] Calculating Mean from a Frequency Distribution \\[ \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} \\] where, \\(f\\) is the class frequency and \\(x\\) is the class midpoint. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f \\cdot x \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 1094.5 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 3588.0 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 1995.0 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 748.5 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 599.0 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum(f \\cdot x) = 8025.0 \\\\ \\end{array} } } \\] \\[ \\displaystyle \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} = \\dfrac{8025.0}{50} = 160.5 \\] Midrange The midrange of a data set is the measure of center that is the value midway between the maximum and minimum values in the original data set. It is found by adding the maximum data value to the minimum data value and then dividing the sum by \\(2\\), as the following formula: \\[ \\text {Midrange} = \\dfrac{\\text{maximum data value + minimum data value}}{2} \\] 4.3 Measures of Spread or Variation Range The range of a set of data is the difference between the maximum and the minimum data values. \\(\\textbf {range = maximum - minimum}\\) The range is sensitive to outliers. A single high or low value will affect the range significantly. Standard Deviation of a Sample SD \\((s)\\) of a set of sample values is a measure of how much, on average, the data values deviate away from the sample mean. In other word, SD describes the variability of the data set within the range of the dataset. Low variability or small spread means that the values tend to be more clustered together. High variability or large spread means that the values tend to be far apart. Calculating the Standard Deviation The standard deviation is the square root of the variance. It is roughly the average distance of the observations from the mean. \\[ \\bbox[yellow,5px] { \\color{black}{s= \\sqrt{\\frac{1}{n-1}\\sum(x_i-\\bar x)^2}} } \\] Exercise: \\(1. Calculate \\space SD \\space of \\space [0,1]\\) \\(2. Calculate \\space SD \\space of \\space [30,20, 41, 21]\\) Which histogram has the largest SD? Hint: pay attention to the range of the distributions. Calculating Standard Deviation from a Frequency Distribution \\(\\text {Step 1: Calculate weighted average from the class midpoint and class frequency}\\) \\[ \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} \\] where, \\(f\\) is the class frequency and \\(x\\) is the class midpoint. \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f \\cdot x \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 1094.5 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 3588.0 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 1995.0 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 748.5 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 599.0 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum(f \\cdot x) = 8025.0 \\\\ \\end{array} } } \\] \\[ \\displaystyle \\bar x = \\dfrac{\\sum (f \\cdot x)}{\\sum f} = \\dfrac{8025.0}{50} = 160.5 \\] \\(\\text {Step 2: Calculate variance}\\) \\[ \\displaystyle s^2 = \\dfrac{1}{\\sum_{i} f_i - 1}\\sum_{i} f_i(x_i - \\bar x)^2 \\] \\[ \\bbox[white,4px] { \\color{black} { \\begin{array}{c|c|c|c} \\text{Time(Seconds)} &amp; \\text{Frequency } f &amp; \\text{Class Midpoint } x &amp; f_i \\cdot(x_i - \\bar x)^2 \\\\ \\hline \\text{75-124} &amp; 11 &amp; 99.5 &amp; 40931 \\\\ \\text{125-174} &amp; 24 &amp; 149.5 &amp; 2904 \\\\ \\text{175-224} &amp; 10 &amp; 199.5 &amp; 15210 \\\\ \\text{225-274} &amp; 3 &amp; 249.5 &amp; 23763 \\\\ \\text{275-324} &amp; 2 &amp; 299.5 &amp; 38642 \\\\ \\hline \\text{Total} &amp; \\sum f = 50 &amp; &amp; \\sum f_i \\cdot (x_i - \\bar x)^2 = 121450 \\\\ \\end{array} } } \\] \\[ \\begin{align} s^2 &amp;= \\dfrac{121450}{50-1} = 2478.6 \\\\ \\end{align} \\] \\(\\text {Step 3: Calculate standard deviation}\\) \\[ \\begin{align} s &amp;= \\sqrt {2478.6} = 49.8 \\end{align} \\] Standard Deviation of a Population \\[ \\bbox[yellow,5px] { \\color{black}{\\sigma= \\sqrt{\\frac{1}{N}\\sum(x_i-\\mu)^2}} } \\] Variance of a Sample and Population The variance of a set of values is a measure of variation equal to the square of the standard variation. Sample variance: \\(s^2 =\\) square of the sample standard deviation \\(s\\). Population variance: \\(\\sigma^2 =\\) square of the population standard deviation \\(\\sigma\\). Why is \\((n-1)\\) used to calculate standard deviation? \\(\\acute s = \\sqrt{\\frac{1}{n}\\sum(x_i-\\bar x)^2}\\) is not an unbiased estimator of the population standard deviation \\(\\sigma\\), meaning that the distribution of \\(\\acute s\\) does not tend to center around \\(\\sigma\\). Therefore, an adjustment is applied by replacing \\(n\\) by \\((n-1)\\) in the denominator. \\(s = \\sqrt{\\frac{1}{n-1}\\sum(x_i-\\bar x)^2}\\) is an unbiased estimator of \\(\\sigma\\). Also, with division by \\(n-1\\), sample variance \\(s^2\\) tend to center around the value of the population variance \\(\\sigma^2\\); with division by \\(n\\), sample variances \\(s^2\\) tend to underestimate the value of the population variance \\(\\sigma^2\\). Coefficient of Variation The coefficient of variation (CV) for a set of non-negative sample or population data, expressed as a percent, describes the standard deviation relative to the mean, and is given by the following: \\[ Sample: CV = \\frac{s}{x}.100 \\\\ Population: CV = \\frac{\\sigma}{x}.100 \\] \\(CV\\) is useful compare the spreads of multiple distributions. Suppose, you are looking for a safe investment option among stocks, EFTs, and bonds. \\[ \\text {CV =} \\dfrac{\\text{volatility } (\\sigma)}{\\text {expected return } (\\mu)} \\times 100 \\% \\] \\[ \\text {CV (Stocks) =} \\dfrac{11\\%}{15\\%} \\times 100 \\% = 73.3\\% \\\\ \\text {CV (EFT) =} \\dfrac{9\\%}{13\\%} \\times 100 \\% = 69.2\\% \\\\ \\text {CV (Bonds) =} \\dfrac{2\\%}{3\\%} \\times 100 \\% = 66.7\\% \\\\ \\] 4.4 The Empirical Rule When a data set has a bell-shaped histogram (or normal distribution), it is often possible to use the standard deviation to provide an approximate description of the data using a rule known as The Empirical Rule. Empirical Rule of Normal Distribution \\[ \\begin{array}{lc} \\text{Interval} &amp; \\text{Percent} \\\\ \\hline \\mu \\pm \\sigma &amp; 68 \\% \\\\ \\mu \\pm 2\\sigma &amp; 95 \\% \\\\ \\mu \\pm 3\\sigma &amp; 99.7 \\% \\\\ \\hline \\end{array} \\] Example: A large class of \\(200\\) students took an exam. The scores had sample mean and sample standard deviation \\(s = 10\\). The histogram is approximately normally distributed. Approximately how many students had scores between \\(45\\) and \\(85?\\) Solution: The value \\(45\\) is two standard deviations below the mean since \\(\\bar x - 2s = 65 - 20 = 45\\) and the value \\(85\\) is two standard deviations above the mean since \\(\\bar x + 2s = 65 + 20 = 85\\) By the Empirical Rule, approximately \\(95\\%\\) of scores are between \\(45\\) and \\(85\\). There were \\(200\\) students in the class. Therefore, the number that had scores between \\(45\\) and \\(85\\) is approximately \\((0.95)(200) = 190\\). 4.4.1 Chebyshev’s Inequality When a distribution is bell-shaped, we use the Empirical Rule to approximate the proportion of data within one or two standard deviations of the mean. Another rule called Chebyshev’s Inequality holds for any data set. In any data set, the proportion of the data that is within \\(K\\) standard deviations of the mean is at least \\(1 - (1/K^2)\\). Specifically, by setting \\(K = 2\\) or \\(K = 3\\), we obtain the following results. At least \\(3/4\\), or \\(75\\%\\), of the data are within two standard deviations of the mean. At least \\(8/9\\), or \\(89\\%\\), of the data are within three standard deviations of the mean. Example: As part of a public health study, systolic blood pressure was measured for a large group of people. The mean was \\(120\\) and the standard deviation was \\(10\\). What information does Chebyshev’s Inequality provide about these data? Solution: We compute the following: \\[\\bar x - 2s = 120 - 2(10) = 100 \\\\ \\bar x + 2s = 120 + 2(10) = 140\\] At least \\(3/4\\) \\((75\\%)\\) had systolic blood pressures between \\(100\\) and \\(140\\). \\[\\bar x - 3s = 120 - 3(10) = 90 \\\\ \\bar x + 3s = 120 + 3(10) = 150\\] At least \\(8/9\\) \\((89\\%)\\) had systolic blood pressures between \\(90\\) and \\(150\\). 4.5 Z-Score A z-score (or standard score or standardized value) is the number of standard deviations that a given value \\(x\\) is above or below the mean. The \\(z\\) score is calculated by using one of the following: \\[z= \\dfrac{x-\\bar x}{s}\\] \\[ \\begin{align} x = \\bar x - 2s &amp;\\implies z = -2 \\\\ x = \\bar x - s &amp;\\implies z = -1 \\\\ x = \\bar x &amp;\\implies z = 0 \\\\ x = \\bar x + s &amp;\\implies z = +1 \\\\ x = \\bar x + 2s &amp;\\implies z = +2 \\\\ \\end{align} \\] Note: \\(z\\)-score is useful in comparing two variables measured on different scales. Example: A National Center for Health Statistics study states that the mean height for adult men in the U.S. is \\(\\mu = 69.4\\) inches, with a standard deviation of \\(\\sigma = 3.1\\) inches. The mean height for adult women is \\(\\mu = 63.8\\) inches, with a standard deviation of \\(\\sigma = 2.8\\) inches. Who is taller relative to their gender, a man \\(73\\) inches tall, or a woman \\(68\\) inches tall? Solution: \\[ z_{\\text {men&#39;s height}} = \\dfrac{x - \\mu}{\\sigma} = \\dfrac{73 - 69.4}{3.1} = 1.16 \\\\ z_{\\text {women&#39;s height}} = \\dfrac{x - \\mu}{\\sigma} = \\dfrac{68 - 63.8}{2.8} = 1.50 \\\\ \\] \\(\\therefore\\) the woman is taller, relative to the population of women’s heights. Example: Finding A Value with a Given Z-Score Data from the National Health and Examination survey estimate the mean total cholesterol level, in milligrams per deciliter, in the adult population to be \\(\\mu = 200\\), with a standard deviation of \\(\\sigma = 44\\). What is the cholesterol level of a person with a z-score of \\(1.5?\\) Solution: The cholesterol level for the person with a z-score of \\(1.5\\) is \\(1.5\\) standard deviations above the mean. This person’s cholesterol level is: \\[x = \\mu + z \\sigma = 200 + 1.5(44) = 266\\] 4.6 Percentiles and Quartiles Percentiles are measures of location, denoted \\(P_1, P_2, \\cdots, P_{99}\\), which divide a set of data into \\(100\\) groups with about \\(1\\%\\) of the values in each group. Example - the \\(50th\\) percentile, denoted \\(P_{50}\\) has about \\(50\\%\\) of the data values below it. The \\(n^{th}\\) percentile is the data value such that \\(n\\) percent of the data lies below that value. \\[ \\text{Percentile of value } x = \\frac{\\text{number of values less than } x}{\\text{total number of values}} \\times 100 \\] Finding a Percentile \\(\\text {Table: (Sorted) Verizon Airport Data Speeds (Mbps)}\\) Find the percentile for the data of \\(11.8\\) mbps. There are \\(50\\) data speeds in the table. There are \\(20\\) data speeds less than \\(11.8\\) mbps. \\(\\therefore\\) percentile of \\(11.8 = \\dfrac{20}{50} \\cdot 100 = 40\\) Interpretation: A data speed of \\(11.8\\) mbps is in the \\(40\\)th percentile. This means \\(40\\%\\) of the airports reported data speeds less than \\(11.8\\) mbps. Find the \\(40th\\) percentile. First, compute the locator, \\(L = \\dfrac{k}{100} \\cdot n = \\dfrac{40}{100} \\cdot 50 = 20.\\) By definition, \\(P_{40}\\) below which \\(40\\%\\) of the data points fall. Hence, the \\(40\\)th percentile is midway between the \\(20\\)th and \\(21\\)st value. In the table above, \\(20th\\) value is \\(11.6\\) and \\(21\\)st value is \\(11.8\\), so the midway between them is \\(11.7\\) mbps. \\(\\therefore P_{40} = 11.7\\) mbps. Note: in some references, a percentile value is chosen from the existing data. Therefore, the \\(21st\\) data point \\(11.8\\) can also be considered the \\(40th\\) percentile value. This answer is consistent with the previous example. Find the \\(25th\\) percentile. \\(L = \\dfrac{k}{100} \\cdot n = \\dfrac{25}{100} \\cdot 50 = 12.5.\\) In this case, we round up the value of \\(L\\) to get \\(13\\) to make sure at least \\(12.5\\) data points fall below that point. \\(\\therefore P_{25} = 7.9\\) mbps. Three Quartiles \\((Q_1, Q_2, Q_3)\\) \\(Q_1\\) represents the first quartile, which is the 25th percentile, and is the median of the smaller half of the data set. \\(Q_2\\) represents the second quartile, which is equivalent to the 50th percentile (i.e. the median). \\(Q_3\\) represents the third quartile, or 75th percentile, and is the median of the larger half of the data set Interquartile Range \\((IQR) = Q_3 - Q_1\\) 4.7 Boxplot 5-Number Summary For a set of data, the 5-number summary consists of five values: minimum first quartile, \\(Q_1\\) second quartile (median), \\(Q_2\\) third quartile, \\(Q_3\\) Maximum A boxplot (or box-and-whisker diagram) is a graph of a data set that consists of a line extending from the minimum value to the maximum value, and a box with lines drawn at the first quartile \\((Q_1)\\), the median \\((Q_2)\\), and the third quartile \\((Q_3)\\). Outlier and Fences When in the context of a box plot, define an outlier as an observation that right fence: \\[ Q_3 + 1.5 \\times IQR &lt; \\text {outlier} \\] left fence: \\[ \\text {outlier} &lt; Q_1 - 1.5 \\times IQR \\] Such points are marked using a dot or asterisk in a box plot. Example: Data: \\([5, 5, 9, 10, 15, 16, 20, 30, 40]\\) \\[ \\text{Q}_1 = 9 \\\\ \\text{Q}_2 = 15 \\\\ \\text{Q}_3 = 20 \\\\ \\text{IQR} = 20 - 9 = 11 \\\\ \\text{LF} = \\text{Q}_1 - 1.5 \\times \\text{IQR} = -7.5 \\\\ \\text{UF} = \\text{Q}_3 + 1.5 \\times \\text{IQR} = 36.5 \\\\ \\] Five Number Summary: Min. 1st Qu. Median Mean 3rd Qu. Max. 5.00 9.00 15.00 16.67 20.00 40.00 Exercise: Drawing a Boxplot with an Outlier Students in one of the author’s statistics classes were surveyed about the number of novels they read in the past year. Here are the anonymous responses (in numbers of novels) of 11 of the students: \\([2, 5, 2, 0, 2, 3, 0, 5, 6, 4, 12]\\) Construct a boxplot. Check if there is any outlier. Box Plot and Shape of the Histogram 4.8 Group Comparison Comparing distributions of median household income for counties by population gain status Source: OpenIntroOrg "],["discrete-probability-distributions.html", "Chapter 5 Discrete Probability Distributions 5.1 Definitions 5.2 Finding Probabilities 5.3 Random Variables 5.4 Probability Distribution for a Discrete Random Variable 5.5 Parameters of a Probability Distribution 5.6 Binomial Probability Distribution 5.7 Example Problems", " Chapter 5 Discrete Probability Distributions Learning Outcome Compute measures of expectation and variation for a discrete probability distribution. In this chapter, we will extend the concept of relative frequencies to understand and calculate the probability of occurrence of a random event. We will also learn about normal distribution, its properties, and methods to calculate probabilities of random events that are described by this distribution. 5.1 Definitions An event is any collection of results or outcomes of a procedure. For example, tossing a coin is an event with possible outcomes heads and tails. A simple event is an event that has one outcome. For example, births of \\(2\\) girls followed by a boy is a simple event because the only possible outcome is \\(\\{ggb\\}\\). However, births of \\(2\\) girls and a boy is an event that has three possible outcomes \\(\\{ggb, gbg, bgg \\}\\). A sample space for a procedure consists of all possible simple events. For example, with births of three children, the sample space consists of eight different simple events \\(\\{ bbb, bbg, bgb, bgg, gbb, gbg, ggb, ggg\\}\\) An event is random if individual outcomes of it are unpredictable, meaning they have no apparent pattern of occurrence, but there is nonetheless a predictable distribution (i.e. the frequencies) of those different outcomes over a large number of repetitions of the event. The probability of any outcome of a random event can be defined as the proportion of times the outcome would occur in a very long series of repetitions. The probability is defined as a proportion, and it always takes values between \\(0\\) and \\(1\\) (inclusively). It may also be displayed as a percentage between \\(0\\%\\) and \\(100\\%\\). \\(0\\%: \\text{event is impossible}\\) \\(100\\%: \\text{event is certain}\\) THEORETICAL VERSUS EXPERIMENTAL PROBABILITY The Theoretical probability is the likelihood of occurring of an event. It is simply the ratio of the number of desired outcomes and the number of all possible outcomes. The experimental probability is an estimate of the likelihood of occurring of an event based on repeated trials. The subjective probability is an estimate of the likelihood of occurring of an event based on someone’s belief. LAW OF LARGE NUMBERS Consider: Rolling a 1 of a die If the sample space of a random experiment consists of \\(n\\) equally likely outcomes and an event \\(E\\) consists of \\(m\\) of those outcomes, then \\[\\text {Theorerical Probability} : P(E) = \\frac{\\text{number of outcomes in the event}(m)}{\\text{total number of outcomes}(n)}\\] Let \\(\\hat{p_n}\\) be the proportion of outcomes that are \\(1\\) after the \\(n\\) rolls. As the number of rolls \\((n)\\) increases, \\(\\hat{p_n}\\) (the relative frequency of rolls, or the experimental probability) will converge to the theoretical probability of rolling a \\(1,\\space p = 1/6.\\) The figure shows the convergence for \\(100,000\\) die rolls. The tendency of \\(\\hat{p_n}\\) to stabilize around \\(p\\), i.e. the tendency of the relative frequency to stabilize around the true probability, is described by the Law of Large Numbers. As more observations are collected, the observed proportion \\(\\hat{p_n}\\) of occurrences with a particular outcome after \\(n\\) trials converges to the true probability \\(p\\) of that outcome. Die Rolls Simulation The figure shows the fraction of die rolls that are \\(1\\) at each stage in a simulation. The relative frequency tends to get closer to the probability \\(1/6 \\approx 0.167\\) as the number of rolls increases. Example: Calculating Classical Probabilities Assuming that births of boys and girls are equally likely, find the probability of getting children of all of the same gender when three children are born. Recall the sample space of three children: \\(\\{ bbb, bbg, bgb, bgg, gbb, gbg, ggb, ggg \\}\\), includes eight equally likely outcomes, and there are exactly two outcomes in which three children are of the same gender: \\(\\{ bbb, ggg \\}\\) . \\[ P(\\text{ three children of the same gender}) = \\dfrac{2}{8} = \\dfrac{1}{4} = 0.25 \\] 5.2 Finding Probabilities This section presents the addition and multiplication rules of calculating probabilities. 5.2.1 Disjoint or mutually exclusive outcomes Two events or outcomes are called disjoint or mutually exclusive if they cannot both happen in the same trial. When rolling a die, the outcomes \\(1\\) and \\(2\\) are disjoint, and we compute the probability that one of these outcomes will occur by adding their separate probabilities: \\[P(1 \\text{ or } 2)=P(1)+P(2)=1/6+1/6=1/3\\] What about the probability of rolling a \\(1, 2, 3, 4, 5, \\ or \\ 6\\) ? \\[ \\begin{array}{ll} P(1 \\text{ or } 2 \\text{ or } 3 \\text{ or } 4 \\text{ or } 5 \\text{ or }6) = P(1)+P(2)+P(3)+P(4)+P(5)+P(6) \\\\ =1/6+1/6+1/6+1/6+1/6+1/6 =1 \\end{array} \\] It is no surprise that the probability is \\(1\\), since it is certain that one of the six outcomes must occur. ADDITION RULE OF DISJOINT OUTCOMES If \\(A_1,...,A_k\\) represent \\(k\\) disjoint outcomes, then the probability that one of them occurs is given by: \\[P(A_1\\text{ or }A_2 \\text{ or ... or }A_k)=P(A_1)+P(A_2)+...+P(A_k)\\] Example: Consider a standard deck of cards. \\[ \\text {4 suits} \\left\\{ \\begin{array}{ll} \\text{hearts: } \\color{red}{\\heartsuit} \\\\ \\text{diamonds: } \\color{red}{\\diamondsuit} \\\\ \\text{clubs: } \\spadesuit \\\\ \\text{spades: } \\clubsuit \\end{array} \\right. \\] \\[\\text{13 cards in each suit: } Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King\\] One card is dealt from a well shuffled deck. \\[ \\begin{align} P(\\text{the card is an ace or a king}) &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a king}) \\\\ &amp; = 4/52+4/52 \\\\ &amp; = 8/52 \\\\ &amp; = 2/13 \\end{align} \\] Venn Diagram - a diagram style to illustrate simple set relationships in probability. Venn Diagram | When events are disjoint \\[ \\begin{align} P(\\text{the card is an ace or a king}) &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a king}) \\\\ &amp; = 4/52+4/52 \\\\ &amp; = 2/13 \\end{align} \\] ## (polygon[GRID.polygon.471], polygon[GRID.polygon.472], polygon[GRID.polygon.473], polygon[GRID.polygon.474], text[GRID.text.475], text[GRID.text.476], text[GRID.text.477], text[GRID.text.478]) 5.2.2 Probabilities when events are NOT disjoint or mutually exclusive \\[ \\begin{align} &amp; P(\\text{the card is an ace or a heart}) \\\\ &amp;= P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a heart})-P(\\text{it&#39;s an ace &amp; heart}) \\\\ &amp; = 4/52+13/52 - \\underbrace{1/52}_{\\text {adjustment made to avoid double-counting of the ace of hearts}} \\\\ &amp; = 16/52 \\\\ &amp; = 4/13 \\end{align} \\] Venn Diagram | When events are not disjoint \\[ \\begin{align} &amp; P(\\text{the card is an ace or a heart}) \\\\ &amp; = P(\\text{it&#39;s an ace})+P(\\text {it&#39;s a heart})-P(\\text{it&#39;s an ace AND heart}) \\\\ &amp; = 4/52+13/52 - 1/52 = 16/52 \\end{align} \\] FALSE (polygon[GRID.polygon.479], polygon[GRID.polygon.480], polygon[GRID.polygon.481], polygon[GRID.polygon.482], text[GRID.text.483], text[GRID.text.484], text[GRID.text.485], text[GRID.text.486], text[GRID.text.487]) GENERAL ADDITION RULE OF PROBABILITIES \\[ \\bbox[yellow,5px] {\\color{black}{P(A \\space or \\space B) = P(A) + P(B) - P(A \\space and \\space B)}} \\] where \\(P(A \\text{ and } B)\\) is the probability that both events occur. If \\(A\\) and \\(B\\) are mutually exclusive, \\(P(A \\space and \\space B) = 0\\) Therefore, \\[ P(A \\space or \\space B) = P(A) + P(B)\\] 5.2.3 Complement of an event The complement of event \\(A\\) is denoted \\(A^c\\), and \\(A^c\\) represents all outcomes not in \\(A\\). \\(A\\) and \\(A^c\\) are mathematically related: \\[ \\begin{align} &amp; P(A) + P(A^c) = 1 \\\\ or, \\space &amp; P(A^c) = 1 - P(A) \\end{align} \\] Example: if an event has chance \\(40\\%\\), then the chance that it doesn’t happen is \\(60\\%\\). Venn Diagram | Exercise \\[ \\begin{align} P(email) &amp;=0.73 \\\\ P(text) &amp;= 0.62 \\\\ P(\\text {email &amp; text}) &amp;= 0.49 \\\\ P(\\text {only email}) &amp;= 0.73 - 0.49 = 0.24 \\\\ P(\\text{only text}) &amp;= 0.62 - 0.49 = 0.13 \\\\ P(\\text{neither email nor text}) &amp;= 1 - (0.24 + 0.49 + 0.13) = 0.14 \\end{align} \\] (polygon[GRID.polygon.488], polygon[GRID.polygon.489], polygon[GRID.polygon.490], polygon[GRID.polygon.491], text[GRID.text.492], text[GRID.text.493], text[GRID.text.494], text[GRID.text.495], text[GRID.text.496]) 5.2.4 Multiplication Rule | for independent processes If \\(A\\) and \\(B\\) represent events from two different and independent processes, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their seprarate probabilities: \\[P(A \\text{ and } B) = P(A) \\times P(B)\\] Similarly, if there are \\(k\\) events \\(A_1,...,A_k\\) from \\(k\\) independent processes, then the probability they all occur is \\[ \\bbox[yellow,5px] { \\color{black} {P(A_1\\text{ and }A_2 \\text{ and ... and }A_k)=P(A_1)\\times P(A_2)\\times...\\times P(A_k)} } \\] Example 1: If a card is randomly drawn from a well-shuffled deck, what is the probability that it is the ace of hearts? [Note: Ace and Hearts are two independent events.] \\[ \\begin{align} P(Ace \\text{ and } Hearts) &amp;= P(Ace) \\times P(Hearts) \\\\ &amp;= (4/52) \\times (13/52) = 1/52 \\end{align} \\] Example 2: About \\(9\\%\\) of people are left-handed. Suppose \\(5\\) people are selected at random from the US population. (a) What is the probability that all are right-handed? (b) What is the probability that all are left-handed? (c) What is the probability that not all of them are right-handed? \\[ \\begin{align} &amp;(a) \\space P\\text{(All are RH)} = (1-0.09)^5 = 0.624 \\\\ &amp;(b) \\space P\\text{(All are LH)} = (0.09)^5 = 0.0000059 \\\\ &amp;(c) \\space P\\text{(not all RH)} = 1- P(\\text {all RH}) = 1-0.624 = 0.376 \\end{align} \\] GENERAL MULTIPLICATION RULE If \\(A\\) and \\(B\\) represent two outcomes or events, then \\[ \\bbox[yellow,5px] {\\color{black}{P(A \\space and \\space B) = P(A|B) \\times P(B)}} \\] Example 3: During the smallpox outbreak, \\(96.08\\%\\) of Boston residents were not inoculated, and \\(85.88\\%\\) of the residents who were not inoculated ended up surviving. What is the probability that a resident was not inoculated and lived? To answer the question, we want to determine \\(\\text {P(lived and inoculated)}\\), and we are given that, \\[ \\begin{align} P(\\text{lived | not inoculated}) &amp;= 0.8588 \\\\ P(\\text {not inoculated}) &amp;= 0.9608 \\end{align} \\] Among the \\(96.08\\%\\) of people who were not inoculated, \\(85.88\\%\\) survived: \\[ P(\\text {lived and not inoculated}) = 0.8588 \\times 0.9608 = 0.8251 \\] This is equivalent to the General Multiplication Rule. 5.2.5 Marginal and joint probabilities If a probability is based on a single variable, it is a marginal probability. The probability of outcomes for two or more variables or processes is called a joint probability. Exercise: Calculating Probabilities with a Contingency Table: Table: College enrollment and parents’ educational attainment \\[ \\begin{array} {l|cc|r} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{total} \\\\ \\hline \\text {teen: college} &amp; 231 &amp; 214 &amp; 445 \\\\ \\text {teen: no college} &amp; 49 &amp; 298 &amp; 347 \\\\ \\hline \\text {total} &amp; 280 &amp; 512 &amp; 792 \\end{array} \\] a) Finding Marginal and Joint Probabilities: \\[ \\begin{array} {l|cc|c} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{marginal} \\\\ \\hline \\text {teen: college} &amp; \\color{red}{0.29} &amp; \\color{red}{0.27} &amp; \\color{blue}{0.56} \\\\ \\text {teen: no college} &amp; \\color{red}{0.06} &amp; \\color{red}{0.38} &amp; \\color{blue}{0.44} \\\\ \\hline \\text {marginal} &amp; \\color{blue}{0.35} &amp; \\color{blue}{0.65} &amp; 1.00 \\end{array} \\] \\[ \\begin{align} &amp;\\color{blue}{\\text{Marginal Probability: }} P(\\text{teen: college})=\\frac{445}{792}=0.56 \\\\ &amp;\\color{red}{\\text{Joint Probability: }} P(\\text {teen: college and parents: no degree})=\\frac{214}{792}=0.27 \\end{align} \\] b) Finding Conditional Probability: Conditional Probability The conditional probability of the outcome of interest \\(A\\) given condition \\(B\\) is computed as the following: \\[P(A|B) = \\frac{P(A \\text{ and } B)}{P(B)}\\] \\[ \\begin{array} {l|cc|r} &amp; \\text{parents: degree} &amp; \\text{parents: no degree} &amp; \\text{total} \\\\ \\hline \\text {teen: college} &amp; 231 &amp; 214 &amp; 445 \\\\ \\text {teen: no college} &amp; 49 &amp; 298 &amp; 347 \\\\ \\hline \\text {total} &amp; 280 &amp; 512 &amp; 792 \\end{array} \\] \\[ \\begin{align} P(\\text {teen college | parents degree}) &amp;= \\frac{231/792}{280/792} = 0.825 \\\\ P(\\text {teen college | parents no degree}) &amp;= \\frac{214/792}{512/792} = 0.418 \\\\ P(\\text {teen no college | parents degree}) &amp;= \\frac{49/792}{280/792} = 0.175 \\\\ P(\\text {teen no college | parents no degree}) &amp;= \\frac{298/792}{512/792} = 0.582 \\end{align} \\] c) Condition of Independence Verify whether one of the following equations holds: \\[ \\begin{align} P(A|B) &amp;= P(A) \\tag 1 \\\\ P(A \\space and \\space B) &amp;=P (A) \\times P(B) \\tag 2 \\end{align} \\] Check if the equality holds in the following equation: \\[ \\begin{align} P(\\text{teen college | parent degree})&amp;\\stackrel{?}{=} P(\\text {teen college}) \\\\ 0.825 &amp;\\ne 0.560 \\end{align} \\] Because both sides are not equal, teenager college attendance and parent degree are not independent. Two events are mutually exclusive If \\(A\\) and \\(B\\) are mutually exclusive events, then they cannot occur at the same time. If asked to determine if events \\(A\\) and \\(B\\) are mutually exclusive, verify one of the following equations holds: \\[ \\begin{align} P(\\text{A and B})&amp;= 0 \\tag 1 \\\\ P(\\text{A or B}) &amp;= P(A)+P(B) \\tag 2 \\end{align} \\] If the equation that is checked holds true, \\(A\\) and \\(B\\) are mutually exclusive. If the equation does not hold, then \\(A\\) and \\(B\\) are not mutually exclusive. At Least One A poker hand (5 cards) is dealt from a well shuffled deck. What is the chance that there is at least one ace in the hand? \\[ \\begin{align} &amp;P(\\text{at least one ace}) \\\\ &amp;=1-P(\\text{no aces}) \\\\ &amp;=1-(48/52) \\times (47/51) \\times (46/50) \\times (45/49) \\times (44/48) \\\\ &amp;=34.11\\% \\end{align} \\] 5.3 Random Variables A random variable is a numerical measure of an outcome from a random experiment. It has a single numerical value, determined by chance, for each outcome of an event. We often use a capital letter such as \\(X\\) to stand for a random variable. Let \\(X\\) be a random variable representing all possible outcomes of rolling a six-sided die once. Find the given probability: \\(1. P(X=4)\\) \\(2. P(X\\le 4)\\) \\(3. P(X&gt;4)\\) \\(4. P(3 \\le X \\le 6)\\) A discrete random variable has a collection of values that is finite or countable, such as number of tosses of a coin before getting heads. A continuous random variable has infinitely many values, and the collection of values is not countable, such as body temperature. 5.4 Probability Distribution for a Discrete Random Variable A probability distribution for a discrete random variable is a description that gives the probability for each value of the random variable. It can be expressed as a table or graph, or formula. Properties There is a numerical (not categorical) random variable \\(x\\), and its number values are associated with corresponding probabilities. \\(\\sum P(x)=1\\), where \\(x\\) assumes all possible values. \\(0 \\le P(x) \\le 1\\) for every individual value of the random variable \\(x\\). \\[ \\begin{array}{c|lcr} x: \\text{ number of heads} \\\\ \\text {when two coins are tossed} &amp; P(x) \\\\ \\hline 0 &amp; 0.25 \\\\ 1 &amp; 0.50 \\\\ 2 &amp; 0.25 \\end{array} \\] Find the probability of getting two heads out of two tosses? Find the probability of getting at least one head out of two tosses? Find the probability of getting at most one head out of two tosses? 5.5 Parameters of a Probability Distribution Mean \\(\\mu\\) of a probability distribution \\(\\mu = \\sum [x_i \\cdot P(x_i)]\\) Variance \\(\\sigma^2\\) for a probability distribution \\(\\sigma^2 = \\sum[(x_i-\\mu)^2 \\cdot P(x_i)] = \\sum[x_i^2 \\cdot P(x_i)]-\\mu^2\\) Standard deviation \\(\\sigma\\) for a probability distribution \\(\\sigma = \\sqrt{\\sum[x_i^2 \\cdot P(x_i)]-\\mu^2}\\) Expected Value In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents. For example, the expected value in rolling a six-sided die is \\(3.5\\), because the average of all the numbers that come up in an extremely large number of rolls is close to \\(3.5\\). The law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same principle applies to an absolutely continuous random variable, except that an integral of the variable with respect to its probability density replaces the sum. The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variable’s value from the variable’s expected value. Expected value of a discrete random variable If \\(X\\) takes outcomes \\(x_1, x_2,\\cdots,x_m\\) with probabilities \\(p_1, p_2,\\cdots, p_m\\) the expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[ \\begin{align} E(X) &amp;= \\mu_X = x_1 \\times p_1 + x_2 \\times p_2 +\\cdots+x_m \\times p_m \\\\ &amp;= \\sum^{m}_{i=1}(x_i \\times p_i) \\end{align} \\] Example: \\(\\text{ Random Variable } X: \\text{the number of spots on one roll of a die}\\) Probability distribution table for \\(X\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6\\\\ \\hline P(x) &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\end{array} \\] \\[ E(X) = 1 \\cdot (1/6) + 2 \\cdot (1/6) + 3 \\cdot (1/6) + 4 \\cdot (1/6) + 5 \\cdot (1/6) + 6 \\cdot (1/6) = 3.5 \\] Variability in Discrete Random Variables Variance and standard deviation of a discrete random variable If \\(X\\) takes outcomes \\(x_1, x_2, \\cdots ,x_m\\) with probabilities \\(p_1, p_2, \\cdots ,p_m\\) and expected value \\(\\mu_x = E(X),\\) then to find the standard deviation of \\(X\\), we first find the variance and then take its square root. \\[ \\begin{align} Var(X)=\\sigma^2_x &amp;= (x_1 - \\mu_x)^2 \\times p_1 + (x_2 - \\mu_x)^2 \\times p_2 + \\cdots + (x_m - \\mu_x)^2 \\times p_m \\\\ &amp;= \\sum^m_{i=1}(x_i-\\mu_x)^2 \\times p_i \\\\ \\\\ SD(X) = \\sigma_x &amp;= \\sqrt{\\sum^m_{i=1}(x_i-\\mu_x)^2 \\times p_i} \\\\ \\\\ \\text {From the above example, } \\\\ \\\\ Var(X)=\\sigma^2_x &amp;= [(1 - 3.5)^2 + (2 - 3.5)^2 + \\cdots + (6 - 3.5)^2]\\times (1/6) \\\\ &amp;= 2.92 \\\\ \\\\ SD(X) = \\sigma_x &amp;= \\sqrt{2.92} = 1.71 \\end{align} \\] Exercise 1: Coin Toss \\[ \\begin{array}{c|lcr} x : \\text{ number of heads} \\\\ \\text {when two coins are tossed} &amp; P(x) \\\\ \\hline 0 &amp; 0.25 \\\\ 1 &amp; 0.50 \\\\ 2 &amp; 0.25 \\end{array} \\] Calculate expected value/average number of heads and SD from three tosses. Solution: \\[\\begin{align} E(X) &amp;= 0 \\times .25 + 1 \\times .50 + 2 \\times .25 = 1 \\\\ VAR(X) &amp;= (0 - 1)^2 \\times .25 + (1 - 1)^2 \\times .50 + (2 - 1)^2 \\times .25 \\\\ &amp;= .5 \\\\ SD(X) &amp;= \\sqrt {.5} \\\\ &amp; = .7071 \\end{align}\\] Exercise 2: Be A Better Bettor \\[ \\begin{array}{c|c|lcr} \\text{Roulette} \\\\ \\text{Event} &amp; x &amp; P(x) \\\\ \\hline \\text{Lose} &amp; - \\$ 5 &amp; \\frac{37}{38} \\\\ \\text{Win} &amp; \\$175 &amp; \\frac{1}{38} \\end{array} \\] \\[ \\begin{array}{c|c|lcr} \\text{Craps Game} \\\\ \\text{Event} &amp; x &amp; P(x) \\\\ \\hline \\text{Lose} &amp; - \\$ 5 &amp; \\frac{251}{495} \\\\ \\text{Win} &amp; \\$5 &amp; \\frac{244}{495} \\end{array} \\] Which of the bets is better in the sense of producing higher expected value? Solution: Roulette \\[ E(X) = (- \\$ 5) \\cdot (\\frac{37}{38}) + (\\$175) \\cdot (\\frac{1}{38}) = -\\$.26 \\] Craps Game \\[ E(X) = (- \\$ 5) \\cdot (\\frac{251}{495}) + (\\$5) \\cdot (\\frac{244}{495}) = -\\$.07 \\] \\(\\therefore\\) Craps Game seems to be a better bet since it has the higher expected return. However, both games will generate profit for the casino owner. 5.6 Binomial Probability Distribution A binomial probability distribution results from a procedure that meets the following requirements: The procedure has a fixed number of trials (A trial is a single observation). The trials must be independent, meaning that the outcome of any individual trial doesn’t affect the probabilities in the other trials. Each trial must have all outcomes classified into exactly two categories, commonly referred to as success and failure. The probability of a success remains the same in all trials. Notation for binomial probability distribution \\[ \\begin{align} S, F &amp;: \\text {success and failure denote the two possible outcomes from each trial.} \\\\ P(S) &amp;= p \\text{ probability of success in one of the } n \\text { trials} \\\\ P(F) &amp;= q = 1 - p \\text{ probability of failure in one of the } n \\text { trials} \\\\ n &amp;: \\text { number of trials} \\\\ x &amp;: \\text {number of successes in } n \\text { trials}; 0 \\le x \\le n \\\\ \\end{align} \\] Probability Distribution of the number of observing \\(1\\) in \\(3\\) independent Rolls of a dice Probability distribution of the number of successes \\((x)\\) in \\(n = 3\\) independent success/failure trials, each of which is a success with chance \\(p = \\dfrac{1}{6}.\\) \\[ \\begin{array}{rclr} x=k &amp; \\text{pattern} &amp; \\text{chance of pattern} &amp; P(x) \\\\ \\hline 0 &amp; FFF &amp; (5/6) \\cdot (5/6) \\cdot (5/6) &amp; 1 \\cdot (5/6)^3=0.5787 \\\\ \\hline 1 &amp; SFF &amp; (1/6) \\cdot (5/6) \\cdot (5/6) &amp; \\\\ &amp; FSF &amp; (5/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; FFS &amp; (5/6) \\cdot (5/6) \\cdot (1/6) &amp; 3 \\cdot (1/6) \\cdot (5/6)^2 = 0.3472 \\\\ \\hline 2 &amp; SSF &amp; (1/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; SFS &amp; (1/6) \\cdot (5/6) \\cdot (1/6) &amp; \\\\ &amp; FSS &amp; (5/6) \\cdot (1/6) \\cdot (1/6) &amp; 3 \\cdot (1/6)^2 \\cdot (5/6) = 0.0694 \\\\ \\hline 3 &amp; SSS &amp; (1/6) \\cdot (1/6) \\cdot (1/6) &amp; 1 \\cdot (1/6)^3 = 0.0046 \\\\ \\hline &amp; &amp; &amp; \\sum = 1.0000 \\end{array} \\] Probability Mass Function for a Binomial Random Variable Let’s reconsider the table above. \\[ \\begin{array}{rclr} x=k &amp; \\text{pattern} &amp; \\text{chance of pattern} &amp; P(x) \\\\ \\hline 0 &amp; FFF &amp; (5/6) \\cdot (5/6) \\cdot (5/6) &amp; \\underbrace{1}_{\\displaystyle \\binom{3}{0}} \\cdot \\underbrace{(5/6)^3}_{\\displaystyle (1/6)^0 \\cdot (5/6)^3} = 0.5787 \\\\ \\hline 1 &amp; SFF &amp; (1/6) \\cdot (5/6) \\cdot (5/6) &amp; \\\\ &amp; FSF &amp; (5/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; FFS &amp; (5/6) \\cdot (5/6) \\cdot (1/6) &amp; \\underbrace{3}_{\\displaystyle \\binom{3}{1}} \\cdot \\underbrace{(1/6) \\cdot (5/6)^2}_{\\displaystyle (1/6)^1 \\cdot (5/6)^2} = 0.3472 \\\\ \\hline 2 &amp; SSF &amp; (1/6) \\cdot (1/6) \\cdot (5/6) &amp; \\\\ &amp; SFS &amp; (1/6) \\cdot (5/6) \\cdot (1/6) &amp; \\\\ &amp; FSS &amp; (5/6) \\cdot (1/6) \\cdot (1/6) &amp; \\underbrace{3}_{\\displaystyle \\binom{3}{2}} \\cdot \\underbrace{(1/6)^2 \\cdot (5/6)^1}_{\\displaystyle (1/6)^2 \\cdot (5/6)^1} = 0.0694 \\\\ \\hline 3 &amp; SSS &amp; (1/6) \\cdot (1/6) \\cdot (1/6) &amp; \\underbrace{1}_{\\displaystyle \\binom{3}{3}} \\cdot \\underbrace{(1/6)^3}_{\\displaystyle (1/6)^3 \\cdot (5/6)^0} = 0.0046 \\\\ \\hline &amp; &amp; &amp; \\sum = 1.0000 \\end{array} \\] The probability of \\(k\\) successes in \\(n\\) independent trials: \\[ \\begin{align} \\displaystyle P(x) &amp;= \\binom{n}{x}(p)^x(1-p)^{n-x} \\\\ &amp;= \\underbrace{\\dfrac{n!}{x!(n-x)!}}_{\\text{The number of outcomes with} \\\\ \\text{exactly x successes among n trials }} \\times \\underbrace{(p)^x(1-p)^{n-x}}_{\\text{ The probability of x successes among} \\\\ \\text{ n trials for any particular order}} \\end{align} \\] where, \\[ \\begin{cases} x = 0,1,2,......,n \\\\ n = \\text{ number of trials} \\\\ k = \\text{ number of successes among } n \\text{ trials} \\\\ p = \\text{ probability of success in any one trial} \\end{cases} \\] When \\(k = 0,\\) the chance of no successes (in other words, the chance of \\(n\\) failures in a row) is \\[ \\frac{n!}{0!(n)!}p^0(1-p)^{n} = (1-p)^n \\] Question: What is the probability that the first success will be observed after \\(n\\) trials? This means the first success is observed on \\((n+1)^{th}\\) trial. \\[ \\underbrace{F}_{ 1^{st} } FF.......FF \\underbrace{F}_{ n^{th} } \\underbrace{S}_{ n+1 } \\\\ \\underbrace{FFF.......FFF}_{ \\displaystyle \\binom{n}{0} (p)^0(1-p)^n } \\underbrace{S}_{\\times p } \\\\ = (1-p)^n \\cdot p \\\\ \\text{ (This is called geometric distribution.)} \\] Sampling with Replacement A random number generator draws at random with replacement from the ten digits \\(0,1,2,3,4,5,6,7,8,9. \\space\\) Run the generator 20 times. Find the chance that \\(0\\) appears once. Solution: \\[binomial: n=20, p = 0.1, k = 1 \\implies \\binom{20}{1}(0.1)^1(0.9)^{19} = 0.2702\\] Find the chance that \\(0\\) appears at most once. Solution: \\[binomial: n=20, p = 0.1, k = (0,1) \\\\ \\implies \\binom{20}{0}(0.1)^0(0.9)^{20} + \\binom{20}{1}(0.1)^1(0.9)^{19} = 0.3917\\] Find the chance that \\(0\\) appears more than once. Solution: \\[binomial: n=20, p = 0.1, k = (2,3,...,20) \\\\ \\implies 1-P(k=0,1)= (1- 0.3917) = 0.6083.\\] Exercise: Cytomegalovirus (CMV) is a virus that infects one half of young adults. If a random sample of \\(10\\) young adults is taken, find the probability that between \\(30\\%\\) and \\(40\\%\\) (inclusive ) of those sampled will have CMV. Solution: \\[\\begin{align} P((.3)(10) \\le X \\le (.4)(10)) &amp;= P(X=3) + P(X=4) \\\\ &amp;= \\binom{10}{3}(.5)^3(.5)^7 + \\binom{10}{4}(.5)^4(.5)^6 \\\\ &amp;= 0.1172 + 0.2051 \\\\ &amp;= 0.3223 \\end{align}\\] 5.6.1 Expected Value of the Binomial Distribution \\(X:\\) number of successes with probability \\(p\\) of success on each trial. Probability distribution table for \\(X:\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 0 \\\\ \\hline P(x) &amp; p &amp; 1-p \\end{array} \\] \\[ \\begin{align} \\text{Average successes per trial: } E(X) &amp;= 1 \\times p + 0 \\times (1-p) = p \\\\ \\text{Expected total successes from } n \\text { trials: } E(X) &amp;= n \\times [1 \\times p + 0 \\times (1-p)] = np \\end{align} \\] 5.6.2 Standard Deviation of the Binomial Distribution \\(X:\\) number of successes with probability \\(p\\) of success on each trial. Probability distribution table for \\(X:\\) \\[ \\begin{array}{c|c|c|c|c|c|c} x &amp; 1 &amp; 0 \\\\ \\hline P(x) &amp; p &amp; 1-p \\end{array} \\] \\[ \\begin{align} \\text {for one trial} \\\\ SD(X) &amp;= \\sqrt{(1-p)^2 \\times p + (0-p)^2 \\times (1-p)} \\\\ &amp;= \\sqrt{(1-p)^2 \\times p + p^2 \\times (1-p)} \\\\ &amp;= \\sqrt{p(1-p)(1-p+p)} \\\\ &amp;= \\sqrt{p(1-p)} \\\\ \\text {for } n \\text{ trials} \\\\ SD(X) &amp;= \\sqrt{np(1-p)} \\end{align} \\] Example: Calculate expected total number of heads from \\(100\\) tosses \\(X\\) is a binomial distributed random variable with parameters \\(n=100\\) and \\(p=0.5\\) \\[ \\begin{align} P(x=k) &amp;= \\binom{100}{k}(0.5)^k(1-0.5)^{100-k}, k = 0,1,2,...,100 \\\\ \\\\ E(X) &amp;= 100 \\times 0.5 \\\\ &amp;= 50 \\\\ \\\\ SE(X) &amp;= \\sqrt{100 \\times 0.5 \\times 0.5} \\\\ &amp;= 5 \\end{align} \\] Exercise Calculate expected total number of sixes from \\(100\\) rolls \\(X\\) has the binomial distribution with parameters \\(n=100\\) and \\(p=1/6\\) Solution: \\[ \\begin{align} P(X=k) &amp;= \\binom{100}{k}(1/6)^k(1-1/6)^{100-k}, k = 0,1,2,...,100 \\\\ \\\\ E(X) &amp;= 100 \\times 1/6 \\\\ &amp;= 16.7 \\\\ \\\\ SE(X) &amp;= \\sqrt{100 \\times 1/6 \\times 5/6} \\\\ &amp;= 3.73 \\end{align} \\] 5.7 Example Problems 1. In a state’s pick 3 lottery game, a player pays \\(\\$1.46\\) to select a sequence of three digits (from \\(0\\) to \\(9\\)), such as \\(822\\). If someone selects the same sequence of three digits that are drawn, the player collects \\(\\$499.38\\). (a) How many different selections are possible? \\[ 10 \\times 10 \\times 10 = 1000 \\] (b) What is the probability of winning? \\[ \\dfrac{1}{1000} \\] (c) If a player wins, what is their net profit? \\(\\$499.38 - \\$1.46 = \\$497.92\\) (d) Find the expected value. \\(E(X) = \\$499.38 \\times \\dfrac{1}{1000} + (-\\$1.46) \\left( 1 - \\dfrac{1}{1000}\\right) = -\\$0.96\\) 2. Based on data from Bloodjournal.org, \\(10\\%\\) of women 65 years of age or older suffer from anemia. In tests of anemia, blood samples from 8 women in that age group are combined. What is the probability that the combined sample tests positive for anemia? For the combined sample to be tested positive, at least one of the eight samples has to be tested positive. \\[ \\begin{align} P(X \\ge 1) &amp;= 1 - P(X = 0) \\\\ &amp;= 1- \\displaystyle \\binom {8}{0}(.1)^0(1-.9)^{8-0} \\\\ &amp;= 0.570 \\\\ \\end{align} \\] Therefore, there is a \\(57\\%\\) probability that the combined sample will be tested positive for anemia. 3. The MedAssist Pharmaceutical Company receives large shipments of aspirin tablets and uses this acceptance sampling plan: Randomly select and test \\(40\\) tablets, then accept the whole batch if there is only one or none that doesn’t meet the required specifications. If one shipment of \\(5000\\) aspirin tablets actually has a \\(3\\%\\) rate of deficits, what is the probability that this whole shipment will be accepted? Given, \\[ \\begin{align} n &amp;= 40 \\\\ p &amp;= 0.03 \\end{align} \\] The whole shipment will be accepted if the sample has at most one does not meet the standard. \\[ \\begin{align} P(X \\le 1) &amp;= P(X = 0) + P(X = 1) \\\\ &amp;= \\binom{40}{0}(0.03)^0(1-0.03)^{40} + \\binom{40}{1}(0.03)^1(1-0.03)^{39} \\\\ &amp;= 0.6615 \\end{align} \\] Therefore, there is a \\(66.15\\%\\) chance that the whole shipment will be accepted. 4. Results from a public hygiene poll found that \\(70\\%\\) of adults always wash their hands after using a public restroom. (a) Find the probability that among \\(8\\) randomly selected adults, exactly \\(5\\) always wash their hands after using a public restroom. \\[ \\begin{align} p &amp;= 0.7 \\\\ n &amp;= 8 \\\\ \\displaystyle P(x = 5) &amp;= \\binom{8}{5}(.7)^5(1-.7)^{8-5} = 0.2541 \\end{align} \\] (b) Find the probability that among \\(8\\) randomly selected adults, at least \\(7\\) always wash their hands after using a public restroom. \\[ \\begin{align} \\displaystyle P(x \\ge 7) &amp;= P(x = 7) + P(x = 8) \\\\ &amp;= \\binom{8}{7}(.7)^7(1-.7)^{8-7} + \\binom{8}{8}(.7)^8(1-.7)^{8-8} \\\\ &amp;= 0.2553 \\\\ \\\\ &amp;\\text{Alternatively,} \\\\ \\displaystyle P(x \\ge 7) &amp;= 1 - P(x \\le 6) \\\\ &amp;= 1 - 0.7447 \\\\ &amp;= 0.2553 \\end{align} \\] (c) For groups of randomly selected adults, find the mean and standard deviation of the numbers in the groups who always wash their hands after using a public restroom. The mean (or expected value) and the standard deviation of a binomial distributed random variable is given by: \\[ \\begin{align} \\text{for, n trials} \\\\ \\\\ E(X) &amp;= np \\\\ SD(X) &amp;= \\sqrt{np(1-p)} \\end{align} \\] \\[ \\begin{align} \\text{In this problem,} \\\\ \\\\ E(X) &amp;= (8) \\cdot (0.7) = 5.6 \\text{ adults} \\\\ SD(X) &amp;= \\sqrt{(8)(0.7)(1-0.7)} = 1.3 \\text{ adults} \\end{align} \\] 5. In a litter of seven kittens, three are female. You pick two kittens at random without replacement. (a) Create a probability model for the number of male kittens you get. \\[ \\begin{array} {l|c|c|c} \\text{number of males }(X) &amp; 0 &amp; 1 &amp; 2 \\\\ \\hline P(X) &amp; \\dfrac{3}{7} \\cdot \\dfrac{2}{6} = \\dfrac{6}{42} &amp; \\dfrac{4}{7} \\cdot \\dfrac{3}{6} + \\dfrac{3}{7} \\cdot \\dfrac{4}{6} = \\dfrac{24}{42} &amp; \\dfrac{4}{7} \\cdot \\dfrac{3}{6} = \\dfrac{12}{42} \\\\ \\hline \\end{array} \\\\ \\] (b) Find the expected number of male kittens. \\[ E(X) = 0 \\cdot \\dfrac{6}{42} + 1 \\cdot \\dfrac{24}{42} + 2 \\cdot \\dfrac{12}{42} = 1.14 \\text { males} \\\\ \\] (c) Find the standard deviation of the number of male of kittens. \\[ \\begin{align} \\sigma^2 &amp;= (0-1.14)^2 \\cdot \\dfrac{6}{42} + (1-1.14)^2 \\cdot \\dfrac{24}{42} + (2-1.14)^2 \\cdot \\dfrac{12}{42} = 0.4082 \\\\ \\\\ \\sigma &amp;= \\sqrt {0.4082} = 0.64 \\text { males} \\end{align} \\] 6. Since the stock market began in 1872, stock prices have risen in about \\(73\\%\\) of the years. Assuming that market performance is independent from year to year, what is the probability that the market will fall during at least one of the next \\(5\\) years? \\[P\\text{(market will fall at least once in 5 yr.)} = 1 - P\\text{(market will not fall at all in 5 yr.)}\\] \\[ \\begin{align} P(X \\ge 1 ) &amp;= 1 - P(X = 0) \\\\ &amp;= 1 - \\binom{5}{0}(1-0.73)^0(0.73)^5 \\\\ &amp;= 0.7927 \\end{align} \\] Hence, there is \\(79.27\\%\\) probability that the market will fall during at least one of the next \\(5\\) years 7. The Center for Disease Control and Prevention say that about \\(30\\%\\) of high school students smoke tobacco. Suppose you randomly select high school students to survey them on their attitudes toward scenes of smoking in the movies. What is the probability that there are no more than \\(2\\) smokers among \\(10\\) people you randomly choose? Given, \\[ \\begin{align} n &amp;= 10 \\\\ p &amp;= 0.3 \\end{align} \\] \\[ \\begin{align} P(X \\le 2) &amp;= P(X = 0) + P(X = 1) + P(X = 2) \\\\ &amp;= \\binom{10}{0}(.3)^0(0.7)^{10} + \\binom{10}{1}(.3)^1(0.7)^{9} + \\binom{10}{2}(.3)^2(0.7)^{8} \\\\ &amp;= 0.3828 \\end{align} \\] Therefore, there is \\(38.38\\%\\) probability that there are no more than \\(2\\) smokers among \\(10\\) people chosen. 8. Your statistics test has \\(10\\) multiple choice questions each with \\(5\\) answer choices. (a) If you randomly select an answer from each question, what is the probability that you will pass the exam? \\[ \\begin{align} p &amp;= \\dfrac{1}{5} = 0.2 \\\\ n &amp;= 10 \\\\ P(X \\ge 6) &amp;= 1 - P(X \\le 5) = 0.0064 = 0.64\\% \\end{align} \\] \\(\\therefore\\) there is less than \\(1\\%\\) chance of passing the test by random guessing. (b) What are the expected value and standard deviation of your test score? \\[ E(X) = np = 10 \\cdot 0.2 = 2 \\\\ SD(X) = \\sqrt{np(1-p)} = \\sqrt{10 \\cdot 0.2 \\cdot 0.8 } = 1.265 \\] "],["normal-probability-distributions.html", "Chapter 6 Normal Probability Distributions 6.1 Continuous Random Variable and Probability Density Function 6.2 Normal Distribution 6.3 Sampling Distribution of a Statistic 6.4 Central Limit Theorem", " Chapter 6 Normal Probability Distributions Learning Outcome Compute probability for binomial and normal distributions. In this chapter, we introduce continuous probability distributions, with the focus on normal probability distributions. We will learn how to calculate probabilities from the standard normal distribution and apply that knowledge to solve some practical problems. Finally, we will learn about sampling distribution and use it to state the Central Limit Theorem. 6.1 Continuous Random Variable and Probability Density Function A random variable \\(X\\) is continuous if possible values comprise either a single interval on the number line or a union of disjoint intervals. Variables such as weight, height, and temperature are continuous. The probability density function (PDF), \\(f_X(x)\\), is used to specify the probability of the random variable falling within a particular range of values \\(a \\le X \\le b\\). This also means that the area under a probability density curve between two values \\(a\\) and \\(b\\) represents the proportion of a population whose values are between \\(a\\) and \\(b\\). For any probability density curve, the area under the entire curve is \\(1\\), because this area represents the entire population. \\[ \\displaystyle P[a \\le X \\le b] = \\int_a^b f_X(x) dx \\] 6.2 Normal Distribution Properties of a Normal Curve A normal curve is unimodal and symmetric. The mean is equal to the median. Both are the center of the curve. Probability density function of the normal distribution: \\[f_X(x) = \\dfrac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2} \\big( \\frac{x-\\mu}{\\sigma} \\big)^2 }, -\\infty&lt;x&lt;+\\infty\\] \\(\\mu\\) is called the location parameter because it determines where the distribution is located on the horizontal axis. \\(\\sigma\\) is called the shape parameter because it determines the shape of the distribution. Linear Transformation of the Nonstandardized Normal Distribution to the Standardized Normal Distribution Recall : A z-score is the number of standard deviations that a given value \\(x\\) is above or below the mean. The \\(z\\) score is calculated by using one of the following: \\[ \\begin{align} z &amp;= \\dfrac{x-\\bar x}{s} \\\\ x = \\bar x - 2s &amp;\\implies z = -2 \\\\ x = \\bar x - s &amp;\\implies z = -1 \\\\ x = \\bar x &amp;\\implies z = 0 \\\\ x = \\bar x + s &amp;\\implies z = +1 \\\\ x = \\bar x + 2s &amp;\\implies z = +2 \\\\ \\end{align} \\] 6.2.1 Standard Normal Distribution The standard normal distribution is a normal distribution with the parameters of \\(\\mu = 0\\) and \\(\\sigma = 1\\). The total area under its density curve is equal to \\(1\\). \\[ \\bbox[yellow,5px] { \\color{black}{{P_Z(z)} = \\frac {1}{\\sqrt {2\\pi}}\\exp{-\\frac{1}{2}z^2}, -\\infty&lt;z&lt;+\\infty} } \\] Example: Compute and Interpret a \\(z\\)-Score The 2014 draft picks for NBA basketball teams have heights that are approximately normally distributed with mean 79.1 inches and standard deviation 3.0 inches (Source: nbadraft.net). Shabazz Napier was the shortest 2014 draft pick with a height of 72 inches. Find the z-score for 72 inches. What does it mean? \\[ z = \\frac {x - \\mu}{\\sigma} \\\\ z = \\frac {72 - 79.1}{3} \\\\ z = - 2.37 \\] The z-score is –2.37, which means that Napier’s height is 2.37 standard deviations less than the mean. Finding Areas Between Two \\(z\\) Scores Notation \\[ \\begin{cases} P(a &lt; z &lt; b) \\ \\text{ denotes the probability that the } z \\text{ score is between } a \\text{ and } b \\\\ P(z &gt; a) \\ \\text{ denotes the probability that the } z \\text{ score is greater than } a \\\\ P(z &lt; a) \\ \\text{ denotes the probability that the } z \\text{ score is less than } a \\end{cases} \\] Exercise: \\[ \\begin{align} &amp;1. \\text { Find } P(z &lt; -1) \\\\ &amp;2. \\text { Find } P(z &gt; 2) \\\\ &amp;3. \\text { Find } P(-3 &lt; z &lt; 3) \\\\ \\end{align} \\] Empirical Rule Probabilities for falling \\(1,\\) \\(2,\\) and \\(3\\) standard deviations of the mean in a normal distribution. Empirical Rules in terms of \\(z\\)-scores Because the z-score of an observation is the number of standard deviations that the observation is from the mean, we can restate the Empirical Rule in terms of \\(z\\)-scores. So, if a distribution is normally distributed, then \\(68\\%\\) of its z-scores lie between \\(-1\\) and \\(1\\) \\(95\\%\\) of its z-scores lie between \\(-2\\) and \\(2\\) \\(99.7\\%\\) of its z-scores lie between \\(-3\\) and \\(3\\) Calculation of percentile from \\(z\\) scores Example: Suppose cumulative SAT scores are approximated by a normal model with \\(\\mu = 1500 \\text { and } \\sigma = 300\\). What is the probability that a randomly selected SAT taker scores at least \\(1630\\) on the SAT? \\(z = \\dfrac{x-\\mu}{\\sigma}=\\dfrac{1630-1500}{300}=\\dfrac{130}{300}=0.43\\) \\(P(z\\ge0.43)=0.3336\\) The probability that a randomly selected score is at least \\(1630\\) on the SAT is \\(33\\%\\). In order words, \\(1630\\) is \\((100 - 33) = 67\\) percentile score. Example: Edward earned a \\(1400\\) on his SAT. What is his percentile? \\(z = \\dfrac{x-\\mu}{\\sigma}=\\dfrac{1400-1500}{300}=\\dfrac{100}{300}=-0.33\\) \\(P(z\\le-0.33)=0.3707\\) Edward is at the \\(37\\)th percentile. Calculation of \\(z\\) score from percentile \\(z = 1.645\\) Interpretation: \\(95\\%\\) of the area under the curve is below \\(z = 1.645.\\) Example: Carlos believes he can get into his preferred college if he scores at least in the \\(80\\)th percentile on the SAT. What score should he aim for? At \\(80th\\) percentile, \\(z = 0.84\\) \\[ \\begin{align} z &amp; = \\dfrac{x-\\mu}{\\sigma} \\\\ 0.84 &amp; = \\dfrac{x-1500}{300} \\\\ 0.84 \\times 300 + 1500 &amp; = x \\\\ x &amp; = 1752 \\end{align} \\] \\(\\therefore\\) The 80th percentile on the SAT corresponds to a score of \\(1752\\). More Exercises The U.S. Air Force requires that pilots have heights between \\(64\\) in. and \\(77\\) in. Heights of women are normally distributed with a mean of \\(63.7\\) in. and a standard deviation of \\(2.9\\) in. What percentage of women meet that height requirement? To recruit more women pilots in the Air Force, if the height requirements are relaxed to allow middle \\(95\\%\\) of women based on the height distribution \\((N \\sim (63.7, 2.9))\\), what will be the heights of the tallest and shortest women meeting the requirements? A professor gives a test and the scores are normally distributed with a mean of \\(60\\) and a standard deviation of \\(12\\). She plans to curve the scores. If she curves by adding \\(15\\) to each grade, what is the new mean and standard deviation? If the grades are curved so that the grades of \\(B\\) are given to scores above the bottom \\(70\\%\\) and below the top \\(10\\%\\), find the new numerical limits for a grade of \\(B\\). Which method of curving the grades is fairer? What is the probability that when a value is randomly selected from a normal distribution, it is an outlier? Note: outliers are defined as data values that are above \\(Q_3\\) by an amount greater than \\(1.5 \\times IQR\\) or below \\(Q_1\\) by an amount greater than \\(1.5 \\times IQR\\), where \\(IQR\\) is the interquartile range. 6.3 Sampling Distribution of a Statistic The sampling distribution of a statistic represents the distribution of all values of the statistic (e.g. \\(\\text { sample mean, sample proporton, sample variance, etc.}\\)) when all possible samples of the same size \\(n\\) are drawn from the same population. Understanding the concept of a sampling distribution is central to understanding statistical inference. Parameter and Statistics A statistic is a value from our observed data. A parameter is a value that describes the population. \\[ \\begin{array} {l|c} \\text{Name} &amp; \\text{Statistic} &amp; \\text{Parameter} \\\\ \\hline \\text {Proportion} &amp; \\hat p &amp; p \\\\ \\text {Mean} &amp; \\bar x &amp; \\mu \\\\ \\text {Std. Deviation} &amp; s &amp; \\sigma \\\\ \\text {Variance} &amp; s^2 &amp; \\sigma^2 \\\\ \\text {Correlation} &amp; r &amp; \\rho \\\\ \\text {Regression Coefficient} &amp; b &amp; \\beta \\\\ \\hline \\end{array} \\] 6.3.1 Sampling Distribution of Sample Mean The sampling distribution of the sample mean is the distribution of all values of the sample mean (or the distribution of the variable \\(\\overline X\\)) when all possible samples of the same size \\(n\\) are drawn from the same population (with mean \\(\\mu\\) and variance \\(\\sigma^2\\)). Recall: \\(E(X) = \\dfrac{1}{6}[1 + 2 + 3 + 4 + 5 + 6] = 3.5\\) We can show that \\[ \\begin{cases} E(\\overline X) = \\mu \\\\ Var(\\overline X) = \\dfrac{\\sigma^2}{n} \\end{cases} \\] Proof (optional): \\(E(\\overline X) = \\mu\\) Let \\(X_1, X_2,...,X_n\\) be \\(n\\) independently drawn observations from a population distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\overline X\\) be the mean of these \\(n\\) independent observations: \\[ \\begin{align} \\overline X &amp;= \\frac{X_1 + X_2 +...+ X_n}{n} \\\\ \\\\ E(\\overline X) &amp;= E\\bigg( \\frac{X_1 + X_2 +...+ X_n}{n} \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) E(X_1 + X_2 +...+ X_n) \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [E(X_1) + E(X_2) +...+ E(X_n)] \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [\\mu + \\mu +...+ \\mu] \\\\ &amp;= \\bigg( \\frac {1}{n} \\bigg) [n.\\mu] \\\\ &amp;= \\mu \\\\ \\end{align} \\] Proof (optional): \\(Var(\\overline X) = \\sigma^2/{n}\\) \\[ \\begin{align} \\overline X &amp;= \\frac{X_1 + X_2 +...+ X_n}{n} \\\\ \\\\ Var(\\overline X) &amp;= Var \\bigg( \\frac{X_1 + X_2 +...+ X_n}{n} \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)Var(X_1 + X_2 +...+ X_n \\bigg) \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[Var(X_1) + Var(X_2) +...+ Var(X_n)] \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[\\sigma^2 + \\sigma^2 +...+ \\sigma^2] \\\\ &amp;= \\bigg( \\frac {1}{n^2} \\bigg)[n.\\sigma^2] \\\\ &amp;= \\frac{\\sigma^2}{n} \\\\ \\sigma^2_{\\overline X} &amp;= \\frac{\\sigma^2}{n} \\\\ SD_{\\overline X} = \\sigma_{\\overline X} &amp;= {\\frac{\\sigma}{\\sqrt n}} \\end{align} \\] 6.3.2 Sampling Distribution of a Sample Proportion The sampling distribution of the sample proportion is the distribution of sample proportions (or the distribution of the variable \\(\\mathbf { \\hat p}\\)) with all samples having the same size \\(n\\) drawn from the same population (with mean proportion \\(p\\) and variance \\(p(1-p)\\)). \\[ \\begin{align} E(\\mathbf { \\hat p}) &amp;= p \\\\ Var(\\mathbf { \\hat p}) &amp;= \\dfrac{p(1-p)}{n} \\end{align} \\] Sample Means of 10,000 Trials 6.3.3 Sampling Distribution of a Sample Variance The sampling distribution of the sample variance is the distribution of sample variances (i.e. the variable \\(s^2\\)), with all samples having the same sample size \\(n\\) taken from the same population. Sample Means of 10,000 Trials Recall: \\(\\sigma^2 = \\dfrac{1}{6}[(1 - 3.5)^2 + (2 - 3.5)^2 + (3 - 3.5)^2 + (4 - 3.5)^2 + (5 - 3.5)^2 + (6 - 3.5)^2] \\\\ = 2.9\\) Notice that the distribution of sample variances tends to be a distribution skewed to the right. 6.4 Central Limit Theorem When taking a random sample of independent observations from a population with a fixed mean and standard deviation, the distribution of \\(\\bar x\\) approaches a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\), as \\(n\\) increases. Normal Approximation for the Sampling Distribution Three important facts about the distribution of a sample proportion \\(\\bar x\\) Population has a normal distribution or \\(n &gt; 30\\) \\[ \\begin{align} \\text {Mean of all values of } \\bar x &amp;: \\mu_{\\bar x} = \\mu \\\\ \\text {Standard deviation of all values of } \\bar x &amp;: \\sigma_{\\bar x} = \\dfrac{\\sigma}{\\sqrt n} \\\\ z \\text { score conversion of } \\bar x &amp;: z = \\dfrac{\\bar x - \\mu}{\\frac{\\sigma}{\\sqrt n}} \\\\ \\end{align} \\] Original population is not normally distributed and \\(n \\le 30\\): The distribution of \\(\\bar x\\) cannot be approximated well by a normal distribution, and the methods of this section do not apply. Three important facts about the distribution of a sample proportion \\(\\hat p\\) When \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\) \\[ \\begin{align} \\text {Mean of all values of } \\hat p &amp;: \\mu_{\\hat p} = p \\\\ \\text {Standard deviation of all values of } \\hat p &amp;: \\sigma_{\\hat p} = \\sqrt{\\dfrac{p(1-p)}{n}} \\\\ z \\text { score conversion of } \\hat p &amp;: z = \\dfrac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\\\ \\end{align} \\] When \\(np \\lt 10\\) and \\(n(1-p) \\lt 10\\): The distribution of \\(\\hat p\\) cannot be approximated well by a normal distribution, and the methods of this section do not apply. Problem 1: In the 2012 Cherry Blossom 10 mile run, the average time for all of the runners is \\(94.52\\) minutes with a standard deviation of \\(8.97\\) minutes. The distribution of run times is approximately normal. Find the probability that a randomly selected runner completes the run in less than \\(90\\) minutes. Solution: Because the distribution of run times is approximately normal, we can use normal approximation. \\[ \\begin{align} z &amp;= \\frac{\\bar x-\\mu_{\\bar x}}{\\sigma_{\\bar x}} \\\\ &amp;= \\frac{90-94.52}{8.97/\\sqrt 1} \\\\ &amp;= -0.504 \\\\ \\\\ P(z &lt; -0.504) &amp;= 0.3072 \\end{align} \\] There is a \\(30.72\\%\\) probability that a randomly selected runner will complete the run in less than \\(90\\) minutes. Problem 2: Find the probability that the average of \\(20\\) runners is less than \\(90\\) minutes. Solution: Here, \\(n = 20 &lt; 30\\), but the distribution of the population, that is, the distribution of run times is stated to be approximately normal. Because of this, the sampling distribution will be normal for any sample size. \\[ \\begin{align} \\sigma_{\\bar x} &amp;= \\frac{\\sigma}{\\sqrt n} = \\frac{8.97}{\\sqrt {20}} = 2.01 \\\\ z &amp;= \\frac{\\bar x-\\mu_{\\bar x}}{\\sigma_{\\bar x}} = \\frac{90-94.52}{2.01}= - 2.25 \\\\ P(z&lt;-0.504) &amp;= 0.0123 \\end{align} \\] There is a \\(1.23\\%\\) probability that the average run time of 20 randomly selected runners will be less than 90 minutes. Problem 3: Find the probability that less than \\(15\\%\\) of the sample of \\(400\\) people will be smokers if the true proportion is \\(20\\%.\\) Solutions: The mean of the sample proportion is the population proportion: \\(\\mu_\\hat p = 0.20.\\) The standard deviation of \\(\\hat p\\) is described by the standard deviation for the proportion: \\[\\sigma_{\\hat p}=\\sqrt \\frac{p(1-p)}{n} = \\sqrt \\frac{0.2(0.8)}{400} = 0.02\\] \\[ \\begin{align} z &amp;= \\frac{\\hat p - \\mu_\\hat p}{\\sigma_\\hat p} = \\frac{0.15 - 0.20}{0.02} = -2.5 \\\\ \\\\ P(z&lt;-2.5) &amp;= 0.0062 \\end{align} \\] Problem 4: \\(13\\%\\) of the US population are left-handed. If an auditorium has \\(15\\) lefty seats, what is the probability that there will not be enough lefty seats for a class of \\(90\\) students (in other words, what is the probability that there will be more than \\(15\\) lefty students in the group)? Solutions: \\[ \\begin{align} \\mu_\\hat p &amp;= 0.13 \\\\ \\hat{p} &amp;= 15/90 = 0.167 \\\\ \\sigma_{\\hat p}&amp;=\\sqrt \\frac{p(1- p)}{n} = \\sqrt \\frac{0.13(0.87)}{90} = 0.035 \\\\ \\\\ z &amp;= \\frac{\\hat p - \\mu_\\hat p}{\\sigma_\\hat p} = \\frac{0.167 - 0.13}{0.035} = 1.06 \\\\ P(\\hat{p}&gt;0.167) &amp;= P(z&gt;1.06) = 0.1446 \\end{align} \\] "],["estimating-parameters-and-determining-sample-sizes.html", "Chapter 7 Estimating Parameters and Determining Sample Sizes Point Estimate 7.1 Confidence Intervals 7.2 Estimate a Population Proportion 7.3 Finding the Sample Size to Estimate a Population Proportion 7.4 Estimating a Population Mean 7.5 Estimating a Population Standard Deviation or Variance", " Chapter 7 Estimating Parameters and Determining Sample Sizes Learning Outcome Perform calculations to estimate parameters using confidence intervals based on the normal distribution and t-distribution. In this chapter, we will use sample data to estimate values of population parameters (such as a population proportions or population mean). We will learn how to construct a confidence interval estimate of a population proportion and interpret such confidence interval estimates. Finally, we will learn how to determine the sample size necessary to estimate a population proportion. Point Estimate A point estimate is a single value used to estimate a population parameter. For example, the sample proportion \\(\\hat p\\) is the best point estimate (also called unbiased estimator) of the population proportion \\(p\\). Unbiased Estimator: An unbiased estimator is a statistic that targets the value of the corresponding population parameter in the sense that the sampling distribution of the statistic has a mean that is equal to the corresponding population parameter. For example, the statistic \\(\\hat p\\) targets the population proportion \\(p\\), \\(E(\\mathbf {\\hat p} ) = p.\\) Also, \\(\\hat p\\) is the consistent estimator of \\(p\\) because as the sample size \\(n\\) increases to indefinitely, the resulting sequences of \\(\\hat p\\) converges in probability to \\(p\\). Consistent Estimator \\({T_1, T_2, T_3, ...}\\) is a sequence of estimators for parameter \\(\\theta_0\\), the true value of which is \\(4\\). This sequence is consistent: the estimators are getting more and more concentrated near the true value \\(\\theta_0\\). The limiting distribution of the sequence is a degenerate random variable which equals \\(\\theta_0\\) with probability \\(1\\). An unbiased estimator can be inconsistent - meaning its expected value is equal to the population parameter, but it does not consistently converge to any value as \\(n\\) increases. On the other hand, a consistent estimator can be biased - meaning it consistently converges to the correct population parameter, but its expected value is higher or lower than the targeted value. Source: Wikipedia 7.1 Confidence Intervals A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. A confidence interval or (interval estimate) is a range (or an interval) of values used to estimate the true value of a population parameter. A confidence interval is sometimes abbreviated as CI. The confidence level is the probability \\(1-\\alpha\\) (such as \\(0.95\\), or \\(95\\%\\)) that the confidence interval actually does contain the population parameter, assuming that the estimate process is repeated a large number of times. (The confidence level is also called the degree of confidence, or the confidence coefficient.) \\(\\alpha : \\text { significance level }\\) Constructing a \\(95\\%\\) confidence interval When the sampling distribution of a point estimate can reasonably be modeled as normal, the point estimate we observe will be within \\(1.96\\) standard errors of the true value of interest about \\(95\\%\\) of the time. Thus, a \\(95\\%\\) confidence interval for such a point estimate can be constructed: \\(\\text {point estimate} \\pm 1.96 \\times SE\\) 95% confidence interval Critical Value The values of the \\(z\\) scores at the borderlines of the confidence interval are called the critical values. Critical Value Example: Find the critical value \\(z_{\\alpha/2}\\) corresponding to a \\(95\\%\\) confidence level. \\[ \\begin{array} {|c|c|} \\hline \\text{Confidence Level} &amp; \\alpha &amp; \\text{Critical Value, } z_{\\alpha/2} \\\\ \\hline 90\\% &amp; 0.1 &amp; 1.645 \\\\ \\hline 95\\% &amp; 0.05 &amp; 1.960 \\\\ \\hline 99\\% &amp; 0.01 &amp; 2.575 \\\\ \\hline \\end{array}\\] Generalizing Confidence Interval If the point estimate follows the normal model with standard error \\(SE\\), then a confidence interval for the population parameter is \\[ \\text {point estimate} \\pm z{^{\\star}_{\\alpha/2}} \\times SE \\] where \\(z^\\star\\) , the critical value, depends on the confidence level (\\(CL\\)) selected, and \\(\\alpha = 1-CL\\) 7.1.1 Margin of Error (ME) The margin of error (ME) is the distance between the point estimate and the lower or upper bound of a confidence interval. \\[ \\begin{align} \\text{confidence interval} &amp;= \\text {point estimate} \\pm z{^{\\star}_{\\alpha/2}} \\times SE \\\\ &amp;= \\text {point estimate} \\pm \\text{margin of error} \\end{align} \\] 7.2 Estimate a Population Proportion This section presents methods for using a sample proportion \\((\\hat p)\\) to make an inference about the value of the corresponding population proportion \\((p)\\). Assumptions: Simple random sample The population is at least \\(20\\) times larger than the sample The population can be divided into two mutually exclusive groups and one group in the sample contains \\(10\\) or more subjects. Calculating Confidence Interval for a Population Proportion Example 1: The heart patients who receive stents are \\(9\\%\\) more likely to suffer stroke from usage of the stent than those who do not have it. The estimate’s standard error \\((SE)\\) is \\(0.028\\). Construct a \\(95\\%\\) confidence interval for the change in stroke rates from the usage of stent. \\[ \\begin{align} \\text {95% Confidence Interval} &amp;= \\text {point estimate} \\pm z^*_{\\alpha/2} \\times SE \\\\ &amp;= \\hat p \\pm z^*_{\\alpha/2} \\cdot \\sqrt{\\dfrac{\\hat p (1 - \\hat p)}{n}} \\\\ &amp;= 0.090 \\pm 1.96 \\times 0.028 \\\\ &amp;=(0.035, 0.145) \\end{align} \\] \\[ \\begin{align} \\text {90% Confidence Interval} &amp;= \\text {point estimate} \\pm 1.645 \\times SE \\\\ &amp;= 0.090 \\pm 1.645 \\times 0.028 \\\\ &amp;=(0.044, 0.136) \\end{align} \\] Example 2: In a survey of \\(800\\) parents, \\(632\\) said that music education has a positive effect on academic performance. Construct a \\(95\\%\\) confidence interval for the proportion of parents who believe that music education has a positive effect. \\[ \\begin{align} \\hat p = \\dfrac{632}{800} = 0.79\\\\ \\text {95% Confidence Interval} &amp;= \\text {point estimate} \\pm z^*_{\\alpha/2} \\times SE \\\\ &amp;= \\hat p \\pm z^*_{\\alpha/2} \\cdot \\sqrt{\\dfrac{\\hat p (1 - \\hat p)}{n}} \\\\ &amp;= 0.79 \\pm 1.96 \\times \\sqrt{\\dfrac{ .79 (1 - .79 )}{800}} \\\\ &amp;=(0.762, 0.818) \\end{align} \\] We are \\(95\\%\\) confident that the proportion of parents who believe that music education has a positive effect is between \\(0.762\\) and \\(0.818\\). Interpretation of Confidence Intervals Suppose, the \\(95\\%\\) confidence interval estimate of the population proportion \\(p\\) is \\(0.405 \\lt p \\lt 0.455\\). \\(\\textbf {Correct }\\) We are \\(95\\%\\) confident that the interval from \\(0.405\\) to \\(0.455\\) actually does contain the true value of the population proportion \\(p\\). \\(\\textbf {Wrong }\\) There is a \\(95\\%\\) chance that the true value of the population proportion \\(p\\) will fall between \\(0.405\\) to \\(0.455\\). Method for constructing confidence intervals with small samples Adjusted sample proportion \\(\\tilde p\\) The method presented for constructing a confidence interval for a proportion requires that we have at least \\(10\\) individuals in each category. When this condition is not met, we can still construct a confidence interval by adjusting the sample proportion a bit. \\(\\tilde p = \\dfrac{x + 2}{n + 4}\\) \\[ \\text {Confidence Interval} = \\tilde p \\pm z^*_{\\alpha/2} \\cdot \\sqrt{\\dfrac{\\tilde p (1 - \\tilde p)}{n+4}} \\\\ \\] Simulating Confidence Intervals \\(50\\) samples of size \\(n = 300\\) were drawn from a population with proportion parameter \\(p = 0.30\\). For each sample, a confidence interval was created to capture the true proportion \\(p\\). How many did not capture \\(p = 0.30?\\) 7.3 Finding the Sample Size to Estimate a Population Proportion We assume that the sampling distribution of sample proportion follows a normal distribution, which allows us to use critical \\(z\\) score to determine the sample size from a given margin of error and confidence level. Example: A pilot study showed that \\(0.5\\%\\) of credit card offers in the mail end up with the person signing up. To be within \\(0.1\\%\\) of the true rate with \\(95\\%\\) confidence, how big does the test mailing have to be? \\[ \\begin{align} ME &amp;= z{^{\\star}_{\\alpha/2}} \\times SE \\\\ ME &amp;= z{^{\\star}_{\\alpha/2}} \\times \\sqrt \\frac{\\hat p \\hat q}{n} \\\\ 0.001 &amp;= 1.96 \\times \\sqrt \\frac{(0.005)(0.995)}{n} \\\\ (0.001)^2 &amp;= (1.96)^2 \\times \\frac{(0.005)(0.995)}{n} \\\\ n &amp;= (1.96)^2 \\times \\frac{(0.005)(0.995)}{(0.001)^2} \\\\ n &amp;= 19112 \\end{align} \\] 7.4 Estimating a Population Mean The main goal of this section is to make an inference about the population mean \\((\\mu)\\) from the mean \\((\\bar x)\\) of a sample drawn from the same population. Therefore, \\(\\bar x\\) is the point estimate of the population \\(\\mu\\). 7.4.1 Estimating a Population Mean When \\(\\sigma\\) Is Known It is extremely rare that we want to estimate an unknown value of a population mean \\(\\mu\\) but we somehow know the value of the population standard deviation \\(\\sigma\\). If we somehow do know the value of \\(\\sigma\\), the confidence interval is constructed using the standard normal distribution. Confidence interval estimate of the true population mean \\(\\mu\\): \\[ ME = z_{\\alpha/2}.\\frac{\\sigma}{\\sqrt{n}} \\text { ; use with known } \\sigma \\] \\[ n = 15, \\bar x = 30.9, \\sigma = 2.9, CL = 95\\% \\\\ ME = (1.96) \\cdot \\left( \\frac{2.9}{\\sqrt{15}} \\right) = 1.46760 \\\\ \\bar x - ME &lt; \\mu &lt; \\bar x + ME \\\\ 30.9 - 1.46760 &lt; \\mu &lt; 30.9 + 1.46760 \\\\ 29.4 &lt; \\mu &lt; 32.4 \\] 7.4.2 Estimating a Population Mean When \\(\\sigma\\) Is Not Known \\(\\text {The sample mean } \\bar x \\text { is the best estimate of the population mean } \\mu.\\) Normality: The method for finding a confidence interval estimate of \\(\\mu\\) is robust against a departure from normality, which means that the normality requirement is relaxed. The distribution need not be perfectly normal, but it has to be symmetric with one mode. In this case, population standard deviation \\(\\sigma\\) is estimated from the sample. The sampling distribution employed to calculate the confidence interval of \\(\\mu\\) from sample mean \\(\\bar x\\) is called \\(\\text{Student } t \\text{ Distribution}.\\) \\(\\text {Student } t- \\text { Distribution}\\) According to the Central Limit Theorem, the sampling distribution of a statistic (e.g. sample mean) will follow a normal distribution, as long as the sample size is sufficiently large \\((n&gt;30)\\). But sample sizes are sometimes small, and often we do not know the standard deviation of the population. When either of these occurs, statisticians rely on the distribution of the \\(\\text {t-statistic (t-score)}\\) with the degrees of freedom \\((n-1)\\), \\(t = \\dfrac {\\bar x - \\mu}{s/\\sqrt n}\\) \\(\\text{Degrees of Freedom} = n - 1\\) \\(\\text {t-distribution }\\) is determined by its degrees of freedom. The degrees of freedom refers to the number of independent observations in a set of data. Properties of the \\(t \\text {-Distribution:}\\) The \\(\\text {Student } t\\) distribution has the same general symmetric shape as the standard normal distribution, but with more variability, which is typical of sampling distributions used for small sample size. As the sample size \\(n\\) gets larger, the \\(t\\) distribution gets closer to the standard normal distribution. The mean of the distribution is equal to \\(0\\). Unlike the variance of the standard normal distribution \\((\\sigma^2 = 1)\\), The variance of \\(t\\) distribution depends on the sample size \\((n)\\) and is equal to \\(\\frac {\\nu}{\\nu-2}\\), where \\(\\nu\\) is the DF and \\(v \\ge 2\\). 7.4.3 \\(t \\text { Confidence Interval}\\) Find a \\(95\\%\\) confidence interval for mirex concentrations in salmon. \\[ \\begin{align} n &amp;= 150 \\\\ \\bar x &amp;= 0.0913 \\space ppm \\\\ s &amp;= 0.0495 \\space ppm \\\\ df &amp;= 150 - 1 = 149 \\\\ \\\\ SE(\\bar x) &amp;= \\frac{0.0495}{\\sqrt{150}} = 0.0040 \\\\ t^*_{149} &amp;= 1.976 \\\\ \\end{align} \\] \\[ \\begin{align} \\text {Confidence Interval of } \\bar x: \\\\ &amp;\\bar x \\pm t^*_{149} \\times SE(\\bar x) \\\\ &amp;= 0.0913 \\pm 1.976 \\times 0.0040 \\\\ &amp;= 0.0913 \\pm 0.0079 \\\\ &amp;= (0.0834, 0.0992) \\end{align} \\] 7.4.4 Finding the Sample Size to Estimate a Population Mean Should you buy a movie download accelerator? To test the download times, find the minimum number of downloads that you need to do during a free trial period to obtain a \\(95\\%\\) CL with a ME &lt; \\(8\\) minutes. Given, \\(\\sigma = 10 \\space min\\) First, calculate \\(n\\) using \\(z\\) score \\[ \\begin{align} ME &amp;&lt; 8 \\\\ \\\\ \\text {with } z^* &amp;= 2 \\text { at 95% CL} \\\\ \\\\ 2 \\times \\frac {10}{\\sqrt n} &amp;&lt; 8 \\\\ n &amp;&gt; 6.25 \\\\ n &amp;\\approx 7 \\\\ \\\\ \\end{align} \\] Sample size turns out to be too small for normal approximation. Therefore, re-estimate \\(n\\) using \\(t\\) score. Second, find the critical value of \\(t_{\\alpha/2}\\) corresponding to a \\(95\\%\\) confidence level, given that \\(n = 6\\). \\[ \\begin{align} df &amp;= 7 - 1 = 6 \\\\ \\\\ \\text {with } t^*_6 &amp;= 2.447 \\text { at 95% CL} \\\\ \\\\ 2.447 \\times \\frac {10}{\\sqrt n} &amp;&lt; 8 \\\\ n &amp;&gt; 9.36 \\\\ n &amp;\\approx 10 \\\\ \\\\ \\end{align} \\] Hence, at least 10 trial downloads need to be run to make sure ME remains less than \\(8\\) min. Choosing between Student \\(t\\) and \\(z\\) Distributions \\[ \\begin{array}{l|c} \\text{conditions} &amp; \\text{method} \\\\ \\hline \\sigma \\text{ not known; normally distributed population or n &gt; 30} &amp; t \\\\ \\hline \\sigma \\text{ known; normally distributed population or n &gt;30} &amp; z \\end{array} \\] 7.5 Estimating a Population Standard Deviation or Variance This section presents methods for using a sample standard deviation \\(s\\) (or a sample variance \\(s^2\\)) to estimate the value of the corresponding population standard deviation \\(\\sigma\\) (or population variance \\(\\sigma^2\\)). Point Estimate: The sample variance \\(s^2\\) is the best point estimator of the population variance \\(\\sigma^2\\). The sample standard deviation \\(s\\) is commonly used as a point estimate of \\(\\sigma\\), even though it is a biased estimator. Confidence Interval: When constructing a confidence interval estimate of a population standard deviation (or population variance), we construct the confidence interval using the \\(\\chi^2 \\text{ distribution}\\). Chi-Squared \\(\\chi^2\\) Distribution In a normally distributed population with variance \\(\\sigma^2\\), if we randomly select independent samples of size \\(n\\) and, for each sample, compute the sample variance \\(s^2\\), the sample statistic \\(\\chi^2 = (n -1)s^2/{\\sigma^2}\\) has sampling distribution called the chi-squared distribution, \\[ \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2} \\] \\(\\text{Degrees of freedom: df } = n -1\\) \\(\\chi^2 \\text{ distribution }\\) is skewed to the right, unlike normal and student \\(t\\) distributions. \\(\\chi^2 \\ge 0\\) The chi-squared distribution is different for each number of degrees of freedom. As the degrees of freedom increases, chi-squared distribution approaches a normal distribution. Critical Chi-Squared \\(\\chi^2\\) Values The critical values for a level \\(100(1-\\alpha)\\%\\) confidence interval are the values that contain the middle \\(100(1-\\alpha)\\%\\) of the area under the curve between them. The notation for the critical values tells how much area is to the right of the critical value. For a level \\(1-\\alpha\\) confidence interval, the critical values are denoted \\(\\chi^2_{1-\\alpha/2}\\) and \\(\\chi^2_{\\alpha/2}\\) 7.5.1 Confidence Interval for Estimating a Population Standard Deviation or Variance Let \\(s^2\\) be the sample variance from a simple random sample of size \\(n\\) from a normal population. A level \\(100(1-\\alpha)\\%\\) confidence interval for the population variance \\(\\sigma^2\\) is Confidence Interval for the Population Variance \\(\\sigma^2\\) \\[ \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{1- \\alpha/2} \\] Confidence Interval for the Population Variance \\(\\sigma\\) \\[ \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}}} &lt; \\sigma &lt; \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}} \\] The critical values are taken from a chi-square distribution with \\(n ??? 1\\) degrees of freedom. Example 1: Confidence Interval for Estimating a Population Standard Deviation or Variance \\[ \\begin{align} &amp;s = 14.29263 \\\\ &amp;n = 22 \\\\ &amp;CL = 95\\% \\\\ \\\\ &amp;\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}} \\\\ \\\\ or, \\ &amp;\\frac{21.(14.29263)^2}{35.479} &lt; \\sigma^2 &lt; \\frac{21.(14.29263)^2}{10.283} \\\\ \\\\ or, \\ &amp;120.9 &lt; \\sigma^2 &lt; 417.2 \\\\ or, \\ &amp;11.0 &lt; \\sigma &lt; 20.4 \\end{align} \\] Example 2: The compressive strengths of seven concrete blocks, in pounds per square inch, are measured, with the following results. \\(1989.9 \\ 1993.8 \\ 2074.5 \\ 2070.5 \\ 2070.9 \\ 2033.6 \\ 1939.6\\) Assume these values are a simple random sample from a normal population. Construct a \\(95\\%\\) confidence interval for the population standard deviation. Solution: \\[ s^2 = \\dfrac{\\sum(x - \\bar x)^2}{7-1} = 2699.8648 \\] Next, find \\(\\chi^2_{0.975}\\) and \\(\\chi^2_{0.0025}\\) for \\(\\text {df} = 6\\). From the table, \\(\\chi^2_{0.975} = 1.237\\) and \\(\\chi^2_{0.0025} = 14.449\\). \\[ \\begin{align} \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} &amp; &lt; \\sigma^2 &lt; \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}} \\\\ \\\\ \\frac{(7-1)(2699.8648)^2}{14.449} &amp; &lt; \\sigma^2 &lt; \\frac{(7-1)(2699.8648)^2}{1.237} \\\\ \\\\ or, \\ 1121.129 &amp; &lt; \\sigma^2 &lt; 13095.545 \\\\ or, \\ 33.48 &amp; &lt; \\sigma &lt; 114.44 \\end{align} \\] We are \\(95\\%\\) confident that the population standard deviation of the strengths of the concrete blocks is between \\(33.48\\) and \\(114.44\\). "],["hypothesis-testing.html", "Chapter 8 Hypothesis Testing 8.1 Hypothesis Testing 8.2 Inference for a Single Proportion 8.3 \\(t \\text {-test}\\) | Testing Hypothesis About \\(\\mu\\) with \\(\\sigma\\) Not Known 8.4 \\(\\chi^2 \\text{-test}\\) | Testing Hypothesis About a Variance", " Chapter 8 Hypothesis Testing Learning Outcome Perform hypotheses testing involving two sample means (independent and dependent), two population proportions, and two standard deviations/variances. This chapter introduces the statistical method of hypothesis testing to test a given claim about a population parameter, such as proportion, mean, standard deviation, or variance. This method combines the concepts covered in the previous chapters, including sampling distribution, standard error, critical scores, and probability theory. 8.1 Hypothesis Testing In statistics, a hypothesis is a claim or statement about a property of a population. A hypothesis test (or test of significance) is a procedure for testing a claim about a property of a population. The null hypothesis (\\(H_0\\)) is a statement that the value of a population parameter (such as proportion, mean, or standard deviation) is equal to some claimed value. The alternative hypothesis (\\(H_A\\)) is a statement that the parameter has a value that somehow differs from the null hypothesis. Example: Probability of getting head from a single toss of coin, \\(p = 0.5\\). Therefore, expected value of the number of heads from \\(20\\) tosses = \\(10\\). Suppose, you have tossed a coin \\(20\\) times and seen \\(15\\) heads, \\(\\hat p = 0.75\\). Is the coin fair, or is it biased towards heads? Null and Alternative Hypotheses Null hypothesis \\((H_0)\\): states that any deviation from what was expected is due to chance error (i.e. the coin is fair). Alternative hypothesis \\((H_A)\\): asserts that the observed deviation is too large to be explained by chance alone (i.e. the coin is biased towards heads). \\[ H_0: p = 0.5 \\\\ H_A: p &gt; 0.5 \\] Now, what is the probability of \\(p \\ge 0.75?\\) From normal approximation of the sampling distribution of \\(\\hat p\\), \\[ \\begin{align} p &amp;= 0.5 \\\\ se &amp;= \\sqrt{(0.5)(0.5)/20} = 0.112 \\\\ z &amp;= (0.75 - 0.50)/0.112 = 2.236 \\\\ \\\\ P(z\\ge2.236) &amp;= 0.0127 \\Leftarrow \\text {this probability is also called p-value} \\end{align} \\] Hence, there is only \\(1.27\\%\\) probability that observing \\(15\\) heads from \\(20\\) tosses is merely a chance. Then the question is, if the coin is indeed fair, is the p-value too small? Interpretation of \\(\\text{p-value}\\) A \\(\\text{p-value}\\) is the probability of obtaining the observed effect (or larger) under a “null hypothesis”. Thus, a \\(\\text{p-value}\\) that is very small indicates that the observed effect is very unlikely to have arisen purely by chance, and therefore provides evidence against the null hypothesis. It has been common practice to interpret a \\(\\text{p-value}\\) by examining whether it is smaller than particular threshold values or “significance level”. In particular, \\(\\text{p-values}\\) less than \\(5\\%\\) are often reported as “statistically significant”, and interpreted as being small enough to justify rejection of the null hypothesis. By definition, the significance level \\(\\alpha\\) is the probability of mistakenly rejecting the null hypothesis when it is true. \\[\\textbf {Significance level } \\alpha = P \\textbf { (rejecting } H_0 \\textbf { when } H_0 \\textbf { is true)} \\] In common practice, \\(\\alpha\\) is set at \\(10\\%, 5\\%\\) or \\(1\\%\\). In the coin toss example: p-value = \\(1.27\\%\\) which is less than the \\(5\\%\\) significance level. Therefore, the result is statistically significant. Conclusion: The coin is biased towards heads. Critical Value Method In a hypothesis test, the critical value(s) separates the critical region (where we reject the null hypothesis) from the values of the test statistic that do not lead to rejection of the null hypothesis. With the critical value method of testing hypothesis, we make a decision by comparing the test statistic to the critical value(s). One-sided and two-sided tests If the researchers are only interested in showing an increase or a decrease, but not both, use a one-sided test. If the researchers would be interested in any difference from the null value - an increase or decrease - then the test should be two-sided. After observing data, it is tempting to turn a two-sided test into a one-sided test. Hypotheses must be set up before observing the data. If they are not, the test must be two-sided. 8.1.1 Type I and Type II Errors When testing a null hypothesis, sometimes the test comes to a wrong conclusion by rejecting it or failing to reject it. There are two kinds of errors: type I and type II errors. \\(\\textbf {Type I error} :\\) The error of rejecting the null hypothesis when it is actually true. \\(\\alpha = P (\\textbf{type I error}) = P (\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true } )\\) The probability of \\(\\text {Type I}\\) can be minimized by choosing a smaller \\(\\alpha\\). \\(\\textbf {Type II error} :\\) The error of failing to reject the null hypothesis when it is actually false. \\(\\beta = P (\\textbf{type II error}) = P (\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false} )\\) The probability of \\(\\text {Type II}\\) can be minimized by choosing a larger sample size \\(n\\). Power of a Hypothesis Test The power of a hypothesis test is the probability \\(1-\\beta\\) of rejecting a false null hypothesis. The value of the power is computed by using a particular significance level \\(\\alpha\\) and a particular value of the population parameter that is an alternative to the value of assumed true in the null hypothesis. In practice, statistical studies are commonly designed with a statistical power of at least \\(80%\\). Figure: Statistical Power Post-hoc Power Calculation for One Study Group vs. Population Suppose, \\[ H_0 : p = P_0 \\\\ H_A : p \\ne P_0 \\\\ \\] \\[ \\begin{align} P_0 &amp;= \\text{proportion of population} \\\\ P_1 &amp;= \\text{proportion observed from the data (an alternative population)} \\\\ N &amp;= \\text{sample size} \\\\ \\alpha &amp;= \\text{probability of type I error} \\\\ \\beta &amp;= \\text{probability of type II error} \\\\ z &amp;= \\text{critical z score for a given } \\alpha \\text { or } \\beta \\end{align} \\] Suppose, \\(P_1\\) is an alternative to the value assumed in \\(H_0\\). Under \\(H_0\\), \\[ P&#39;_0 = P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\] Under \\(H_A\\), \\[ \\therefore z_{\\beta} = \\dfrac{P&#39;_0 - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} = \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\\\ \\\\ P(\\textbf{Type II error}) = \\beta = \\Phi \\left \\{ \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\right \\} \\\\ \\] \\[ \\begin{align} \\text{where,} \\\\ Q_0 &amp;= 1 - P_0 \\\\ Q_1 &amp;= 1 - P_1 \\\\ \\Phi &amp;= \\text{cumulative normal distribution function} \\end{align} \\] Example: Calculate statistical power for various alternative hypotheses. \\[ Suppose, \\begin{cases} H_0: p = 0.5 \\\\ H_A: p \\ne 0.5 \\\\ P(\\text{type I error}) = \\alpha = 0.05 \\\\ \\text{Critical z score, } z_{1 - \\alpha/2} = 1.96 \\\\ N = 14 \\\\ \\end{cases} \\] \\[ \\begin{array}{r|r|r} P_1 &amp; \\Phi(z_{\\beta}) = \\beta &amp; 1-\\beta \\\\ \\hline 0.6 &amp; \\Phi(1.2367) = 0.8919 &amp; 0.1081 \\\\ 0.7 &amp; \\Phi(0.5055) = 0.6934 &amp; 0.3066 \\\\ 0.8 &amp; \\Phi(-0.3562) = 0.3608 &amp; 0.6392 \\\\ 0.9 &amp; \\Phi(-1.7222) = 0.0425 &amp; 0.9575 \\\\ \\hline \\end{array} \\] Example: Sample size calculation to achieve power (when \\(P_0\\) and \\(P_1\\) are known) \\[ \\begin{align} z_{\\beta} = \\Phi^{-1}(\\beta) &amp;= \\dfrac{\\Bigg( P_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\dfrac{P_0Q_0}{N}} \\Bigg) - P_1}{\\sqrt{\\dfrac{P_1Q_1}{N}}} \\\\ \\\\ z_{1-\\alpha/2} &amp;= 1.96 \\\\ 1- \\beta &amp;= 0.8 \\\\ \\\\ \\Phi^{-1}(0.2) = -0.84 &amp;= \\dfrac{\\Bigg( 0.5 + 1.96 \\cdot \\sqrt{\\dfrac{(0.5) (0.5)}{N}} \\Bigg) - 0.9}{\\sqrt{\\dfrac{(0.9)(0.1)}{N}}} \\\\ \\implies N &amp;= \\Bigg( \\dfrac{1.96\\sqrt{0.25} + 0.84\\sqrt{0.09}}{0.4} \\Bigg)^2 \\\\ &amp;\\approx 10 \\end{align} \\] Example: Sample size calculation to achieve power (when \\(P_0\\) and \\(P_1\\) are unknown) \\[ N = \\Bigg( \\dfrac{z_{1-\\alpha/2} + z_{1-\\beta}}{ES} \\Bigg)^2 \\\\ \\] where, \\[ \\text{effect size, } ES = \\dfrac{|P_1-P_0|}{\\sqrt{P_0Q_0}} \\] Statistical power and design of experiment: When designing an experiment, it is essential to determine the minimize sample size that would be needed to detect an acceptable difference between the true value of the population parameter and what is observed from the data. A \\(5\\%\\) significance level \\((\\alpha)\\) and a statistical power of at least \\(80\\%\\) are common requirements for determining that a hypothesis test is effective. Formal Test of Hypothesis Follow these seven steps when carrying out a hypothesis test. State the name of the test being used. Verify conditions to ensure the standard error estimate is reasonable and the point estimate follows the appropriate distribution and is unbiased. Write the hypotheses and set them up in mathematical notation. Identify the significance level \\(\\alpha\\). Calculate the test statistics (e.g. \\(z\\)), using an appropriate point estimate of the paramater of interest and its standard error. \\[\\text{test statistics} = \\frac{\\text{point estimate - null value}}{\\text{SE of estimate}}\\] Find the \\(\\text{p-value}\\), compare it to \\(\\alpha\\), and state whether to reject or not reject the null hypothesis. Write your conclusion in context. 8.2 Inference for a Single Proportion Conduct a formal hypothesis test of a claim about a population proportion \\(p\\). Requirements: The sample observations are simple random sample. The trials are independent with two possible outcomes. The sampling distribution for \\(\\hat p\\), taken from a sample of size \\(n\\) from a population with a true proportion \\(p\\), is nearly normal when the sample observations are independent and we expect to see at least \\(10\\) successes and \\(10\\) failures in our sample, i.e. \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\). This is called the success-failure condition. If the conditions are met, then the sampling distribution of \\(\\hat p\\) is nearly normal with mean \\(\\mu_{\\hat p} = p\\) and standard deviation \\(\\sigma_{\\hat p} = \\sqrt {\\dfrac{p(1-p)}{n}}\\). \\[ \\begin{cases} n = \\text {samlpe size } \\\\ \\hat p = \\dfrac{x}{n} \\text { (sample proportion) } \\\\ p = \\text {population proportion } \\\\ q = 1 - p \\\\ \\end{cases} \\] Test Statistic for Testing a Claim About a Proportion \\[ z = \\dfrac{\\hat p - p}{\\sqrt {\\dfrac{pq}{n}} } \\] Example: The DMV claims that \\(80\\%\\) of all drivers pass the driving test. In a survey of \\(90\\) teens, only \\(61\\) passed. Is there evidence that teen pass rates are significantly below \\(80\\%?\\) Let’s say, \\(p\\) is the true population proportion. \\[ \\begin{align} \\text {One-tailed test} &amp;:\\\\ H_0&amp;: p = 0.80 \\\\ H_A&amp;: p &lt; 0.80 \\end{align} \\] Verify success-failure condition: \\[ \\begin{align} np \\ge 10 \\rightarrow 90 \\times 0.80 \\ge 10 \\\\ n(1-p) \\ge 10 \\rightarrow 90 \\times (1-0.80) \\ge 10 \\end{align} \\] Therefore, the conditions for a normal model are met. Now, \\[ \\begin{align} \\hat p &amp;= \\frac {61}{90} = 0.678 \\\\ \\\\ SE(\\hat p) &amp;= \\sqrt \\frac{pq}{n} = \\sqrt \\frac{(0.80)(0.20)}{90} = 0.042 \\\\ \\\\ z &amp;= \\frac {0.678-0.80}{0.042} = -2.90 \\\\ \\\\ p\\text{-value} &amp;= P(z &lt; -2.90) = 0.002 &lt; 0.05 \\end{align} \\] Hence, we reject \\(H_0\\). Teen pass rate is significantly below population pass rate. Example: Under natural conditions, \\(51.7\\%\\) of births are male. In Punjab India’s hospital \\(56.9\\%\\) of the \\(550\\) births were male. Is there evidence that the proportion of male births is significantly different for this hospital? \\[ \\begin{align} \\text {Two-tailed test} &amp;:\\\\ H_0&amp;: p = 0.517 \\\\ H_A&amp;: p \\ne 0.517 \\end{align} \\] Verify success-failure condition: \\[ \\begin{align} np \\ge 10 \\rightarrow 550 \\times 0.517 \\ge 10 \\\\ n(1-p) \\ge 10 \\rightarrow 550 \\times (1-0.517) \\ge 10 \\end{align} \\] \\[ \\begin{align} \\hat p &amp;= 0.569 \\\\ \\\\ SE(\\hat p) &amp;= \\sqrt \\frac{pq}{n} = \\sqrt \\frac{(0.517)(1-0.517)}{550} = 0.0213 \\\\ \\\\ z &amp;= \\frac {0.569-0.517}{0.0213} = 2.44 \\\\ \\\\ p\\text{-value} &amp;= 2 \\times P(z &gt; 2.44) = 2 \\times 0.0073 = 0.0146 &lt; 0.05 \\end{align} \\] Hence, we reject \\(H_0\\). Male birth rate is significantly higher at the hospital than the natural birth rate. 8.3 \\(t \\text {-test}\\) | Testing Hypothesis About \\(\\mu\\) with \\(\\sigma\\) Not Known The null hypothesis claims about a population mean \\(\\mu\\). Notation: \\[ \\begin{cases} \\mu_{\\bar x} = \\text {population mean } \\\\ s = \\text {sample standard deviation } \\\\ n = \\text {size of the sample drawn from the population } \\\\ \\bar x = \\text{sample mean} \\\\ \\end{cases} \\] Requirements: The sample is simple random sample. The population is normally distributed or \\(n &gt; 30\\). Test Statistic for Testing a Claim About a Mean \\[ t_{n-1} = \\dfrac{\\bar x - \\mu_{\\bar x}}{\\dfrac{s}{\\sqrt n}} \\] Example : Average weight of a mice population of a particular breed and age is \\(30 \\text{ gm}\\). Weights recorded from a random sample of \\(5\\) mice from that population are \\({31.8, 30.9, 34.2, 32.1, 28.8}.\\) Test whether the sample mean is significantly greater than the population mean. \\[ \\begin{align} H_0&amp;: \\mu = 30 \\\\ H_A&amp;: \\mu &gt; 30 \\\\ \\\\ \\bar x &amp;= 31.56 \\\\ s &amp;= 1.9604 \\\\ SE(\\bar x) &amp;= 1.9604/\\sqrt 5 = 0.8767 \\\\ \\\\ t &amp;= (31.56 - 30)/0.8767 = 1.779 \\\\ df &amp;= (5 -1) = 4 \\\\ p-value &amp;= 7.5\\% &gt; 5\\% \\end{align} \\] Conclusion: \\(H_0\\) cannot be rejected. The sample mean is not significantly greater than the population mean. Example : EPA recommended mirex screening is 0.08 ppm. A study of a sample of 150 salmon found an average mirex concentration of 0.0913 ppm with a std. deviation of 0.0495 ppm. Are farmed salmon contaminated beyond the permitted EPA level? Also, find a \\(95\\%\\) confidence interval for the mirex concentration in salmon. \\[ \\begin{align} H_0&amp;: \\mu = 0.08 \\\\ H_A&amp;: \\mu &gt; 0.08 \\\\ \\\\ \\bar x &amp;= 0.0913 \\\\ s &amp;= 0.0495 \\\\ SE(\\bar x) &amp;= 0.0495/\\sqrt {150} = 0.0040 \\\\ \\\\ t_{149} &amp;= \\dfrac{\\bar x - \\mu}{SE(\\bar x)} = \\dfrac{(0.0913 - 0.08)}{0.0040} = 2.795 \\\\ df &amp;= (150 -1) = 149 \\\\ p-value &amp;= P(t_{149}&gt;2.795)= 0.29\\% &lt; 5\\% \\end{align} \\] Conclusion: Reject \\(H_0\\). The sample mean mirex level significantly higher that the EPA screening level. 8.4 \\(\\chi^2 \\text{-test}\\) | Testing Hypothesis About a Variance Listed below are the heights (cm) for the simple random sample of female supermodels. Use a \\(0.01\\) significance level to test the claim that supermodels have heights with a standard deviation that is less than \\(\\sigma=7.5 \\text { cm}\\) for the population of women. Does it appear that heights of supermodels vary less than heights of women from the population? \\[ \\text{178, 177, 176, 174, 175, 178, 175, 178} \\\\ \\text{178, 177, 180, 176, 180, 178, 180, 176} \\\\ s^2 = 3.4 \\] \\[ \\begin{align} H_0: \\sigma^2 = 56.25 \\\\ H_A: \\sigma^2 &lt; 56.25 \\\\\\\\ \\chi^2 = (n-1)\\frac{s^2}{\\sigma^2} &amp;= (15)\\frac{(3.4)}{(56.25)} \\\\ &amp;= 0.907 \\\\ \\\\ \\end{align} \\] From \\(\\chi^2\\) table, \\[ \\text {The critical value of } \\chi^2 = 5.229 \\text { at } \\alpha = 0.01. \\\\ \\] Hence, we reject \\(H_0\\). Confidence Interval Calculation: \\[ \\sqrt{ \\dfrac{(n-1)s^2}{\\chi_R^2} } &lt; \\sigma &lt; \\sqrt{ \\dfrac{(n-1)s^2}{\\chi_L^2} } \\\\ \\sqrt{ \\dfrac{(16-1)3.4}{30.578} } &lt; \\sigma &lt; \\sqrt{ \\dfrac{(16-1)3.4}{5.229} } \\\\ 1.3 \\text{ cm } &lt; \\sigma &lt; 3.1 \\text { cm } \\] "],["inferences-from-two-samples.html", "Chapter 9 Inferences from Two Samples 9.1 Inferences about Two Proportions 9.2 Inferences about Two Independent Sample Means 9.3 Two Dependent Samples (Matched Pairs)", " Chapter 9 Inferences from Two Samples Learning Outcome Perform hypotheses testing two population proportions and two sample means (independent and dependent). The previous chapters covered the methods of estimating values of population parameters using confidence intervals and testing hypotheses about population parameters with a sample from one population. This chapter extends these methods to situations involving two populations. 9.1 Inferences about Two Proportions Objectives: Hypothesis test: conduct a hypothesis test of a claim about two population proportions. Confidence interval: construct a confidence interval estimate of the difference between two population proportions. Notations: \\[ \\begin{cases} p_1 = \\text {proportion in population 1 } \\\\ n_1 = \\text {size of the sample drawn from population 1 } \\\\ x_1 = \\text {number of successes oberved in sample 1} \\\\ \\hat p_1 = \\dfrac{x_1}{n_1} \\ (\\text{sample 1 proportion}) \\\\ \\hat q_1 = 1- \\hat p_1 \\ (\\text{complement of sample 1 proportion}) \\\\ \\end{cases} \\] The corresponding notations \\(p_2, n_2, x_2, \\hat p_2, \\hat q_2\\) apply to population 2. Requirements Random - The sample proportions are from two simple random samples. Independent - The two samples are independent. If drawn from the same population, the samples are not related or they are not naturally paired or matched with the sample values from the other population. Sample size - For each of the two samples, there are at least \\(5\\) successes and at least \\(5\\) failures. (That is \\(n \\hat p \\ge 5\\) and \\(n \\hat q \\ge 5\\) for each of the two samples.) 9.1.1 Hypothesis Test The null hypothesis claims that the two population proportions are equal. It implies that both samples came from the same population and have equal variance. Under the assumption of equal proportions, the best estimate of the common population proportion is estimated by pooling both samples into one large sample, so that \\(\\bar p\\) is the estimator of the common population proportion. Furthermore, pooled sample variance is calculated from \\(\\bar p\\). Let’s consider, the difference in two population proportions: \\(p_1 - p_2\\). A reasonable point estimate of \\(p_1 - p_2\\) based on the samples drawn from the same populations can be written in the form: \\(\\hat p_1 - \\hat p_2\\). \\[ H_0: p_1 - p_2 = 0 \\\\ H_A: p_1 - p_2 \\ne 0 \\\\ \\] Pooled Sample Proportion \\[ \\bar p = \\dfrac{x_1 + x_2}{n_1 + n_2} \\\\ \\bar q = 1 - \\bar p \\] Hypothesis Test Statistic for Two Proportions The mean of the sampling distribution of \\((\\hat p_1 - \\hat p_2)\\) is \\(p_1 - p_2\\), and standard error \\(\\sqrt{\\dfrac{\\bar p \\bar q}{n_1}+\\dfrac{\\bar p \\bar q }{n_2}}\\). The \\(z\\)-score at \\((\\hat p_1 - \\hat p_2)\\), \\[ z = \\dfrac{(\\hat p_1 - \\hat p_2) - (p_1 - p_2)}{\\sqrt{\\dfrac{\\bar p \\bar q}{n_1}+\\dfrac{\\bar p \\bar q }{n_2}}} \\\\ \\text{where, } p_1 - p_2 = 0 \\] 9.1.2 Confidence Interval Estimate of \\(p_1 - p_2\\) The confidence interval estimate of the difference \\(p_1 - p_2\\) is \\((\\hat p_1 - \\hat p_2) - E \\lt (p_1 - p_2) \\lt (\\hat p_1 - \\hat p_2) + E\\) where, the standard error: \\(SE_{\\hat p_1 - \\hat p_2} = \\sqrt{SE^2_{\\hat p_1} + SE^2_{\\hat p_2}} = \\sqrt{\\dfrac{\\hat p_1 \\hat q_1 }{n_1}+\\dfrac{\\hat p_2 \\hat q_2}{n_2}}\\) margin of error \\(E = z^*_{\\alpha/2} \\sqrt{\\dfrac{\\hat p \\hat q}{n_1}+\\dfrac{\\hat p \\hat q }{n_2}}\\) \\(z^*_{\\alpha/2}\\) is the critical value that corresponds to the confidence level \\((1-\\alpha)\\). Notice that the standard error calculation in the confidence interval is based on \\(\\hat p_1\\) and \\(\\hat p_2\\), whereas the hypothesis test uses a standard error based on pooled proportion \\(\\bar p\\). Example (Two-proportional \\(z\\)-interval): How much difference is there in the proportion of male drivers who wear seat belts when sitting next to a man and the proportion when sitting next to a woman? With female passengers: \\(2777\\) wore seat belts, \\(1431\\) did not. With male passengers: \\(1363\\) wore seat belts, \\(1400\\) did not. Solution: \\[ \\begin{align} n_F &amp;= 4208, \\\\ n_M &amp;= 2763, \\\\ \\hat p_F &amp;= \\frac{2777}{4208} = 0.660, \\\\ \\hat p_M &amp;= \\frac{1363}{2763} = 0.493 \\\\ \\\\ SE_{\\hat {p_F} - \\hat p_M} &amp;= \\sqrt{\\frac{\\hat p_F(1- \\hat p_F)}{n_F}+\\frac{\\hat p_M(1-\\hat p_M)}{n_M}} \\\\ &amp;= \\sqrt{\\frac{(0.660)(1-0.660)}{4208}+\\frac{0.493(1-0.493)}{2763}} \\\\ &amp;= 0.012 \\\\ \\\\ ME &amp;= z^* \\times SE(\\hat p_F - \\hat p_M) = 1.96 \\times 0.012 = 0.024 \\\\ \\\\ \\hat p_F - \\hat p_M &amp;= 0.660 - 0.493 = 0.167 \\\\ \\\\ \\text{Confidence Interval:} \\\\ (\\hat p_F - \\hat p_M) - ME &amp;\\lt ( p_F - p_M) \\lt (\\hat p_F - \\hat p_M) + ME \\\\ \\\\ 0.167 - 0.024 &amp;\\lt ( p_F - p_M) \\lt 0.167 + 0.024 \\\\ \\\\ 0.143 &amp;\\lt ( p_F - p_M) \\lt 0.191 \\end{align} \\] Example (Two-proportional \\(z\\)-test): The Sleep in America Poll found that \\(205\\) of \\(293\\) of Gen-Y and \\(235\\) of \\(469\\) of Gen-X use the Internet before sleep. Is this difference real? \\[ \\hat p_Y = \\dfrac{205}{293} = 0.700 \\\\ \\hat p_X = \\dfrac{235}{469} = 0.501 \\\\ \\hat p_Y - \\hat p_X = 0.700 - 0.501 = 0.199 \\] The null hypothesis claims that the two proportions are equal. We want to test whether the difference observed in the sample is statistically different from \\(0\\) or not. In other words, Null Model: \\(p_Y - p_X = 0\\) The sampling distribution of \\((\\hat p_Y - \\hat p_X)\\) is centered around \\(0\\). \\[ \\begin{align} \\hat p_{pooled} &amp;= \\frac{x_Y + x_X}{n_Y + n_X} = \\frac{205+235}{293+469} = 0.5774 \\\\ \\\\ SE_{pooled}(\\hat p_Y - \\hat p_X) &amp;= \\sqrt{\\frac{\\hat p_{pooled}(1- \\hat p_{pooled})}{n_X}+\\frac{\\hat p_{pooled}(1-\\hat p_{pooled})}{n_Y}} \\\\ &amp;= \\sqrt{\\frac{0.5774 \\times (1- 0.5774)}{293}+\\frac{0.5774 \\times (1-0.5774)}{469}} \\\\ &amp;= 0.0368 \\\\ \\\\ \\end{align} \\] \\[ \\begin{align} &amp;\\text{Two-tailed two-proportional z-test} \\\\ \\\\ &amp;H_0: p_Y - p_X = 0 \\\\ &amp;H_A: p_Y - p_X \\ne 0 \\\\ \\\\ &amp;\\hat p_Y - \\hat p_X = 0.700 - 0.501 = 0.199 \\\\ \\\\ &amp;z = \\frac{0.199 - 0}{0.0368} = 5.41 \\\\ \\\\ &amp;p \\text{-value} = 2 \\times P(z&gt;5.41) \\le 0.05 \\end{align} \\] Hence, reject \\(H_0\\). The difference between the proportions of Gen Y and Gen X is significantly different. Example (Two-proportional \\(z\\)-test): \\(62\\) of \\(325\\) girls and \\(75\\) of \\(268\\) boys have online profiles. Is there a real difference between all boys and girls? Null Model: mean: \\(p_B - p_G = 0\\) \\[ \\begin{align} \\hat p_{pooled} &amp;= \\frac{x_B + x_G}{n_B + n_G} = \\frac{75+62}{268+325} = 0.231 \\\\ \\\\ SE_{pooled}(\\hat p_B - \\hat p_G) &amp;= \\sqrt{\\frac{\\hat p_{pooled}(1- \\hat p_{pooled})}{n_B}+\\frac{\\hat p_{pooled}(1-\\hat p_{pooled})}{n_G}} \\\\ &amp;= \\sqrt{\\frac{0.231 \\times (1- 0.231)}{268}+\\frac{0.231 \\times (1-0.231)}{325}} \\\\ &amp;= 0.0348 \\\\ \\\\ \\end{align} \\] \\[ \\begin{align} &amp;\\text{Two-tailed two-proportional z-test} \\\\ &amp;H_0: p_B - p_G = 0 \\\\ &amp;H_A: p_B - p_G \\ne 0 \\\\ \\\\ &amp;\\hat p_B - \\hat p_G = 0.28 - 0.19 = 0.09 \\\\ \\\\ &amp;z = \\frac{0.09 - 0}{0.0348} = 2.59 \\\\ \\\\ &amp;p-value = 2 \\times P(z&gt;2.59) = 0.0096 \\le 0.05 \\end{align} \\] Reject \\(H_0\\). There is strong evidence to say that there is a statistically significant difference between the proportions of boys and girls who have online profiles. 9.2 Inferences about Two Independent Sample Means Objectives: Hypothesis test: conduct a hypothesis test of a claim about two population means. Confidence interval: construct a confidence interval estimate of the difference between two population means. Three Scenarios: The standard deviations of the two populations are unknown and are not assumed to be equal. The two population standard deviations are unknown but are assumed to be equal. The two population standard deviations are both known. Notations: \\[ \\begin{cases} \\mu_1 = \\text {mean of population 1 } \\\\ \\sigma_1 = \\text {standard deviation of population 1} \\\\ n_1 = \\text {size of the sample drawn from population 1 } \\\\ \\bar x_1 = \\text{mean of sample 1} \\\\ s_1 = \\text{standard deviation of sample 1} \\\\ \\end{cases} \\] The corresponding notations \\(\\mu_2, \\sigma_2, n_2, \\bar x_2, s_2\\) apply to population 2. 9.2.1 Hypothesis Test of Independent Samples: \\(\\sigma_1\\) and \\(\\sigma_2\\) Unknown and Not Assumed Equal Requirements Unequal variance - The values of \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown and we do not assume that they are equal. Independent - The two samples are independent. If drawn from the same population, the samples are not related or they are not naturally paired or matched with the sample values from the other population. Sample size - Both samples are simple random samples and either sample sizes are large (with \\(n_1 \\gt 30\\) and \\(n_2 \\gt 30\\)) or both samples come from populations having normal distributions. The methods presented in this section is robust against departures from normality, so they perform well on small samples as long as the departures from normality are not too extreme. The null hypothesis claims that the two population means are equal, i.e. the difference in two population means: \\(\\mu_1 - \\mu_2 = 0\\). A reasonable point estimate of \\(\\mu_1 - \\mu_2\\) based on the samples drawn from two populations can be written in the form: \\(\\bar x_1 - \\bar x_2\\). The sampling distribution of \\(\\bar x_1 - \\bar x_2\\) is distributed with a mean or expected value of \\(\\mu_1 - \\mu_2\\) and standard error: \\(SE_{\\bar x_1 - \\bar x_2} = \\sqrt{SE^2_{\\bar x_1} + SE^2_{\\bar x_2}} = \\sqrt{\\dfrac{\\sigma^2_1}{n_1}+\\dfrac{\\sigma^2_2}{n_2}}\\) When sample size is small or population standard deviations \\((\\sigma_1, \\sigma_2)\\) are unknown, each sample is assumed to follow \\(t_{df=\\nu}\\)-distribution with mean \\(0\\) and \\(SE_{\\bar x_1 - \\bar x_2} = \\sqrt{\\dfrac{s^2_1}{n_1}+\\dfrac{s^2_2}{n_2}}\\). Hypothesis Test Statistic for Two Means \\[ t_\\nu = \\dfrac{(\\bar x_1 - \\bar x_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\dfrac{s^2_1}{n_1}+\\dfrac{s^2_2}{n_2}}} \\ \\text{ where, } \\mu_1 - \\mu_2 = 0 \\\\ \\\\ \\text{degrees of freedom } (\\nu) = \\dfrac{(A + B)^2}{\\dfrac{A^2}{n_1 - 1} + \\dfrac{B^2}{n_2 - 1}} \\\\ \\\\ \\text{where, } \\ A = \\dfrac{s^2_1}{n_1} \\ \\text{ and } \\ B = \\dfrac{s^2_2}{n_2} \\\\ \\text{Alternatively, } df = min(n_1-1, n_2-1) \\\\ \\] 9.2.2 Confidence Interval Estimate of \\(\\mu_1 - \\mu_2\\): Independent Samples with \\(\\sigma_1\\) and \\(\\sigma_2\\) Unknown and Not Assumed Equal The confidence interval estimate of the difference \\(\\mu_1 - \\mu_2\\) is \\[(\\bar x_1 - \\bar x_2) - E &lt; \\mu_1 - \\mu_2 &lt; (\\bar x_1 - \\bar x_2) + E \\] where, \\[ E = t^*_{\\nu, \\alpha/2} \\times \\sqrt{\\dfrac{s^2_1}{n_1}+\\dfrac{s^2_2}{n_2}}\\] \\(t^*_{\\nu, \\alpha/2}\\) is the critical \\(t\\) score that corresponds to the confidence level \\((1-\\alpha)\\). Example: Two-Sample \\(t\\)-interval Find the \\(95\\%\\) confidence interval about the difference in sample means. \\[ \\begin{array}{c|c|c} &amp; \\text{Sample 1} &amp; \\text{Sample 2} \\\\ \\hline n &amp; 27 &amp; 27 \\\\ \\bar x &amp; 8.5 &amp; 14.7 \\\\ s &amp; 6.1 &amp; 8.4 \\end{array} \\] \\[ \\begin{align} &amp; \\bar x_1 - \\bar x_2 = 14.7 - 8.5 = 6.2 \\\\ &amp; t_{47.46} = 2.011 \\text { at CL} = 95\\% \\\\ \\\\ &amp; SE = \\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}} = \\sqrt{\\frac{8.4^2}{27}+\\frac{6.1^2}{27}} = 2 \\\\ \\\\ &amp; ME = 2.011 \\times 2 = 4.02 \\\\ &amp; CI: 6.2 \\pm 4.02 = [2.18, 10.22] \\end{align} \\] Example: Two-Sample \\(t\\)-test: Testing for the Difference between the Two Means Is there a statistically significant difference between two sample means? Generally, use unpooled \\(t\\)-test. Equal variance assumption is often violated in small samples. \\[ \\begin{array}{c|c|c} &amp; \\text{Sample 1} &amp; \\text{Sample 2} \\\\ \\hline n &amp; 8 &amp; 7 \\\\ \\bar y &amp; 281.88 &amp; 211.43 \\\\ s &amp; 18.31 &amp; 46.43 \\end{array} \\] \\[ \\begin{align} \\text{Two-tailed t test} \\\\ \\\\ H_0: \\mu_1 - \\mu_2 &amp;= 0 \\\\ H_A: \\mu_1 - \\mu_2 &amp;\\ne 0 \\\\ \\\\ \\bar y_1 - \\bar y_2 &amp;= 0.281.88 - 211.43 = 70.45 \\\\ \\\\ SE(\\bar y_1 - \\bar y_2) &amp;= \\sqrt{\\frac{18.31^2}{8}+\\frac{46.43^2}{7}} = 18.70 \\\\ t_{7.62} &amp;= \\frac{70.45 - 0}{18.70} = 3.77 \\\\ \\\\ p-value &amp;= 2 \\times P(t&gt;3.77) = 0.006 \\le 0.05 \\end{align} \\] Hence, we reject \\(H_0\\). 9.2.3 Pooled \\(t\\)-test: \\(\\sigma_1 = \\sigma_2\\) To use the pooled \\(t\\)-test, we must make the Equal Variance Assumption that the variances of the two populations from which the samples have been drawn are equal. That is \\(\\sigma_1^2 = \\sigma_2^2.\\) Even when the specific values of \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown but assumed equal, the sample variances \\(s_1^2\\) and \\(s_2^2\\) can be pooled to obtain an estimate of the common population variance \\(\\sigma^2\\). \\[ \\begin{align} &amp;\\text {Pooled variance:} \\\\ &amp;s^2_{p} = \\frac{(n_1-1)s^2_1+(n_2-1)s^2_2}{(n_1-1)+(n_2-1)} \\\\ &amp;SE_{p} = \\sqrt{s^2_{p} \\bigg (\\dfrac{1}{n_1} + \\dfrac{1}{n_2} \\bigg )} \\end{align} \\] \\[ \\text{Test Statistic: } \\ t = \\dfrac{(\\bar x_1- \\bar x_2) - (\\bar \\mu_1- \\bar \\mu_2)}{\\sqrt{s^2_{p} \\bigg (\\dfrac{1}{n_1} + \\dfrac{1}{n_2} \\bigg )}} \\] \\[ \\begin{align} &amp;\\text{Hypothesis test: } \\\\\\\\ &amp;H_0: \\mu_1 - \\mu_2 = 0 \\\\ &amp;H_A: \\mu_1 - \\mu_2 \\ne 0 \\\\ \\\\ &amp;\\text {pooled t-score, } t = \\dfrac{(\\bar x_1 - \\bar x_2 )-0}{SE_{p}} \\\\ &amp;df = (n_1 + n_2 - 2) \\\\ \\\\ &amp;\\text {Confidence Interval: } (\\bar x_1- \\bar x_2) \\pm t_{df}^* \\times SE_{p} &amp;\\end{align} \\] 9.3 Two Dependent Samples (Matched Pairs) This section presents methods for testing hypotheses and constructing confidence intervals involving the mean of the differences of the values from two populations that are dependent in the sense that the data consist of matched pairs. The pairs must be matched according to some relationship, such as before/after measurements from the same subjects or IQ scores of siblings. Notation for Dependent Samples \\[ \\begin{cases} d = \\text {individual difference between the two values in a single matched pair} \\\\ \\mu_d = \\text {mean value of the differences } d \\text{ for the population of all matched pairs} \\\\ \\bar d = \\text {mean value of the difference } d \\text{ for the matched sample data} \\\\ s_d = \\text{standard deviation of the differences } d \\text{ for the paired sample data} \\\\ n = \\text{ number of pairs of sample data} \\\\ \\end{cases} \\] Requirements The sample data are dependent (matched pairs). The matched pair are a simple random sample. Either or both of these conditions are satisfied: the number of pairs of sample data is large \\((n &gt; 30)\\) or the pairs of values have differences that are from a population having a distribution that is approximately normal. 9.3.1 Hypothesis Testing for Paired Data: The paired \\(t\\)-test The null hypothesis claims that the mean of the differences between two dependent samples (matched pairs) is equal to \\(0\\). Test Statistic for Dependent Samples (with \\(H_0: \\mu_d = 0\\)) \\[ t_{n-1} = \\dfrac{\\bar d - \\mu_d}{\\dfrac{s_d}{\\sqrt n}} \\] Example: The table shows the times recorded by \\(17\\) track athletes to complete their inner and outer circles in an track and field competition. Conduct a hypothesis test to verify the claim that the recorded inner circle times are not significantly different from the outer circle times. \\[ \\begin{array}{c|c|c} &amp; \\text{Inner Time} &amp; \\text{Outer Time} &amp; Diff\\\\ \\hline 1 &amp; 125.75 &amp; 122.34 &amp; 3.41 \\\\ 2 &amp; 121.63 &amp; 122.12 &amp; -0.49 \\\\ 3 &amp; 122.24 &amp; 123.35 &amp; -1.11 \\\\ 4 &amp; 120.85 &amp; 120.45 &amp; 0.40 \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ 17 &amp; 122.15 &amp; 122.75 &amp; -0.60 \\\\ \\end{array} \\] \\[ \\begin{align} &amp; \\text{Hypotheses} \\\\ &amp; H_0:\\mu_d = 0 \\\\ &amp; H_1: \\mu_d \\ne 0 \\\\ \\\\ &amp; n = 17, \\\\ &amp;\\bar d = 0.499, \\\\ &amp;s_d = 2.333 \\\\ \\\\ &amp; SE(\\bar d) = \\dfrac{s_d}{\\sqrt n} = \\dfrac{2.333}{\\sqrt 17} = 0.5658 \\\\ \\\\ &amp; t_{16} = \\dfrac{\\bar d-0}{SE(\\bar d)} = \\dfrac{0.499}{0.5658} = 0.882 \\\\ \\\\ &amp; p-value = 2 \\times P(t_{16}&gt;0.882) = 0.39 &gt; 0.05 \\end{align} \\] Hence, \\(H_0\\) cannot be rejected. The inner and outer circle times are not significantly different. 9.3.2 Paired \\(t\\)-Interval The \\(95\\%\\) confidence interval for the mean paired difference is \\[ \\begin{align} &amp; \\bar d \\pm t_{n-1}^* \\times \\frac{s_d}{\\sqrt n} \\\\ \\\\ &amp; = 0.499 \\pm 2.12 \\times 0.5658 \\\\ \\\\ CI &amp;= [-0.7005, 1.6985] \\end{align} \\] "],["correlation-and-regression.html", "Chapter 10 Correlation and Regression 10.1 Correlation 10.2 Correlation Coefficient 10.3 Estimation from One Variable 10.4 Regression: Estimation from Two Variables", " Chapter 10 Correlation and Regression Learning Outcome Construct linear regression models and correlation coefficients from datasets. This chapter continues to build on the analysis of paired sample data. Here, we present methods for determining whether there is a correlation, or association, between two variables. For linear correlations, we identify an equation of a straight line that best fits the data which can be used to predict the value of one variable given the value of the other variable. 10.1 Correlation A correlation exists between two variables when the values of one variable are somehow associated with the values of the other variable. A linear correlation exists between two variables when there is a correlation and the plotted points of paired data result in a pattern that can be approximated by a straight line. Univariate Data Bivariate Data - Scatterplot Scatterplots exhibit the relationship between two numeric variables. They are used for detecting patterns, trends, and relationships. Bivariate Data - Positive Association Unit of observation used in the plot is ‘county’. The plot shows positive association between education and income. Linear Association between Variables Linear Association: the data points in the scatterplot is clustered around a straight line Positive (Linear) Association: above average values of one variable tend to go with above average values of the other; scatterplot slopes up. Negative (Linear) Association: above average values of one variable tend to go with below average values of the other; scatterplot slopes down. No (Linear) Association: Scatterplot shows no direction 10.2 Correlation Coefficient Correlation coefficient: \\(\\lbrace r | -1 \\le r \\le +1 \\rbrace\\) measures the strength of the linear correlation (association) between the paired quantitative \\(x\\) values and \\(y\\) values in a sample; in other words, how tightly the points are clustered about a straight line. in order to have a linear correlation between two random variables, the pairs of \\((x, y)\\) data must have a bivariate normal distribution. 10.2.1 Calculating Correlation Coefficient \\[ \\begin{array}{c|c|c|c} x &amp; y &amp; z_x &amp; z_y &amp; z_x \\cdot z_y \\\\ \\hline 1 &amp; 2 &amp; -1.414 &amp; -0.777 &amp; 1.098 \\\\ 2 &amp; 3 &amp; -0.707 &amp; -0.291 &amp; 0.206 \\\\ 3 &amp; 1 &amp; 0 &amp; -1.263 &amp; 0 \\\\ 4 &amp; 6 &amp; 0.707 &amp; 1.166 &amp; 0.824 \\\\ 5 &amp; 6 &amp; 1.414 &amp; 1.166 &amp; 1.648 \\\\ \\hline \\bar x = 5, s_x = 1.58 &amp; \\bar y = 3.6, s_y = 2.30 &amp; &amp; &amp; r = \\dfrac{1}{n} \\sum z_x \\cdot z_y = 0.755 \\end{array} \\] Formula for Calculating Correlation Coefficient \\(\\text {If the data are} \\space (x_i, y_i), 1\\le i\\le n, \\text {then}\\) \\[ {\\text{population: } \\rho = \\dfrac{1}{n}\\sum_{i=1}^n \\left(\\frac{x_i-\\mu_x}{\\sigma_x}\\right)\\left(\\frac{y_i-\\mu_y}{\\sigma_y}\\right)} \\\\ {\\text{sample: } r = \\frac{1}{n-1}\\sum_{i=1}^n \\left(\\dfrac{x_i-\\bar x}{s_x}\\right)\\left(\\frac{y_i-\\bar y}{s_y}\\right)} \\] Properties of Correlation Coefficient \\(r\\) is a pure number with no units \\(-1\\le r\\le +1\\) Adding a constant to one of the variables does not affect \\(r\\) Multiplying one of the variables by a positive constant does not affect \\(r\\) Multiplying one of the variables by a negative constant switches the sign of \\(r\\) but does not affect the absolute value of \\(r\\) \\(r\\) measures the strength of a linear relationship. It is not designed to measure the strength of a non-linear relationship. \\(r\\) is very sensitive to outliers. Correlation coefficient measures linear association If two variables have a non-zero correlation, then they are related to each other in some way, but that does not mean that one causes the other. Two variable appear to strongly associated, but \\(r\\) is close to \\(0\\). This is because the relationship is clearly nonlinear. \\(r\\) measures linear association. Don’t use it if the scatter diagram is nonlinear. 10.2.2 Hypothesis Testing for Correlation Coefficient Hypotheses If conducting a formal hypothesis test to determine whether there is a significant linear correlation between two variables, use the following null and alternative hypotheses that use \\(\\rho\\) to represent the linear correlation coefficient of the population: \\[ \\text{Null Hypothesis } H_0: \\rho = 0 \\text{ (no correlation)} \\\\ \\text{Alt. Hypothesis } H_a: \\rho \\ne 0 \\text{ (correlation)} \\\\ \\\\ t_{df = n-2} = \\dfrac{r}{\\sqrt{\\dfrac{1-r^2}{n-2}}} \\\\ \\\\ \\] Then, calculate p-value and compare with sig. level 5% to accept or reject null hypothesis. Example: \\[ \\begin{align} H_0&amp;: \\rho = 0 \\\\ H_a&amp;: \\rho \\ne 0 \\\\ \\\\ \\text{From data, } r &amp;= 0.828 \\\\ n &amp;= 6 - 2 = 4 \\\\ t &amp;= \\dfrac{r}{\\sqrt{\\frac{1-r^2}{n-2}}} = \\frac{0.828}{\\sqrt{\\frac{1-0.828^2}{6-2}}} \\\\ &amp;= 2.955 \\\\ \\\\ p -value &amp;= 0.04176 &lt; 0.05 \\\\ \\end{align} \\] Hence, we reject null hypothesis. The data suggests that there is a linear association between dinner bill and tips. 10.3 Estimation from One Variable When we have information available from only one variable, the mean of the variable is the best estimate of an unknown data point in that variable. Estimate the height of one of these people: Heights (inches) ~ \\(N(67,3)\\) Let’s say, estimate = \\(c\\) estimation error = actual height - \\(c\\) The “best” \\(c\\) is the one that makes the smallest root mean squared (r.m.s) error The root mean squared (r.m.s) of the errors will be smallest if \\(c = \\mu\\) least squared estimate = \\(\\mu\\) = 67 and least squared error = \\(\\sigma\\) = 3 10.4 Regression: Estimation from Two Variables We estimate the value of one variable \\((y)\\) from another variable \\((x)\\). Assume, both variables are approximately normally distributed. The method for finding the equation of the straight line that best fits the points in a scatterplot of paired sample data. The best-fitting straight line is called the regression line, and its equation is called the regression equation. We can use the regression equation to make predictions for the value of one of the variables \\((y)\\), given the specific value of the other variable \\((x)\\). Requirements The sample of paired \\((x,y)\\) data is a random sample of quantitative data. For each value of \\(x\\), the corresponding values of \\(y\\) have a normal distribution. For the different values of \\(x\\), the distributions of the corresponding \\(y\\)-values all the the same variance. Note: Outliers can have strong effect on the regression equation. 10.4.1 Regression Line Regression Equation: \\[ \\begin{align} &amp;\\text{population model: } y = \\beta_0 + \\beta_1 x \\\\ &amp;\\text{sample model: } \\hat y = b_0 + b_1 x \\\\ \\\\ &amp;x: \\text{explanatory, predictor, or independent variable} \\\\ &amp;y: \\text{response or dependent variable} \\\\ \\end{align} \\] Regression Equation in Standard Units 10.4.2 Derivation of Bivariate Regression Model From bivariate scatter plot in standard units: \\[ \\begin{align} z_y &amp; = \\rho.z_x \\\\ \\dfrac {y-\\mu_y}{\\sigma_y} &amp; = \\rho. \\dfrac {x-\\mu_x}{\\sigma_x} \\\\ y-\\mu_y &amp; = \\rho. \\dfrac {\\sigma_y}{\\sigma_x} (x-\\mu_x) \\\\ y &amp; = \\rho. \\dfrac {\\sigma_y}{\\sigma_x} (x-\\mu_x) + \\mu_y\\\\ y &amp; = \\left ( \\mu_y - \\rho. \\dfrac {\\sigma_y}{\\sigma_x}\\mu_x \\right ) + \\left ( \\rho. \\dfrac {\\sigma_y}{\\sigma_x} \\right ).x \\\\ y &amp; = \\beta_0 + \\beta_1.x \\\\ \\text {Where, } &amp; \\begin{cases} \\text{slope } (\\beta_1) &amp; = \\rho. \\dfrac {\\sigma_y}{\\sigma_x} \\\\ \\text{intercept } (\\beta_0) &amp; = \\mu_y - \\beta_1.\\mu_x \\\\ \\end{cases} \\\\ \\text {When, } x &amp; = \\mu_x, y = \\mu_y \\\\ \\end{align} \\] \\[ \\bbox[yellow,5px] { \\color {black} {\\implies \\text {The regression line passes through the points of averages } (\\mu_x, \\mu_y).} } \\] Example: Finding the Equation Find the equation of the regression line for estimating weight based on height. Height (inches) (x): mean = 67 sd = 3 Weight (lb) (y): mean = 174 sd = 21 r = 0.304 \\[ \\begin{align} \\text{slope } (b_1) &amp; = r. \\dfrac {s_y}{s_x} \\\\ \\text{intercept } (b_0) &amp; = \\bar y - b_1.\\bar x \\\\ \\end{align} \\] slope (b1) = 2.07 lb per inch intercept (b0) = 35 lb Regression Equation: Est. weight = 35 + 2.07.(height) A person who is 60 inches tall is estimated to be 159 lb 10.4.3 Interpretation of Regression Equation Intercept Mathematically, the intercept is described as the mean response \\((Y)\\) value when all predictor variables \\((X)\\) are set to zero. Sometimes a zero setting for the predictor variable(s) is nonsensical, which makes the intercept noninterpretable. For example, in the following equation: \\(\\hat {Weight} = 35 + 2 \\times Height\\) \\(Height = 0\\) is nonsensical; therefore, the model intercept has no interpretation. Why is it still crucial to include the intercept in the model? The constant in regression model guarantees that the residuals have a mean of zero, which is a key assumption in regression analysis. If we don’t include the constant, the regression line is forced to go through the origin. This means that all of the predictors and the response variable must equal to zero at that point. If the fitted line doesn’t naturally go through the origin, the regression coefficients and predictions will be biased. The constant guarantees that the residuals don’t have an overall positive or negative bias. Slope The slope of the regression line measures how much the value of \\(Y\\) changes for every unit of change in \\(X\\). For example, in the following equation: \\(\\hat {Weight} = 35 + 2 \\times Height\\) The slope is \\(2 \\text { lb per inch}\\) - meaning that if a group of people is one inch taller than another group, the former group will be on average \\(2\\) lb heavier than the later. Remember, the slope should NOT be interpreted as: if one person gets taller by 1 inch, he/she will put on 2 lb of weight. Outliers and Influential points In a scatterplot, an outlier is a point lying far away from the other data points. Paired sample data may include one or more influential points, which are points that strongly affect the graph of the regression line. 10.4.4 Residuals Which line to use? Objectively, we want a line that produces the least estimation error. \\(\\text {Data } (y) = \\text {Fit } (\\hat y) + \\text {Residual } (e)\\) Residuals are the leftover variation in the data after accounting for the model fit. These are the difference between observed and expected. The residual of the \\(i^{th}\\) observation \\((x_i, y_i)\\) is the difference between the observed response \\((y_i)\\) and its predicted value based on model fit \\((\\hat y_i)\\): \\(e_i = y_i - \\hat y_i\\) 10.4.5 Least Squared Line In the scatter plot, residual (in other words, estimation error) is shown as the vertical distance between the observed point and the line. If an observation is above the line, then its residual is positive. Observations below the line have negative residuals. One goal in picking the right linear model is for these residuals to be as small as possible. Common practice is to choose the line that minimizes the sum of squared residuals. \\(\\text{sum of squared residuls} = e_1^2 + e_2^2 + ... + e_n^2\\) There is only one line that minimizes the sum of squared residuals. It is called the least squared line. Mathematically, it can be shown that the regression line is the least squared line. r.m.s error of regression root mean squared (r.m.s) error of regression = r.m.s of residuals = \\[ \\bbox[yellow,5px] { \\color{black}{\\sqrt {1-\\rho^2}.\\sigma_y} } \\] \\(\\rho = 1 \\text { or } -1: \\space\\) Scatter is a perfect straight line; r.m.s error of regression = \\(0\\) \\(\\rho = 0: \\space\\) No linear association; r.m.s error of regression = \\(\\sigma_y\\) All other \\(\\rho: \\space\\) Regression is not perfect, but better than using the average; r.m.s error of regression \\(\\lt \\sigma_y\\) Conditions for the Least Squared Line Residual Plot A residual plot is a scatterplot of the points \\(x, y - \\hat y\\). The plot helps to examine whether the assumptions of the linear regression model are satisfied. One of the most important assumptions of linear regression is that the residuals are independent, normally distributed, and uncorrelated with the explanatory variable \\((x)\\). If these assumptions are satisfied, then the residual plot should show random and near symmetrical distribution of the residuals about the line \\(y = 0\\). A good residual plot should show no pattern. Another assumption, known as the constant error variance, is tested by examining the plot of residual versus fit \\((\\hat y)\\). If this assumption is satisfied, the plot should show constant variability of the residuals about the line \\(y = 0\\). The spread of the residuals is roughly equal at each level of the fitted values. These assumptions are violated in data with correlated observations such as time series data (e.g. daily stock price). Linear regression is not appropriate to analyze data with such underlying structures. `geom_smooth()` using formula &#39;y ~ x&#39; 10.4.6 Coefficient of Determination \\[ \\text{total variation = explained variation + unexplained variation} \\\\ \\sum(y - \\bar y)^2 = \\sum(\\hat y - \\bar y)^2 + \\sum(y - \\hat y)^2 \\] \\(R^2\\) is the proportion of the variance in the dependent variable \\(y\\) that is explained by the linear relationship between \\(x\\) and \\(y\\). \\[ r^2 = \\frac{\\text{explained variation}}{\\text{total variation}} \\] \\(R^2 = 58\\%\\) suggests that \\(58\\%\\) of the variability in \\(y\\) can be explained by the variability in \\(x\\). \\(R^2 \\space\\) provides a measure of how useful the regression line is as a prediction tool. If \\(R^2 \\space\\) is close to 1, then the regression line is useful. If \\(R^2 \\space\\) is close to 0, then the regression line is not useful. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
